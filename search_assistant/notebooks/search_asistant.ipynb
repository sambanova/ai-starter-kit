{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Assitant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "from langchain.prompts import PromptTemplate, load_prompt\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from urllib.parse import urljoin, urlparse, urldefrag\n",
    "\n",
    "from utils.sambanova_endpoint import SambaNovaEndpoint, SambaverseEndpoint\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sambeverse llm\n",
    "llm = SambaverseEndpoint(\n",
    "            sambaverse_model_name=\"Meta/llama-2-70b-chat-hf\",\n",
    "            model_kwargs={\n",
    "                \"do_sample\": False, \n",
    "                \"max_tokens_to_generate\": 500,\n",
    "                \"temperature\": 0.01,\n",
    "                \"top_p\": 1,\n",
    "                \"process_prompt\": True,\n",
    "                \"select_expert\": \"llama-2-70b-chat-hf\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "#sambastudio llm\n",
    "#llm = SambaNovaEndpoint(\n",
    "#    model_kwargs={\"do_sample\": False, \"temperature\": 0.0},\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only admits Google Search\n",
    "def querySerper(query: str, limit: int = 5, do_analysis: bool = True ,include_site_links: bool = False):\n",
    "    \"\"\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\"\"\"\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    payload = json.dumps({\n",
    "        \"q\": query,\n",
    "        \"num\": limit\n",
    "    })\n",
    "    headers = {\n",
    "        'X-API-KEY': os.environ.get(\"SERPER_API_KEY\"),\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=payload).json()\n",
    "    results=response[\"organic\"]\n",
    "    links = [r[\"link\"] for r in results]\n",
    "    if include_site_links:\n",
    "        sitelinks = []\n",
    "        for r in [r.get(\"sitelinks\",[]) for r in results]:\n",
    "            sitelinks.extend([site.get(\"link\", None) for site in r])\n",
    "        links.extend(sitelinks)\n",
    "    links=list(filter(lambda x: x is not None, links))\n",
    "    \n",
    "    if do_analysis:\n",
    "        prompt = load_prompt(os.path.join(kit_dir, \"prompts/llama70b-SerperSearchAnalysis.yaml\"))\n",
    "        formatted_prompt = prompt.format(question=query, context=json.dumps(results))\n",
    "        return llm.invoke(formatted_prompt), links\n",
    "    else:\n",
    "        return response, links\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querySerper(\"who is the president of America\", do_analysis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryOpenSerp(query: str, limit: int = 5, do_analysis: bool = True, engine=\"google\") -> str:\n",
    "    \"\"\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\"\"\"\n",
    "    if engine not in [\"google\",\"yandex\",\"baidu\"]:\n",
    "        raise ValueError(\"engine must be either google, yandex or baidu\")\n",
    "    url = f\"http://127.0.0.1:7000/{engine}/search\"\n",
    "    params = {\n",
    "        \"lang\": \"EN\",\n",
    "        \"limit\": limit,\n",
    "        \"text\": query\n",
    "    }\n",
    "\n",
    "    results = requests.get(url, params=params).json()\n",
    "    \n",
    "    links = [r[\"url\"] for r in results]\n",
    "    if do_analysis:\n",
    "        prompt = load_prompt(os.path.join(kit_dir, \"prompts/llama70b-OpenSearchAnalysis.yaml\"))\n",
    "        formatted_prompt = prompt.format(question=query, context=json.dumps(results))\n",
    "        return llm.invoke(formatted_prompt), links\n",
    "    else:\n",
    "        return results, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryOpenSerp(\"who is the president of America\", do_analysis=True, engine=\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "def querySerpapi(query: str, limit: int = 5, do_analysis: bool = True, engine=\"google\") -> str:\n",
    "    if engine not in [\"google\", \"bing\"]:\n",
    "        raise ValueError(\"engine must be either google or bing\")\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"num\": limit,\n",
    "        \"engine\":engine,\n",
    "        \"api_key\": os.environ.get(\"SERPAPI_API_KEY\")\n",
    "        }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    response= search.get_dict()\n",
    "    \n",
    "    knowledge_graph = response.get(\"knowledge_graph\", None)\n",
    "    results =  response.get(\"organic_results\",None)\n",
    "\n",
    "    links = []\n",
    "    links = [r[\"link\"] for r in results]\n",
    "    \n",
    "    \n",
    "    if do_analysis:\n",
    "        prompt = load_prompt(os.path.join(kit_dir, \"prompts/llama70b-SerpapiSearchAnalysis.yaml\"))\n",
    "        if knowledge_graph:\n",
    "            knowledge_graph_str = json.dumps(knowledge_graph)\n",
    "            knowledge_graph = remove_links(knowledge_graph_str)\n",
    "            print(knowledge_graph)\n",
    "            formatted_prompt = prompt.format(question=query, context=json.dumps(knowledge_graph))\n",
    "        else:\n",
    "            results_str = json.dumps(results)\n",
    "            results_str = remove_links(results_str)\n",
    "            formatted_prompt = prompt.format(question=query, context=json.dumps(results))\n",
    "        return llm.invoke(formatted_prompt), links\n",
    "    else:\n",
    "        return response, links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(querySerpapi(\"Who is the president of USA\", engine=\"bing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(querySerpapi(\"Who is the president of USA\", engine=\"google\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapaing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = os.path.join(kit_dir,\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_info():\n",
    "    \"\"\"\n",
    "    Loads json config file\n",
    "    \"\"\"\n",
    "    # Read config file\n",
    "    with open(CONFIG_PATH, 'r') as yaml_file:\n",
    "        config = yaml.safe_load(yaml_file)\n",
    "    api_info = config[\"api\"]\n",
    "    llm_info =  config[\"llm\"]\n",
    "    retrieval_info = config[\"retrieval\"]\n",
    "    web_crawling_params = config[\"web_crawling\"]\n",
    "    extra_loaders = config[\"extra_loaders\"]\n",
    "    \n",
    "    \n",
    "    return api_info, llm_info, retrieval_info, web_crawling_params, extra_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_remote_pdf(url):\n",
    "    \"\"\"\n",
    "    Load PDF files from the given URL.\n",
    "    Args:\n",
    "        url (str): URL to load pdf document from.\n",
    "    Returns:\n",
    "        list: A list of loaded pdf documents.\n",
    "    \"\"\"\n",
    "    loader = UnstructuredURLLoader(urls=[url])\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_htmls(urls, extra_loaders=None):\n",
    "    \"\"\"\n",
    "    Load HTML documents from the given URLs.\n",
    "    Args:\n",
    "        urls (list): A list of URLs to load HTML documents from.\n",
    "    Returns:\n",
    "        list: A list of loaded HTML documents.\n",
    "    \"\"\"\n",
    "    if extra_loaders is None:\n",
    "        extra_loaders = []\n",
    "    docs=[]\n",
    "    for url in urls:\n",
    "        if url.endswith(\".pdf\"):\n",
    "            if \"pdf\" in extra_loaders:\n",
    "                docs.extend(load_remote_pdf(url))\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            loader = AsyncHtmlLoader(url, verify_ssl=False)\n",
    "            docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_filter(all_links, excluded_links):\n",
    "    \"\"\"\n",
    "    Filters a list of links based on a list of excluded links.\n",
    "    Args:\n",
    "        all_links (List[str]): A list of links to filter.\n",
    "        excluded_links (List[str]): A list of excluded links.\n",
    "    Returns:\n",
    "        Set[str]: A list of filtered links.\n",
    "    \"\"\"\n",
    "    clean_excluded_links=set()\n",
    "    for excluded_link in excluded_links:\n",
    "        parsed_link=urlparse(excluded_link)\n",
    "        clean_excluded_links.add(parsed_link.netloc + parsed_link.path)\n",
    "    filtered_links = set()\n",
    "    for link in all_links:\n",
    "        # Check if the link contains any of the excluded links\n",
    "        if not any(excluded_link in link for excluded_link in clean_excluded_links):\n",
    "            filtered_links.add(link)\n",
    "    return filtered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_docs(docs):\n",
    "    \"\"\"\n",
    "    Clean the given HTML documents by transforming them into plain text.\n",
    "    Args:\n",
    "        docs (list): A list of langchain documents with html content to clean.\n",
    "    Returns:\n",
    "        list: A list of cleaned plain text documents.\n",
    "    \"\"\"\n",
    "    html2text_transformer = Html2TextTransformer()\n",
    "    docs=html2text_transformer.transform_documents(documents=docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_crawl(urls, excluded_links=None):\n",
    "    \"\"\"\n",
    "    Perform web crawling, retrieve and clean HTML documents from the given URLs, with specified depth of exploration.\n",
    "    Args:\n",
    "        urls (list): A list of URLs to crawl.\n",
    "        excluded_links (list, optional): A list of links to exclude from crawling. Defaults to None.\n",
    "        depth (int, optional): The depth of crawling, determining how many layers of internal links to explore. Defaults to 1\n",
    "    Returns:\n",
    "        tuple: A tuple containing the langchain documents (list) and the scrapped URLs (list).\n",
    "    \"\"\"\n",
    "    *_, web_crawling_params, extra_loaders = get_config_info()\n",
    "    if excluded_links is None:\n",
    "        excluded_links = []\n",
    "    excluded_links.extend([\"facebook.com\", \"twitter.com\", \"instagram.com\", \"linkedin.com\", \"telagram.me\", \"reddit.com\", \"whatsapp.com\", \"wa.me\"])\n",
    "    excluded_link_suffixes = {\".ico\", \".svg\", \".jpg\", \".png\", \".jpeg\", \".\", \".docx\", \".xls\", \".xlsx\"}\n",
    "    scrapped_urls=[]\n",
    "    \n",
    "    urls = [url for url in urls if not url.endswith(tuple(excluded_link_suffixes))]\n",
    "    urls = link_filter(urls, set(excluded_links)) \n",
    "    urls = list(urls)[:web_crawling_params[\"max_scraped_websites\"]]   \n",
    "        \n",
    "    scraped_docs = load_htmls(urls, extra_loaders)\n",
    "    scrapped_urls.append(urls)\n",
    "        \n",
    "    docs=clean_docs(scraped_docs)\n",
    "    return docs, scrapped_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,links=querySerpapi(\"Who is the president of USA\", engine=\"google\", do_analysis=False)\n",
    "docs, links = web_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieval and vdb creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectordb.vector_db import VectorDb\n",
    "vectordb=VectorDb()\n",
    "config={\"persist_directory\":\"NoneDirectory\"}\n",
    "documents =  docs\n",
    "def create_load_vector_store(force_reload: bool = False, update: bool = False):\n",
    "        \n",
    "        *_, retrieval_info, _, _ = get_config_info()\n",
    "        \n",
    "        persist_directory = config.get(\"persist_directory\", \"NoneDirectory\")\n",
    "        \n",
    "        embeddings = vectordb.load_embedding_model()\n",
    "        \n",
    "        if os.path.exists(persist_directory) and not force_reload and not update:\n",
    "            vector_store = vectordb.load_vdb(persist_directory, embeddings, db_type = retrieval_info[\"db_type\"])\n",
    "        \n",
    "        elif os.path.exists(persist_directory) and update:\n",
    "            chunks = vectordb.get_text_chunks(documents , retrieval_info[\"chunk_size\"], retrieval_info[\"chunk_overlap\"])\n",
    "            vector_store = vectordb.load_vdb(persist_directory, embeddings, db_type = retrieval_info[\"db_type\"])\n",
    "            vector_store = vectordb.update_vdb(chunks, embeddings, retrieval_info[\"db_type\"], persist_directory)\n",
    "            \n",
    "        else:\n",
    "            chunks = vectordb.get_text_chunks(documents , retrieval_info[\"chunk_size\"], retrieval_info[\"chunk_overlap\"])\n",
    "            vector_store = vectordb.create_vector_store(chunks, embeddings, retrieval_info[\"db_type\"], None)\n",
    "            \n",
    "        return vector_store\n",
    "    \n",
    "def create_and_save_local(self, input_directory, persist_directory, update=False):\n",
    "    \n",
    "    *_, retrieval_info, _, _ = get_config_info()\n",
    "    \n",
    "    chunks = vectordb.get_text_chunks(documents , retrieval_info[\"chunk_size\"], retrieval_info[\"chunk_overlap\"])\n",
    "    embeddings = vectordb.load_embedding_model()\n",
    "    if update:\n",
    "        config[\"update\"]=True\n",
    "        vector_store = vectordb.update_vdb(chunks, embeddings, retrieval_info[\"db_type\"], input_directory, persist_directory)\n",
    "\n",
    "    else:\n",
    "        vector_store = vectordb.create_vector_store(chunks, embeddings, retrieval_info[\"db_type\"], persist_directory)\n",
    "        \n",
    "    return vector_store\n",
    "\n",
    "vector_store=create_load_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def retrieval_qa_chain():\n",
    "    *_, retrieval_info, _, _ = get_config_info()\n",
    "    prompt = load_prompt(os.path.join(kit_dir,\"prompts/llama7b-web_scraped_data_retriever.yaml\"))\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\"score_threshold\": retrieval_info[\"score_treshold\"], \"k\": retrieval_info[\"k_retrieved_documents\"]},\n",
    "    )\n",
    "    qa_chain = RetrievalQA.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        verbose=True,\n",
    "        input_key=\"question\",\n",
    "        output_key=\"answer\",\n",
    "        prompt=prompt\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval_qa_chain() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"who is joe biden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
