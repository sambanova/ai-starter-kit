{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Assitant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "from langchain_classic.prompts import PromptTemplate, load_prompt\n",
    "from langchain_classic.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain_classic.document_loaders import AsyncHtmlLoader\n",
    "from langchain_classic.document_transformers import Html2TextTransformer\n",
    "from urllib.parse import urljoin, urlparse, urldefrag\n",
    "from langchain_classic.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from utils.model_wrappers.langchain_llms import SambaNovaCloud, SambaStudio\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sambastudio LLM\n",
    "#llm = SambaStudio(\n",
    "#    model_kwargs={\n",
    "#     'max_tokens': 500,\n",
    "#     'temperature': 0.01,\n",
    "#     'top_p': 1,\n",
    "#     'process_prompt': False,\n",
    "#     'model': 'Meta-Llama-3-8B-Instruct'\n",
    "#   },\n",
    "# )\n",
    "\n",
    "#sncloud llm\n",
    "llm = SambaNovaCloud(\n",
    "    max_tokens = 500,\n",
    "    model= 'llama3-8b',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only admits Google Search\n",
    "def querySerper(query: str, limit: int = 5, do_analysis: bool = True, include_site_links: bool = False):\n",
    "    \"\"\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\"\"\"\n",
    "    url = 'https://google.serper.dev/search'\n",
    "    payload = json.dumps({'q': query, 'num': limit})\n",
    "    headers = {'X-API-KEY': os.environ.get('SERPER_API_KEY'), 'Content-Type': 'application/json'}\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=payload).json()\n",
    "    results = response['organic']\n",
    "    links = [r['link'] for r in results]\n",
    "    if include_site_links:\n",
    "        sitelinks = []\n",
    "        for r in [r.get('sitelinks', []) for r in results]:\n",
    "            sitelinks.extend([site.get('link', None) for site in r])\n",
    "        links.extend(sitelinks)\n",
    "    links = list(filter(lambda x: x is not None, links))\n",
    "\n",
    "    if do_analysis:\n",
    "        prompt = load_prompt(os.path.join(kit_dir, 'prompts/llama3-serp_analysis.yaml'))\n",
    "        formatted_prompt = prompt.format(question=query, context=json.dumps(results))\n",
    "        return llm.invoke(formatted_prompt), links\n",
    "    else:\n",
    "        return response, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querySerper('who is the president of America', do_analysis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryOpenSerp(query: str, limit: int = 5, do_analysis: bool = True, engine='google') -> str:\n",
    "    \"\"\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\"\"\"\n",
    "    if engine not in ['google', 'yandex', 'baidu']:\n",
    "        raise ValueError('engine must be either google, yandex or baidu')\n",
    "    url = f'http://127.0.0.1:7000/{engine}/search'\n",
    "    params = {'lang': 'EN', 'limit': limit, 'text': query}\n",
    "\n",
    "    results = requests.get(url, params=params).json()\n",
    "\n",
    "    links = [r['url'] for r in results]\n",
    "    if do_analysis:\n",
    "        prompt = load_prompt(os.path.join(kit_dir, 'prompts/llama3-serp_analysis.yaml'))\n",
    "        formatted_prompt = prompt.format(question=query, context=json.dumps(results))\n",
    "        return llm.invoke(formatted_prompt), links\n",
    "    else:\n",
    "        return results, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryOpenSerp('who is the president of America', do_analysis=True, engine='google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "\n",
    "def querySerpapi(query: str, limit: int = 5, do_analysis: bool = True, engine='google') -> str:\n",
    "    if engine not in ['google', 'bing']:\n",
    "        raise ValueError('engine must be either google or bing')\n",
    "    params = {'q': query, 'num': limit, 'engine': engine, 'api_key': os.environ.get('SERPAPI_API_KEY')}\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    response = search.get_dict()\n",
    "\n",
    "    knowledge_graph = response.get('knowledge_graph', None)\n",
    "    results = response.get('organic_results', None)\n",
    "\n",
    "    links = []\n",
    "    links = [r['link'] for r in results]\n",
    "\n",
    "    if do_analysis:\n",
    "        prompt = load_prompt(os.path.join(kit_dir, 'prompts/llama3-serp_analysis.yaml'))\n",
    "        if knowledge_graph:\n",
    "            knowledge_graph_str = json.dumps(knowledge_graph)\n",
    "            knowledge_graph = remove_links(knowledge_graph_str)\n",
    "            print(knowledge_graph)\n",
    "            formatted_prompt = prompt.format(question=query, context=json.dumps(knowledge_graph))\n",
    "        else:\n",
    "            results_str = json.dumps(results)\n",
    "            results_str = remove_links(results_str)\n",
    "            formatted_prompt = prompt.format(question=query, context=json.dumps(results))\n",
    "        return llm.invoke(formatted_prompt), links\n",
    "    else:\n",
    "        return response, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(querySerpapi('Who is the president of USA', engine='bing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(querySerpapi('Who is the president of USA', engine='google'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = os.path.join(kit_dir, 'config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_info():\n",
    "    \"\"\"\n",
    "    Loads json config file\n",
    "    \"\"\"\n",
    "    # Read config file\n",
    "    with open(CONFIG_PATH, 'r') as yaml_file:\n",
    "        config = yaml.safe_load(yaml_file)\n",
    "    api_info = config['api']\n",
    "    llm_info = config['llm']\n",
    "    retrieval_info = config['retrieval']\n",
    "    web_crawling_params = config['web_crawling']\n",
    "    extra_loaders = config['extra_loaders']\n",
    "\n",
    "    return api_info, llm_info, retrieval_info, web_crawling_params, extra_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_remote_pdf(url):\n",
    "    \"\"\"\n",
    "    Load PDF files from the given URL.\n",
    "    Args:\n",
    "        url (str): URL to load pdf document from.\n",
    "    Returns:\n",
    "        list: A list of loaded pdf documents.\n",
    "    \"\"\"\n",
    "    loader = UnstructuredURLLoader(urls=[url])\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_htmls(urls, extra_loaders=None):\n",
    "    \"\"\"\n",
    "    Load HTML documents from the given URLs.\n",
    "    Args:\n",
    "        urls (list): A list of URLs to load HTML documents from.\n",
    "    Returns:\n",
    "        list: A list of loaded HTML documents.\n",
    "    \"\"\"\n",
    "    if extra_loaders is None:\n",
    "        extra_loaders = []\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "        if url.endswith('.pdf'):\n",
    "            if 'pdf' in extra_loaders:\n",
    "                docs.extend(load_remote_pdf(url))\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            loader = AsyncHtmlLoader(url, verify_ssl=False)\n",
    "            docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_filter(all_links, excluded_links):\n",
    "    \"\"\"\n",
    "    Filters a list of links based on a list of excluded links.\n",
    "    Args:\n",
    "        all_links (List[str]): A list of links to filter.\n",
    "        excluded_links (List[str]): A list of excluded links.\n",
    "    Returns:\n",
    "        Set[str]: A list of filtered links.\n",
    "    \"\"\"\n",
    "    clean_excluded_links = set()\n",
    "    for excluded_link in excluded_links:\n",
    "        parsed_link = urlparse(excluded_link)\n",
    "        clean_excluded_links.add(parsed_link.netloc + parsed_link.path)\n",
    "    filtered_links = set()\n",
    "    for link in all_links:\n",
    "        # Check if the link contains any of the excluded links\n",
    "        if not any(excluded_link in link for excluded_link in clean_excluded_links):\n",
    "            filtered_links.add(link)\n",
    "    return filtered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_docs(docs):\n",
    "    \"\"\"\n",
    "    Clean the given HTML documents by transforming them into plain text.\n",
    "    Args:\n",
    "        docs (list): A list of langchain documents with html content to clean.\n",
    "    Returns:\n",
    "        list: A list of cleaned plain text documents.\n",
    "    \"\"\"\n",
    "    html2text_transformer = Html2TextTransformer()\n",
    "    docs = html2text_transformer.transform_documents(documents=docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_crawl(urls, excluded_links=None):\n",
    "    \"\"\"\n",
    "    Perform web crawling, retrieve and clean HTML documents from the given URLs, with specified depth of exploration.\n",
    "    Args:\n",
    "        urls (list): A list of URLs to crawl.\n",
    "        excluded_links (list, optional): A list of links to exclude from crawling. Defaults to None.\n",
    "        depth (int, optional): The depth of crawling, determining how many layers of internal links to explore. Defaults to 1\n",
    "    Returns:\n",
    "        tuple: A tuple containing the langchain documents (list) and the scrapped URLs (list).\n",
    "    \"\"\"\n",
    "    *_, web_crawling_params, extra_loaders = get_config_info()\n",
    "    if excluded_links is None:\n",
    "        excluded_links = []\n",
    "    excluded_links.extend(\n",
    "        [\n",
    "            'facebook.com',\n",
    "            'twitter.com',\n",
    "            'instagram.com',\n",
    "            'linkedin.com',\n",
    "            'telagram.me',\n",
    "            'reddit.com',\n",
    "            'whatsapp.com',\n",
    "            'wa.me',\n",
    "        ]\n",
    "    )\n",
    "    excluded_link_suffixes = {'.ico', '.svg', '.jpg', '.png', '.jpeg', '.', '.docx', '.xls', '.xlsx'}\n",
    "    scrapped_urls = []\n",
    "\n",
    "    urls = [url for url in urls if not url.endswith(tuple(excluded_link_suffixes))]\n",
    "    urls = link_filter(urls, set(excluded_links))\n",
    "    urls = list(urls)[: web_crawling_params['max_scraped_websites']]\n",
    "\n",
    "    scraped_docs = load_htmls(urls, extra_loaders)\n",
    "    scrapped_urls.append(urls)\n",
    "\n",
    "    docs = clean_docs(scraped_docs)\n",
    "    return docs, scrapped_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, links = querySerpapi('Who is the president of USA', engine='google', do_analysis=False)\n",
    "docs, links = web_crawl(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieval and vdb creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vectordb.vector_db import VectorDb\n",
    "\n",
    "vectordb = VectorDb()\n",
    "config = {'persist_directory': 'NoneDirectory'}\n",
    "documents = docs\n",
    "\n",
    "def create_and_save_local(input_directory=None, persist_directory=None, update=False):\n",
    "    *_, retrieval_info, _, _ = get_config_info()\n",
    "    persist_directory = config.get('persist_directory', 'NoneDirectory')\n",
    "\n",
    "    chunks = vectordb.get_text_chunks(documents, retrieval_info['chunk_size'], retrieval_info['chunk_overlap'])\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"BAAI/bge-large-en\",\n",
    "        embed_instruction=\"\",  # no instruction is needed for candidate passages\n",
    "        query_instruction=\"Represent this paragraph for searching relevant passages: \",\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    if update and os.path.exists(persist_directory):\n",
    "        config['update'] = True\n",
    "        vector_store = vectordb.update_vdb(\n",
    "            chunks, embeddings, retrieval_info['db_type'], input_directory, persist_directory\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        if os.path.exists(persist_directory):\n",
    "            vector_store = vectordb.create_vector_store(\n",
    "                chunks, embeddings, retrieval_info['db_type'], persist_directory\n",
    "            )\n",
    "        else:\n",
    "            vector_store = vectordb.create_vector_store(chunks, embeddings, retrieval_info['db_type'], None)\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "\n",
    "vector_store = create_and_save_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "\n",
    "def retrieval_qa_chain():\n",
    "    *_, retrieval_info, _, _ = get_config_info()\n",
    "    prompt = load_prompt(os.path.join(kit_dir, 'prompts/llama3-web_scraped_data_retriever.yaml'))\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type='similarity_score_threshold',\n",
    "        search_kwargs={\n",
    "            'score_threshold': retrieval_info['score_treshold'],\n",
    "            'k': retrieval_info['k_retrieved_documents'],\n",
    "        },\n",
    "    )\n",
    "    qa_chain = RetrievalQA.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        verbose=True,\n",
    "        input_key='question',\n",
    "        output_key='answer',\n",
    "        prompt=prompt,\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval_qa_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke('who is joe biden')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
