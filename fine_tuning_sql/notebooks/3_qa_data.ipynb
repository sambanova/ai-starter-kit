{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: Basic QA-QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes over a way to compare the downloaded jsonl files data with the generated .hdf5, and getting some metrics in the training files before uploading them to Sambastudio.\n",
    "These steps shown in this Notebook can be omitted if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import h5py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# Set the id_model with the tokenizer to use\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning nstext2sql dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare jsonl and hdf5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(kit_dir, \"data\", \"output\", \"fine-tune-nstext2sql\" )\n",
    "\n",
    "sql_files = sorted(glob.glob(f'{data_dir}/splits/*.jsonl'))\n",
    "hdf_files = sorted(glob.glob(f'{data_dir}/*.hdf5'))\n",
    "\n",
    "for sql_file, hdf_file in zip(sql_files, hdf_files):\n",
    "\n",
    "    sql_data = []\n",
    "    # Read sql file\n",
    "    with open(sql_file) as f:\n",
    "        for line in f:\n",
    "            sql_data.append(json.loads(line))\n",
    "    \n",
    "    hdf_data = []\n",
    "    # Read hdf file\n",
    "    f = h5py.File(hdf_file, 'r')\n",
    "    for i in range(f['input_ids'].shape[0]):\n",
    "        hdf_data.append(tokenizer.decode(f['input_ids'][i]))\n",
    "    \n",
    "    break\n",
    "\n",
    "rows_to_compare = 5\n",
    "for i, d_sql, d_hdf in zip(range(rows_to_compare),sql_data[:5], hdf_data[:5]):\n",
    "    print(f\"{i}\")\n",
    "    print(\"jsonl row:\")\n",
    "    print(f\"Prompt:\\n{d_sql['prompt']}\\nCompletion:\\n{d_sql['completion']}\\n\")\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(\"hdf5:\\n\")\n",
    "    print(f\"{d_hdf}\")\n",
    "    print('----------------------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics from hdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sequences = 0\n",
    "ocurrences = 0\n",
    "for hdf_file in hdf_files:\n",
    "    f = h5py.File(hdf_file, 'r')\n",
    "    total_sequences += f['input_ids'].shape[0]\n",
    "    for i in range(f['input_ids'].shape[0]):\n",
    "        text = tokenizer.decode(f['input_ids'][i,])\n",
    "        occurences += text.count('</s>')\n",
    "print(f'total_sequences: {total_sequences}')\n",
    "print(f'occurrences: {occurences}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain squad-smol-sql dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare jsonl and hdf5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(kit_dir, \"data\", \"output\", \"pretrain-squad-smol-sql\" )\n",
    "\n",
    "sql_files = sorted(glob.glob(f'{data_dir}/splits/*.jsonl'))\n",
    "hdf_files = sorted(glob.glob(f'{data_dir}/*.hdf5'))\n",
    "\n",
    "for sql_file, hdf_file in zip(sql_files, hdf_files):\n",
    "\n",
    "    sql_data = []\n",
    "    # Read sql file\n",
    "    with open(sql_file) as f:\n",
    "        for line in f:\n",
    "            sql_data.append(json.loads(line))\n",
    "    \n",
    "    hdf_data = []\n",
    "    # Read hdf file\n",
    "    f = h5py.File(hdf_file, 'r')\n",
    "    for i in range(f['input_ids'].shape[0]):\n",
    "        hdf_data.append(tokenizer.decode(f['input_ids'][i]))\n",
    "    \n",
    "    break\n",
    "\n",
    "rows_to_compare = 5\n",
    "for i, d_sql, d_hdf in zip(range(rows_to_compare),sql_data[:5], hdf_data[:5]):\n",
    "    print(f\"{i}\")\n",
    "    print(\"jsonl row:\")\n",
    "    print(f\"Prompt:\\n{d_sql['prompt']}\\nCompletion:\\n{d_sql['completion']}\\n\")\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(\"hdf5:\\n\")\n",
    "    print(f\"{d_hdf}\")\n",
    "    print('----------------------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics from hdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sequences = 0\n",
    "ocurrences = 0\n",
    "for hdf_file in hdf_files:\n",
    "    f = h5py.File(hdf_file, 'r')\n",
    "    total_sequences += f['input_ids'].shape[0]\n",
    "    for i in range(f['input_ids'].shape[0]):\n",
    "        text = tokenizer.decode(f['input_ids'][i,])\n",
    "        occurences += text.count('</s>')\n",
    "print(f'total_sequences: {total_sequences}')\n",
    "print(f'occurrences: {occurences}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain stack dedup dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare jsonl and hdf5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(kit_dir, \"data\", \"output\", \"pretrain-the-stack-dedup\")\n",
    "\n",
    "sql_files = sorted(glob.glob(f'{data_dir}/splits/*.jsonl'))\n",
    "hdf_files = sorted(glob.glob(f'{data_dir}/*.hdf5'))\n",
    "\n",
    "for sql_file, hdf_file in zip(sql_files, hdf_files):\n",
    "\n",
    "    sql_data = []\n",
    "    # Read sql file\n",
    "    with open(sql_file) as f:\n",
    "        for line in f:\n",
    "            sql_data.append(json.loads(line))\n",
    "    \n",
    "    hdf_data = []\n",
    "    # Read hdf file\n",
    "    f = h5py.File(hdf_file, 'r')\n",
    "    for i in range(f['input_ids'].shape[0]):\n",
    "        hdf_data.append(tokenizer.decode(f['input_ids'][i]))\n",
    "    \n",
    "    break\n",
    "\n",
    "rows_to_compare = 5\n",
    "for i, d_sql, d_hdf in zip(range(rows_to_compare),sql_data[:5], hdf_data[:5]):\n",
    "    print(f\"{i}\")\n",
    "    print(\"jsonl row:\")\n",
    "    print(f\"Prompt:\\n{d_sql['prompt']}\\nCompletion:\\n{d_sql['completion']}\\n\")\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(\"hdf5:\\n\")\n",
    "    print(f\"{d_hdf}\")\n",
    "    print('----------------------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics from hdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sequences = 0\n",
    "ocurrences = 0\n",
    "for hdf_file in hdf_files:\n",
    "    f = h5py.File(hdf_file, 'r')\n",
    "    total_sequences += f['input_ids'].shape[0]\n",
    "    for i in range(f['input_ids'].shape[0]):\n",
    "        text = tokenizer.decode(f['input_ids'][i,])\n",
    "        occurences += text.count('</s>')\n",
    "print(f'total_sequences: {total_sequences}')\n",
    "print(f'occurrences: {occurences}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sn_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
