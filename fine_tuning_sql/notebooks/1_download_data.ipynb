{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Download data\n",
    "This notebook goes over data download for SQL model [Pretraining](#pretraining-dataset) and [Fine-tuning](#fine-tunning-dataset)\n",
    "\n",
    "We recommend you going directly to the [Fine-tuning](#fine-tunning-dataset) if you want to use as base model an instruct model like Llama-7b-Instruct-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining dataset\n",
    "Here you will find two datasets for pretraining. The stack-smol dataset that can be used for testing with small subset, but for a full pretraining job you should use the Full squad Stack-dedup dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack smol dataset for SQL test Pretraining\n",
    "- [bigcode/the-stack-smol](https://huggingface.co/datasets/bigcode/the-stack-smol)\n",
    "- Total samples: 155813472\n",
    "- Total dataset size 902MB\n",
    "- SQL dataset Size 158MB\n",
    "- The jsonl files contains ```prompt``` and ```completions``` pairs per sample. For fine-tuning, both ```prompt``` and ```completions``` fields are populated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For downloading the dataset you will need to request access in [huggingface bigcode/the-stack-smol](https://huggingface.co/datasets/bigcode/the-stack-smol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# specific language: SQL\n",
    "pretrain_dataset = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/sql\", split=\"train\")\n",
    "\n",
    "prompt = [\"\"] * len(pretrain_dataset)\n",
    "pretrain_dataset = pretrain_dataset.rename_columns({'content': 'completion'}).select_columns('completion').add_column(\"prompt\", prompt)\n",
    "pretrain_dataset.to_json(os.path.join(kit_dir,\"data/pre-training/pretrain-squad-smol-sql.jsonl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full squad Stack-dedub dataset for SQL Pretraining\n",
    "\n",
    "- [bigcode/the-stack-dedup](https://huggingface.co/datasets/bigcode/the-stack-dedup)\n",
    "- Total samples: 13669625293\n",
    "- Total dataset size 996GB\n",
    "- SQL dataset size 4.3GB \n",
    "- The jsonl files contains ```prompt``` and ```completions``` pairs per sample. For pretraining, ```prompt``` field is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For downloading the dataset you will need to request access in [huggingface bigcode/the-stack-dedup](https://huggingface.co/datasets/bigcode/the-stack-dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# specific language: SQL\n",
    "pretrain_dataset = load_dataset(\"bigcode/the-stack-dedup\", data_dir=\"data/sql\", split=\"train\")\n",
    "\n",
    "prompt = [\"\"] * len(pretrain_dataset)\n",
    "pretrain_dataset = pretrain_dataset.rename_columns({'content': 'completion'}).select_columns('completion').add_column(\"prompt\", prompt)\n",
    "pretrain_dataset.to_json(os.path.join(kit_dir,\"data/pre-training/pretrain-the-stack-dedup.jsonl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSText2SQL dataset for SQL Fine-tuning\n",
    "- [NSText2SQL](https://huggingface.co/datasets/NumbersStation/NSText2SQL)\n",
    "- Total samples: 289288\n",
    "- Dataset size 433MB\n",
    "- The jsonl files contains ```prompt``` and ```completions``` pairs per sample. For fine-tuning, both ```prompt``` and ```completions``` fields are populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fine_tune_dataset = load_dataset(\"NumbersStation/NSText2SQL\", split=\"train\")\n",
    "fine_tune_dataset = fine_tune_dataset.rename_columns({'instruction': 'prompt', 'output': 'completion'}).select_columns(['prompt', 'completion'])\n",
    "fine_tune_dataset.to_json(os.path.join(kit_dir,\"data/fine-tuning/fine-tune-nstext2sql.jsonl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "After this stage we should have 2 jsonl files in the [data folder](../data/), one for pretraining and other for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other recipes\n",
    "\n",
    "- [Anyscale](https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications): Used [b-mc2/sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset from Hugging Face, which is a combination of the [WikiSQL](https://huggingface.co/datasets/wikisql) and [Spider](https://huggingface.co/datasets/spider) datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
