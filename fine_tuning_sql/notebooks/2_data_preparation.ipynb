{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Generate training files and train the model in SambaStudio\n",
    "\n",
    "This notebook is a guide of how to convert your jsonl files to hdf5 files that are the files you will need to upload in order to start a training job in Sambastudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative data preparation package dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [generative Data preparation package]() is a package developed by Sambanova systems used to prepare your data to be used as dataset in your SambaStudio Environment, this package is included as git submodule in the AI Starter kit, so for instalations you should onlY execute teh following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_data_prep_dir = os.path.join(repo_dir, \"utils\", \"fine_tuning\", \"generative_data_prep\")\n",
    "! pip install $generative_data_prep_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative data preparation usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dataset generation you will need to specify the tokenizer of the model you want to train setting its model id in the `TOKENIZER` variable\n",
    "- [Llama 2](https://huggingface.co/meta-llama/Llama-2-7b-hf): meta-llama/Llama-2-7b-hf\n",
    "- [Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B): meta-llama/Meta-Llama-3-8B\n",
    "- [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2): mistralai/Mistral-7B-Instruct-v0.2\n",
    "\n",
    "> You will need to request access to the models to get the tokenizers in their HuggingFace spaces  [Llama 2](https://huggingface.co/meta-llama/Llama-2-7b-hf), [Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B), [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain with squad-smol-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = os.path.join(kit_dir, \"data\" , \"pre-training\", \"pretrain-squad-smol-sql.jsonl\")\n",
    "OUTPUT_PATH = os.path.join(kit_dir, \"data\", \"output\", \"pretrain-squad-smol-sql\")\n",
    "TOKENIZER = \"meta-llama/Llama-2-7b-hf\"  # set with the model id\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m generative_data_prep pipeline \\\n",
    "--input_file_path=$INPUT_PATH \\\n",
    "--output_path=$OUTPUT_PATH \\\n",
    "--pretrained_tokenizer=$TOKENIZER \\\n",
    "--max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "--shuffle=on_RAM \\\n",
    "--keep_split_jsonls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain with the stack dedup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = os.path.join(kit_dir, \"data\" , \"pre-training\", \"pretrain-the-stack-dedup.jsonl\")\n",
    "OUTPUT_PATH = os.path.join(kit_dir, \"data\", \"output\", \"pretrain-the-stack-dedup\")\n",
    "TOKENIZER = \"meta-llama/Llama-2-7b-hf\" \n",
    "MAX_SEQ_LENGTH = 4096\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m generative_data_prep pipeline \\\n",
    "--input_file_path=$INPUT_PATH \\\n",
    "--output_path=$OUTPUT_PATH \\\n",
    "--pretrained_tokenizer=$TOKENIZER \\\n",
    "--max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "--shuffle=on_RAM \\\n",
    "--keep_split_jsonls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune with the NSText2SQL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = os.path.join(kit_dir, \"data\" , \"fine-tuning\", \"fine-tune-nstext2sql.jsonl\")\n",
    "OUTPUT_PATH = os.path.join(kit_dir, \"data\", \"output\", \"fine-tune-nstext2sql\")\n",
    "TOKENIZER = \"meta-llama/Llama-2-7b-hf\" \n",
    "MAX_SEQ_LENGTH = 4096\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m generative_data_prep pipeline \\\n",
    "--input_file_path=$INPUT_PATH \\\n",
    "--output_path=$OUTPUT_PATH \\\n",
    "--pretrained_tokenizer=$TOKENIZER \\\n",
    "--max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "--shuffle=on_RAM \\\n",
    "--keep_split_jsonls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> find more details of the data prepariton proces in the [Generative data preparation Readme](../../utils/fine_tuning/generative_data_prep/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
