{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart - Inference\n",
    "\n",
    "This notebook covers a simple client usage, including the following points:\n",
    "- List available models.\n",
    "- Use the SambaNova inference adaptor to interact with cloud-based LLM chat models.\n",
    "- Implement a chat loop conversation using the SambaNova inference adaptor.\n",
    "\n",
    "Run inference via chat completions with the llama-stack Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from llama_stack_client import LlamaStackClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HTTP client\n",
    "client = LlamaStackClient(base_url=f\"http://localhost:{os.environ['LLAMA_STACK_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available models: ---\n",
      "- sambanova/Meta-Llama-3.1-8B-Instruct\n",
      "- meta-llama/Llama-3.1-8B-Instruct\n",
      "- sambanova/Meta-Llama-3.1-70B-Instruct\n",
      "- meta-llama/Llama-3.1-70B-Instruct\n",
      "- sambanova/Meta-Llama-3.1-405B-Instruct\n",
      "- meta-llama/Llama-3.1-405B-Instruct-FP8\n",
      "- sambanova/Meta-Llama-3.2-1B-Instruct\n",
      "- meta-llama/Llama-3.2-1B-Instruct\n",
      "- sambanova/Meta-Llama-3.2-3B-Instruct\n",
      "- meta-llama/Llama-3.2-3B-Instruct\n",
      "- sambanova/Meta-Llama-3.3-70B-Instruct\n",
      "- meta-llama/Llama-3.3-70B-Instruct\n",
      "- sambanova/Llama-3.2-11B-Vision-Instruct\n",
      "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
      "- sambanova/Llama-3.2-90B-Vision-Instruct\n",
      "- meta-llama/Llama-3.2-90B-Vision-Instruct\n",
      "- sambanova/Meta-Llama-Guard-3-8B\n",
      "- meta-llama/Llama-Guard-3-8B\n",
      "- all-MiniLM-L6-v2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"- {m.identifier}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an inference model from the previous list\n",
    "model = \"sambanova/Meta-Llama-3.3-70B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Chat Completion Request\n",
    "Use the ``chat_completion function to define the conversation context. Each message you include should have a specific role and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With gentle eyes and a soft, fuzzy face, the llama roams the Andes with a peaceful, gentle pace. Its long neck bends as it grazes with glee, a symbol of serenity in a world wild and free.\n"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a two-sentence poem about llama.\"},\n",
    "    ],\n",
    "    model_id=model,\n",
    ")\n",
    "\n",
    "print(response.completion_message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Loop\n",
    "To create a continuous conversation loop, where users can input multiple messages in a session, use the following structure. This example runs an asynchronous loop, ending when the user types 'exit,' 'quit,' or 'bye.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m> Response: I can be used in a variety of ways, from helping you plan a vacation to creating art. I'm here to assist you in finding the help or information you need. My strengths include answering questions, generating text and images and even just chatting with you.\u001b[0m\n",
      "\u001b[33mEnding conversation. Goodbye!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "async def chat_loop():\n",
    "    while True:\n",
    "        user_input = input(\"User> \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            cprint(\"Ending conversation. Goodbye!\", \"yellow\")\n",
    "            break\n",
    "\n",
    "        message = {\"role\": \"user\", \"content\": user_input}\n",
    "        response = client.inference.chat_completion(messages=[message], model_id=model)\n",
    "        cprint(f\"> Response: {response.completion_message.content}\", \"cyan\")\n",
    "\n",
    "\n",
    "# Run the chat loop in a Jupyter Notebook cell using await\n",
    "await chat_loop()\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(chat_loop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation History\n",
    "Maintaining a conversation history allows the model to retain context from previous interactions. Use a list to accumulate messages, enabling continuity throughout the chat session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m> Response: I can be used in a variety of ways, from helping you plan a vacation to creating art. I'm here to assist you in finding the help or information you need. My strengths include answering questions, generating text and images and even just chatting with you.\u001b[0m\n",
      "\u001b[33mEnding conversation. Goodbye!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async def chat_loop():\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        user_input = input(\"User> \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            cprint(\"Ending conversation. Goodbye!\", \"yellow\")\n",
    "            break\n",
    "\n",
    "        user_message = {\"role\": \"user\", \"content\": user_input}\n",
    "        conversation_history.append(user_message)\n",
    "\n",
    "        response = client.inference.chat_completion(\n",
    "            messages=conversation_history,\n",
    "            model_id=model,\n",
    "        )\n",
    "        cprint(f\"> Response: {response.completion_message.content}\", \"cyan\")\n",
    "\n",
    "        # Append the assistant message with all required fields\n",
    "        assistant_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": response.completion_message.content,\n",
    "            # Add any additional required fields here if necessary\n",
    "        }\n",
    "        conversation_history.append(assistant_message)\n",
    "\n",
    "\n",
    "# Use `await` in the Jupyter Notebook cell to call the function\n",
    "await chat_loop()\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(chat_loop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "Llama Stack offers a stream parameter in the chat_completion function, which allows partial responses to be returned progressively as they are generated. This can enhance user experience by providing immediate feedback without waiting for the entire response to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser> Please write me a 3 sentence poem about llamas.\u001b[0m\n",
      "\u001b[36mAssistant> \u001b[0m\u001b[33mHere is a 3 sentence poem about llamas:\n",
      "\u001b[0m\u001b[33mLlamas roam the Andean \u001b[0m\u001b[33mhighlands with \u001b[0m\u001b[33mgentle ease, their soft fur \u001b[0m\u001b[33ma warm and \u001b[0m\u001b[33mfuzzy breeze. \u001b[0m\u001b[33mWith ears \u001b[0m\u001b[33mso tall and eyes so bright, they watch the \u001b[0m\u001b[33mworld \u001b[0m\u001b[33mwith quiet delight. In \u001b[0m\u001b[33mtheir tranquil presence, all \u001b[0m\u001b[33mworries cease, and \u001b[0m\u001b[33mpeace \u001b[0m\u001b[33mdescends like \u001b[0m\u001b[33ma \u001b[0m\u001b[33msoft, \u001b[0m\u001b[33mllama-filled release.\u001b[0m\u001b[97m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "async def run_main(stream: bool = True):\n",
    "    message = {\"role\": \"user\", \"content\": \"Please write me a 3 sentence poem about llamas.\"}\n",
    "    cprint(f'User> {message[\"content\"]}', \"green\")\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=[message],\n",
    "        model_id=model,\n",
    "        stream=stream,\n",
    "    )\n",
    "\n",
    "    if not stream:\n",
    "        cprint(f\"> Response: {response.completion_message.content}\", \"cyan\")\n",
    "    else:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "\n",
    "\n",
    "# In a Jupyter Notebook cell, use `await` to call the function\n",
    "await run_main()\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(run_main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
