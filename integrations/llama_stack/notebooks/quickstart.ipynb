{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart - Inference\n",
    "Run inference via chat completions with llama-stack Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from llama_stack_client import LlamaStackClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HTTP client\n",
    "client = LlamaStackClient(base_url=f\"http://localhost:{os.environ['LLAMA_STACK_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available models: ---\n",
      "- Llama-3.2-11B-Vision-Instruct\n",
      "- Llama-3.2-90B-Vision-Instruct\n",
      "- Meta-Llama-3.1-405B-Instruct\n",
      "- Meta-Llama-3.1-70B-Instruct\n",
      "- Meta-Llama-3.1-8B-Instruct\n",
      "- Meta-Llama-3.2-1B-Instruct\n",
      "- Meta-Llama-3.2-3B-Instruct\n",
      "- Meta-Llama-3.3-70B-Instruct\n",
      "- Meta-Llama-Guard-3-8B\n",
      "- all-MiniLM-L6-v2\n",
      "- meta-llama/Llama-3.1-405B-Instruct-FP8\n",
      "- meta-llama/Llama-3.1-70B-Instruct\n",
      "- meta-llama/Llama-3.1-8B-Instruct\n",
      "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
      "- meta-llama/Llama-3.2-1B-Instruct\n",
      "- meta-llama/Llama-3.2-3B-Instruct\n",
      "- meta-llama/Llama-3.2-90B-Vision-Instruct\n",
      "- meta-llama/Llama-3.3-70B-Instruct\n",
      "- meta-llama/Llama-Guard-3-8B\n",
      "- sambanova/Llama-3.2-11B-Vision-Instruct\n",
      "- sambanova/Llama-3.2-90B-Vision-Instruct\n",
      "- sambanova/Meta-Llama-3.1-405B-Instruct\n",
      "- sambanova/Meta-Llama-3.1-70B-Instruct\n",
      "- sambanova/Meta-Llama-3.1-8B-Instruct\n",
      "- sambanova/Meta-Llama-3.2-1B-Instruct\n",
      "- sambanova/Meta-Llama-3.2-3B-Instruct\n",
      "- sambanova/Meta-Llama-3.3-70B-Instruct\n",
      "- sambanova/Meta-Llama-Guard-3-8B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"- {m.identifier}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Chat Completion Request\n",
    "Use the chat_completion function to define the conversation context. Each message you include should have a specific role and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an inference model from the previous list\n",
    "inference_model = \"sambanova/Meta-Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With gentle eyes and soft, fuzzy hair,\n",
      "The llama roams, a gentle, peaceful air.\n"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a two-sentence poem about llama.\"},\n",
    "    ],\n",
    "    model_id=inference_model,\n",
    ")\n",
    "\n",
    "print(response.completion_message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Loop\n",
    "To create a continuous conversation loop, where users can input multiple messages in a session, use the following structure. This example runs an asynchronous loop, ending when the user types 'exit,' 'quit,' or 'bye.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m> Response: I can be used in a variety of ways, so feel free to ask me anything that's on your mind. Here are some suggestions to get you started:\n",
      "\n",
      "1. **Learning a new skill**: I can provide information and guidance on a wide range of topics, from science and history to culture and entertainment.\n",
      "2. **Research assistance**: I can help you find information on a particular topic or answer specific questions you may have.\n",
      "3. **Language practice**: I can practice conversing with you in a language you're learning.\n",
      "4. **Writing and proofreading**: I can help you generate text or edit what you've written.\n",
      "5. **Conversation and entertainment**: We can chat about your day, interests, or hobbies.\n",
      "6. **Brainstorming and ideas**: I can help generate ideas or suggestions for projects, problems, or creative pursuits.\n",
      "7. **Jokes and humor**: I can try to make you laugh with a joke or a funny story.\n",
      "8. **Trivia and games**: We can play text-based games like Hangman, 20 Questions, or Word Jumble.\n",
      "9. **Recommendations**: I can suggest books, movies, music, or other media based on your interests.\n",
      "10. **Just talking**: I'm here to listen and chat if you need someone to talk to.\n",
      "\n",
      "What sounds interesting to you?\u001b[0m\n",
      "\u001b[33mEnding conversation. Goodbye!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "model = \"sambanova/Meta-Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "async def chat_loop():\n",
    "    while True:\n",
    "        user_input = input(\"User> \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            cprint(\"Ending conversation. Goodbye!\", \"yellow\")\n",
    "            break\n",
    "\n",
    "        message = {\"role\": \"user\", \"content\": user_input}\n",
    "        response = client.inference.chat_completion(messages=[message], model_id=model)\n",
    "        cprint(f\"> Response: {response.completion_message.content}\", \"cyan\")\n",
    "\n",
    "\n",
    "# Run the chat loop in a Jupyter Notebook cell using await\n",
    "await chat_loop()\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(chat_loop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation History\n",
    "Maintaining a conversation history allows the model to retain context from previous interactions. Use a list to accumulate messages, enabling continuity throughout the chat session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m> Response: I can be used in a variety of ways, so feel free to ask me anything that's on your mind. Here are some suggestions to get you started:\n",
      "\n",
      "1. **Learning a new skill**: I can provide information and guidance on a wide range of topics, from science and history to culture and entertainment.\n",
      "2. **Research assistance**: I can help you find information on a particular topic or answer specific questions you may have.\n",
      "3. **Language practice**: I can practice conversing with you in a language you're learning.\n",
      "4. **Writing and proofreading**: I can help you generate text or edit what you've written.\n",
      "5. **Conversation and entertainment**: We can chat about your day, interests, or hobbies.\n",
      "6. **Brainstorming and ideas**: I can help generate ideas or suggestions for projects, problems, or creative pursuits.\n",
      "7. **Jokes and humor**: I can try to make you laugh with a joke or a funny story.\n",
      "8. **Trivia and games**: We can play text-based games like Hangman, 20 Questions, or Word Jumble.\n",
      "9. **Recommendations**: I can suggest books, movies, music, or other media based on your interests.\n",
      "10. **Just talking**: I'm here to listen and chat if you need someone to talk to.\n",
      "\n",
      "What sounds interesting to you?\u001b[0m\n",
      "\u001b[33mEnding conversation. Goodbye!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = \"sambanova/Meta-Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "async def chat_loop():\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        user_input = input(\"User> \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            cprint(\"Ending conversation. Goodbye!\", \"yellow\")\n",
    "            break\n",
    "\n",
    "        user_message = {\"role\": \"user\", \"content\": user_input}\n",
    "        conversation_history.append(user_message)\n",
    "\n",
    "        response = client.inference.chat_completion(\n",
    "            messages=conversation_history,\n",
    "            model_id=model,\n",
    "        )\n",
    "        cprint(f\"> Response: {response.completion_message.content}\", \"cyan\")\n",
    "\n",
    "        # Append the assistant message with all required fields\n",
    "        assistant_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": response.completion_message.content,\n",
    "            # Add any additional required fields here if necessary\n",
    "        }\n",
    "        conversation_history.append(assistant_message)\n",
    "\n",
    "\n",
    "# Use `await` in the Jupyter Notebook cell to call the function\n",
    "await chat_loop()\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(chat_loop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "Llama Stack offers a stream parameter in the chat_completion function, which allows partial responses to be returned progressively as they are generated. This can enhance user experience by providing immediate feedback without waiting for the entire response to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser> Please write me a 3 sentence poem about llamas.\u001b[0m\n",
      "\u001b[36mAssistant> \u001b[0m\u001b[33mHere's a 3-sentence poem about llamas:\n",
      "\n",
      "With gentle eyes and soft fur so bright,\n",
      "\u001b[0m\u001b[33mThe llama roams, a gentle, peaceful sight.\n",
      "Their soft humming calls, a \u001b[0m\u001b[33msoothing delight.\u001b[0m\u001b[97m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "model = \"sambanova/Meta-Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "async def run_main(stream: bool = True):\n",
    "    message = {\"role\": \"user\", \"content\": \"Please write me a 3 sentence poem about llamas.\"}\n",
    "    cprint(f'User> {message[\"content\"]}', \"green\")\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=[message],\n",
    "        model_id=model,\n",
    "        stream=stream,\n",
    "    )\n",
    "\n",
    "    if not stream:\n",
    "        cprint(f\"> Response: {response.completion_message.content}\", \"cyan\")\n",
    "    else:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "\n",
    "\n",
    "# In a Jupyter Notebook cell, use `await` to call the function\n",
    "await run_main()\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(run_main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
