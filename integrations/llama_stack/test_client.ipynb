{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import mimetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_http_client():\n",
    "    from llama_stack_client import LlamaStackClient\n",
    "\n",
    "    return LlamaStackClient(base_url=f\"http://localhost:{os.environ['LLAMA_STACK_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = create_http_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"sambanova/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about coding\"},\n",
    "    ],\n",
    ")\n",
    "print(response)\n",
    "print(response.completion_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_url_from_image(file_path):\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    if mime_type is None:\n",
    "        raise ValueError(\"Could not determine MIME type of the file\")\n",
    "\n",
    "    with open(file_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    data_url = f\"data:{mime_type};base64,{encoded_string}\"\n",
    "    return data_url\n",
    "\n",
    "\n",
    "data_url_from_image(\"../../images/SambaNova-dark-logo-1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models():\n",
    "    models = []\n",
    "    for model in client.models.list():\n",
    "        models.append(model.identifier)\n",
    "    return models\n",
    "\n",
    "\n",
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_llm_text_only(stream: bool):\n",
    "    model_ids = list_models()\n",
    "    print(\"========== Inference: Text Only ==========\")\n",
    "    assert len(model_ids) > 0\n",
    "    for model_id in model_ids:\n",
    "        if \"guard\" not in model_id.lower() and \"sambanova\" in model_id:\n",
    "            print(f\">>>>> Sending request to {model_id}\")\n",
    "            iterator = client.inference.chat_completion(\n",
    "                model_id=model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": \"Write a haiku on llamas\"},\n",
    "                ],\n",
    "                stream=stream,\n",
    "            )\n",
    "            if stream:\n",
    "                print(\"<<<<< Streaming Response\")\n",
    "                text = \"\"\n",
    "                for chunk in iterator:\n",
    "                    print(f\"{chunk.event.delta.text}\", end=\"\", flush=True)\n",
    "                    text += chunk.event.delta.text\n",
    "                assert text != \"\"\n",
    "                print()\n",
    "            else:\n",
    "                print(\"<<<<< Non-streaming Response\")\n",
    "                print(f\"Type: {type(iterator.completion_message.content)}, Value:{iterator.completion_message.content}\")\n",
    "                assert iterator.completion_message.content != \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference_llm_text_only(stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference_llm_text_only(stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_llm_text_tool(stream: bool):\n",
    "    model_ids = [\n",
    "        # \"sambanova/Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"sambanova/Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"sambanova/Meta-Llama-3.1-405B-Instruct\",\n",
    "        \"sambanova/Meta-Llama-3.3-70B-Instruct\",\n",
    "    ]\n",
    "\n",
    "    print(\"========== Inference: Text and Tool ==========\")\n",
    "\n",
    "    assert len(model_ids) > 0\n",
    "    for model_id in model_ids:\n",
    "        print(f\">>>>> Sending request to {model_id}\")\n",
    "        iterator = client.inference.chat_completion(\n",
    "            model_id=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an assistant that can solve quadratic equations given coefficients a, b, and c.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Find all the roots of a quadratic equation given coefficients a = 3, b = -11, and c = -4.\",\n",
    "                },\n",
    "            ],\n",
    "            tools=[\n",
    "                {\n",
    "                    \"tool_name\": \"solve_quadratic\",\n",
    "                    \"description\": \"Solve a quadratic equation given coefficients a, b, and c.\",\n",
    "                    \"parameters\": {\n",
    "                        \"a\": {\n",
    "                            \"param_type\": \"integer\",\n",
    "                            \"description\": \"Coefficient of the squared term.\",\n",
    "                            \"required\": True,\n",
    "                        },\n",
    "                        \"b\": {\n",
    "                            \"param_type\": \"integer\",\n",
    "                            \"description\": \"Coefficient of the linear term.\",\n",
    "                            \"required\": True,\n",
    "                        },\n",
    "                        \"c\": {\"param_type\": \"integer\", \"description\": \"Constant term.\", \"required\": True},\n",
    "                        \"root_type\": {\n",
    "                            \"param_type\": \"string\",\n",
    "                            \"description\": \"Type of roots: 'real' or 'all'.\",\n",
    "                            \"required\": True,\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "            stream=stream,\n",
    "        )\n",
    "\n",
    "        if stream:\n",
    "            print(\"<<<<< Streaming Response\")\n",
    "            for chunk in iterator:\n",
    "                delta = chunk.event.delta\n",
    "                if delta.type == \"tool_call\":\n",
    "                    print(f\"{delta}\")\n",
    "                else:\n",
    "                    print(delta.text)\n",
    "\n",
    "        else:\n",
    "            print(\"<<<<< Non-streaming Response\")\n",
    "            tool_calls = iterator.completion_message.tool_calls\n",
    "            print(tool_calls)\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference_llm_text_tool(stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference_llm_text_tool(stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_llm_text_image(stream: bool):\n",
    "    model_ids = [\"sambanova/Llama-3.2-11B-Vision-Instruct\", \"sambanova/Llama-3.2-90B-Vision-Instruct\"]\n",
    "    data_url = data_url_from_image(\"../../images/SambaNova-dark-logo-1.png\")\n",
    "\n",
    "    print(\"========== Inference: Text and Image ==========\")\n",
    "\n",
    "    assert len(model_ids) > 0\n",
    "    for model_id in model_ids:\n",
    "        print(f\">>>>> Sending request to {model_id}\")\n",
    "        iterator = client.inference.chat_completion(\n",
    "            model_id=model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": {\"type\": \"image\", \"image\": {\"url\": {\"uri\": data_url}}}},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"How many different colors are in this image?\",\n",
    "                },\n",
    "            ],\n",
    "            stream=stream,\n",
    "        )\n",
    "\n",
    "        if stream:\n",
    "            print(\"<<<<< Streaming Response\")\n",
    "            text = \"\"\n",
    "            for chunk in iterator:\n",
    "                if chunk.event is not None:\n",
    "                    print(f\"{chunk.event.delta.text}\", end=\"\", flush=True)\n",
    "                    text += chunk.event.delta.text\n",
    "\n",
    "            assert text != \"\"\n",
    "            print()\n",
    "        else:\n",
    "            print(\"<<<<< Non-streaming Response\")\n",
    "            print(f\"Type: {type(iterator.completion_message.content)}, Value:{iterator.completion_message.content}\")\n",
    "            assert iterator.completion_message.content != \"\"\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference_llm_text_image(stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_llm_image_only(stream: bool):\n",
    "    model_ids = [\"sambanova/Llama-3.2-11B-Vision-Instruct\", \"sambanova/Llama-3.2-11B-Vision-Instruct\"]\n",
    "\n",
    "    data_url = data_url_from_image(\"../../images/SambaNova-dark-logo-1.png\")\n",
    "\n",
    "    print(\"========== Inference: Text and Image ==========\")\n",
    "\n",
    "    assert len(model_ids) > 0\n",
    "    for model_id in model_ids:\n",
    "        print(f\">>>>> Sending request to {model_id}\")\n",
    "        iterator = client.inference.chat_completion(\n",
    "            model_id=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": {\"type\": \"image\", \"image\": {\"url\": {\"uri\": data_url}}}}],\n",
    "            stream=stream,\n",
    "        )\n",
    "\n",
    "        if stream:\n",
    "            print(\"<<<<< Streaming Response\")\n",
    "            text = \"\"\n",
    "            for chunk in iterator:\n",
    "                if chunk.event is not None:\n",
    "                    print(f\"{chunk.event.delta.text}\", end=\"\", flush=True)\n",
    "                    text += chunk.event.delta.text\n",
    "\n",
    "            assert text != \"\"\n",
    "            print()\n",
    "        else:\n",
    "            print(\"<<<<< Non-streaming Response\")\n",
    "            print(f\"Type: {type(iterator.completion_message.content)}, Value:{iterator.completion_message.content}\")\n",
    "            assert iterator.completion_message.content != \"\"\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference_llm_image_only(stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference_llm_image_only(stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_safety_text_only(stream: bool):\n",
    "    model_ids = list_models()\n",
    "    print(\"========== Safety on Inference: Text Only ==========\")\n",
    "    assert len(model_ids) > 0\n",
    "    for model_id in model_ids:\n",
    "        if \"guard\" in model_id.lower() and \"sambanova\" in model_id.lower():\n",
    "            print(f\">>>>> Sending request to {model_id}\")\n",
    "            iterator = client.inference.chat_completion(\n",
    "                model_id=model_id, messages=[{\"role\": \"user\", \"content\": \"Write a haiku on llamas\"}], stream=stream\n",
    "            )\n",
    "\n",
    "            if stream:\n",
    "                print(\"<<<<< Streaming Response\")\n",
    "                text = \"\"\n",
    "                for chunk in iterator:\n",
    "                    if chunk.event is not None:\n",
    "                        print(f\"{chunk.event.delta.text}\", end=\"\", flush=True)\n",
    "                        text += chunk.event.delta.text\n",
    "\n",
    "                assert text != \"\"\n",
    "                print()\n",
    "            else:\n",
    "                print(\"<<<<< Non-streaming Response\")\n",
    "                print(f\"Type: {type(iterator.completion_message.content)}, Value:{iterator.completion_message.content}\")\n",
    "                assert iterator.completion_message.content != \"\"\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference_safety_text_only(stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inference_safety_text_only(stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_safety():\n",
    "    model_ids = [\n",
    "        # \"Meta-Llama-Guard-3-8B\",\n",
    "        \"sambanova/Meta-Llama-Guard-3-8B\"\n",
    "    ]\n",
    "\n",
    "    print(\"========== Safety:Text Only ==========\")\n",
    "\n",
    "    assert len(model_ids) > 0\n",
    "    for model_id in model_ids:\n",
    "        print(f\">>>>> Sending request to {model_id}\")\n",
    "        iterator = client.safety.run_shield(\n",
    "            shield_id=model_id, messages=[{\"role\": \"user\", \"content\": \"how to make a gun\"}], params={}\n",
    "        )\n",
    "\n",
    "        print(\" Response\")\n",
    "        print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_safety()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG example\n",
    "import uuid\n",
    "from termcolor import cprint\n",
    "from llama_stack_client.types import Document\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "urls = [\"chat.rst\", \"llama3.rst\", \"memory_optimizations.rst\", \"lora_finetune.rst\"]\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_providers = [provider for provider in client.providers.list() if provider.api == \"vector_io\"]\n",
    "print(vector_providers)\n",
    "provider_id = vector_providers[0].provider_id  # Use the first available vector provider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a vector database\n",
    "vector_db_id = f\"test-vector-db-{uuid.uuid4().hex}\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    provider_id=provider_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the documents into the vector database\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=\"sambanova/Meta-Llama-3.3-70B-Instruct\",\n",
    "    # Define instructions for the agent ( aka system prompt)\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    enable_session_persistence=False,\n",
    "    # Define tools available to the agent\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\n",
    "                \"vector_db_ids\": [vector_db_id],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = rag_agent.create_session(\"test-session\")\n",
    "\n",
    "user_prompts = [\n",
    "    \"How to optimize memory usage in torchtune? use the knowledge_search tool to get information.\",\n",
    "]\n",
    "\n",
    "# Run the agent loop by calling the `create_turn` method\n",
    "for prompt in user_prompts:\n",
    "    cprint(f\"User> {prompt}\", \"green\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple react agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "\n",
    "\n",
    "# Example tool definition\n",
    "def get_weather(city: str) -> int:\n",
    "    \"\"\"\n",
    "    Runs get weather tool.\n",
    "    returns the temperature in celius\n",
    "    :param city: city\n",
    "    \"\"\"\n",
    "    return 26\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=\"sambanova/Meta-Llama-3.1-70B-Instruct\",\n",
    "    instructions=\"You are a helpful assistant. Use the tools you have access to for providing relevant answers\",\n",
    "    sampling_params={\n",
    "        \"strategy\": {\"type\": \"top_p\", \"temperature\": 1.0, \"top_p\": 0.9},\n",
    "    },\n",
    "    tools=[get_weather],\n",
    "    # input_shields=available_shields if available_shields else [],\n",
    "    # output_shields=available_shields if available_shields else [],\n",
    "    enable_session_persistence=False,\n",
    ")\n",
    "\n",
    "\n",
    "response = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"how is the weather in paris?\"}],\n",
    "    session_id=agent.create_session(\"tool_session\"),\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# print(response.output_message.content)\n",
    "for event in response:\n",
    "    print(event)\n",
    "    if event.event.payload.event_type == \"turn_complete\":\n",
    "        print(\"#####\")\n",
    "        print(event.event.payload.turn.output_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
