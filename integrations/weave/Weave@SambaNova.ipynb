{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736bf04e",
   "metadata": {},
   "source": [
    "# Weave @SambaNova\n",
    "\n",
    "This notebook demonstrates how to use [W&B Weave](https://weave-docs.wandb.ai/) with [SambaNova](https://sambanova.ai/) as your fastest model provider of choice for open source models.\n",
    "\n",
    "\n",
    "In order to use `Weave` with `SambaNova`, you need to set the environment variable `SAMBANOVA_API_KEY`: Your API key for accessing your SambaNova instance. You can create your API key [here](https://cloud.sambanova.ai/apis).\n",
    "\n",
    "To get started, simply call `weave.init()` at the beginning of your script, with the project name as attribute.\n",
    "\n",
    "`Weave` ops make results reproducible by automatically versioning code as you experiment, and they capture their inputs and outputs.\n",
    "\n",
    "Simply create a function decorated with `@weave.op()` that calls into each completion function and Weave will track the inputs and outputs for you. \n",
    "\n",
    "By using the `weave.Model` class, you can capture and organize the experimental details of your app like your system prompt or the model you're using.\n",
    "This helps organize and compare different iterations of your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb4330ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Optional\n",
    "\n",
    "import weave\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d3a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have SAMBANOVA_API_KEY in your .env file\n",
    "SAMBANOVA_API_KEY = os.getenv('SAMBANOVA_API_KEY')\n",
    "\n",
    "# Initialize Weave with your project name\n",
    "model = 'Meta-Llama-3.3-70B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe47689",
   "metadata": {},
   "source": [
    "## Via LangChain\n",
    "\n",
    "`Weave` is designed to make tracking and logging all calls made through the `LangChain` Python library effortless, after `weave.init()` is called.\n",
    "\n",
    "You can access all the features of the `LangChain` + `Weave` integration, by using our `LangChain` chat object, `langchain_sambanova.ChatSambaNovaCloud`.\n",
    "\n",
    "For more details on all the `Weave` features supported by `LangChain`, please refer to [Weave @LangChain](https://weave-docs.wandb.ai/guides/integrations/langchain/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624d9554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: francescar.\n",
      "View Weave data at https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_langchain/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x11b1b2d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_sambanova import ChatSambaNovaCloud\n",
    "\n",
    "# Initialize Weave project\n",
    "weave.init('weave_integration_sambanova_langchain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d84b94",
   "metadata": {},
   "source": [
    "### Simple call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e94709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LangChain SambaNova Chat object\n",
    "llm = ChatSambaNovaCloud(\n",
    "    model=model,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# The prompt template\n",
    "prompt = PromptTemplate.from_template('1 + {number} = ')\n",
    "\n",
    "# The LLM chain\n",
    "llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0fcb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 2 = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_langchain/r/call/019688f6-fb50-7e92-a43a-6ed5af7c15f6\n",
      "üç© https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_langchain/r/call/019688f6-fecb-7f02-a54e-9ff4db50d84b\n"
     ]
    }
   ],
   "source": [
    "# Invoke the LLM on the prompt\n",
    "output = llm_chain.invoke({'number': 2})\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e092f",
   "metadata": {},
   "source": [
    "### Tracking Call Metadata\n",
    "To track metadata from your `LangChain` calls, you can use the `weave.attributes` context manager. This context manager allows you to set custom metadata for a specific block of code, such as a chain or a single request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4914db56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 2 = 3\n"
     ]
    }
   ],
   "source": [
    "# The LLM chain with Weave attributes\n",
    "with weave.attributes({'number_to_increment': 'value'}):\n",
    "    output = llm_chain.invoke({'number': 2})\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952e711",
   "metadata": {},
   "source": [
    "## Via LiteLLM\n",
    "\n",
    "`Weave` automatically tracks and logs LLM calls made via LiteLLM, after `weave.init()` is called.\n",
    "\n",
    "You can access all the features of the `Weave` + `LiteLLM` integration, by specifying the `SambaNova` model name in the `LiteLLM` constructor, as explained in [LiteLLM @SambaNova](https://docs.litellm.ai/docs/providers/sambanova).\n",
    "\n",
    "For more details on all the `Weave` features supported by `LiteLLM`, please refer to [Weave @LiteLLM](https://weave-docs.wandb.ai/guides/integrations/litellm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1339da55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: francescar.\n",
      "View Weave data at https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_litellm/weave\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "\n",
    "# Initialize Weave project\n",
    "weave.init('weave_integration_sambanova_litellm')\n",
    "model_litellm = 'sambanova/' + model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7b868",
   "metadata": {},
   "source": [
    "### Simple call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "816c7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `int` - serialized value may not be as expected [input_value=1746054613.439177, input_type=float])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Error capturing call output:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/op.py\", line 424, in _call_sync_func\n",
      "    res = on_output(res)\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/op.py\", line 407, in on_output\n",
      "    return handler(output, finish, call.inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/op.py\", line 1094, in on_output\n",
      "    wrapped_on_finish(value)\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/op.py\", line 1082, in wrapped_on_finish\n",
      "    on_finish(value, e)\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/op.py\", line 398, in finish\n",
      "    client.finish_call(\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/trace_sentry.py\", line 204, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/weave_client.py\", line 1255, in finish_call\n",
      "    summary = sum_dict_leaves([child.summary or {} for child in call._children])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/weave_client.py\", line 758, in sum_dict_leaves\n",
      "    result[k] = sum_dict_leaves([result.get(k, {}), v])\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/weave_client.py\", line 758, in sum_dict_leaves\n",
      "    result[k] = sum_dict_leaves([result.get(k, {}), v])\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/weave/trace/weave_client.py\", line 760, in sum_dict_leaves\n",
      "    result[k] = result.get(k, 0) + v\n",
      "                ~~~~~~~~~~~~~~~~~^~~\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
      " (subsequent messages of this type will be suppressed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_litellm/r/call/019688f7-051e-7811-967a-1cb64377283e\n",
      "The translation of 'Hello, how are you?' to French is:\n",
      "\n",
      "\"Bonjour, comment allez-vous?\"\n",
      "\n",
      "This is a formal way of asking. If you want to ask someone you're familiar with, you can use:\n",
      "\n",
      "\"Salut, comment √ßa va?\"\n",
      "\n",
      "Both of these phrases are commonly used in French to ask about someone's well-being.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_litellm/r/call/019688f7-0a9f-71f1-8cb3-680c79a126b0\n",
      "üç© https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_litellm/r/call/019688f7-0c7f-78a0-97b1-0179538f996c\n",
      "üç© https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_litellm/r/call/019688fa-3ac6-7b92-b42c-797180c511a7\n",
      "üç© https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_litellm/r/call/019688fa-8b42-7312-b9a0-69d7ac8a0cfb\n"
     ]
    }
   ],
   "source": [
    "question = \"Translate 'Hello, how are you?' to French.\"\n",
    "response = litellm.completion(\n",
    "    model=model_litellm,\n",
    "    messages=[{'role': 'user', 'content': question}],\n",
    "    max_tokens=1024,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6c9d7",
   "metadata": {},
   "source": [
    "### @weave.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def translate_litellm(model: str, text: str, target_language: str) -> Any:\n",
    "    response = litellm.completion(\n",
    "        model=model, messages=[{'role': 'user', 'content': f\"Translate '{text}' to {target_language}\"}], max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a27d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `int` - serialized value may not be as expected [input_value=1746054614.2416449, input_type=float])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The translation of \\'Hello, how are you?\\' to French is:\\n\\n\"Bonjour, comment allez-vous?\"\\n\\nThis is a formal way of asking. If you want to ask someone you know, you can use the informal version:\\n\\n\"Salut, comment √ßa va?\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_litellm(model_litellm, 'Hello, how are you?', 'French')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd70ba",
   "metadata": {},
   "source": [
    "### weave.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee86a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteLLMTranslatorModel(weave.Model):  # type: ignore\n",
    "    model: str\n",
    "    temperature: float\n",
    "\n",
    "    @weave.op()  # type: ignore\n",
    "    def predict(self, text: str, target_language: str) -> Any:\n",
    "        response = litellm.completion(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': f'You are a translator. Translate the given text to {target_language}.'},\n",
    "                {'role': 'user', 'content': text},\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be7b7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment allez-vous aujourd'hui ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `int` - serialized value may not be as expected [input_value=1746054844.0954764, input_type=float])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the translator weave.Model\n",
    "translator = LiteLLMTranslatorModel(model=model_litellm, temperature=0.3)\n",
    "\n",
    "english_text = 'Hello, how are you today?'\n",
    "french_text = translator.predict(english_text, 'French')\n",
    "\n",
    "print(french_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674aac35",
   "metadata": {},
   "source": [
    "## Via the OpenAI SDK\n",
    "\n",
    "`SambaNova` supports the `OpenAI` SDK compatibility ([docs](https://docs.sambanova.ai/cloud/docs/capabilities/openai-compatibility)) which `Weave` automatically detects and integrates with.\n",
    "\n",
    "To switch to using the SambaNova API, simply switch out the `api_key` to your SambaNova API key, `base_url` to https://api.sambanova.ai/v1, and `model` to one of our chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d88f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: francescar.\n",
      "View Weave data at https://sambanova.wandb.io/ai-solutions-team/weave_integration_sambanova_openai_sdk/weave\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize Weave project\n",
    "weave.init('weave_integration_sambanova_openai_sdk')\n",
    "\n",
    "# SambaNova URL: https://api.sambanova.ai/v1\n",
    "SAMBANOVA_URL = os.getenv('SAMBANOVA_URL')\n",
    "\n",
    "# Set the sambanova client\n",
    "sambanova_client = OpenAI(base_url=SAMBANOVA_URL, api_key=SAMBANOVA_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f3d8c",
   "metadata": {},
   "source": [
    "### Simple call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646db090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corrected sentence is:\n",
      "\n",
      "\"That was so easy, it was a piece of cake!\"\n",
      "\n",
      "The original phrase \"a piece of pie\" is not a common idiomatic expression. The correct idiom is \"a piece of cake,\" which means something is very easy to do. However, it's worth noting that the original sentence \"a piece of pie\" is still grammatically correct and can be used in a literal sense, but it doesn't convey the same idiomatic meaning as \"a piece of cake.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `int` - serialized value may not be as expected [input_value=1746056030.9465058, input_type=float])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# Create a grammar corrector\n",
    "response = sambanova_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a grammar checker, correct the following user input.'},\n",
    "        {'role': 'user', 'content': 'That was so easy, it was a piece of pie!'}],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e78b8",
   "metadata": {},
   "source": [
    "### @weave.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d61a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(model: str, system_prompt: str, user_prompt: str) -> Optional[str]:\n",
    "    \"\"\"Correct the grammar of a text.\"\"\"\n",
    "    \n",
    "    response = sambanova_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': user_prompt}],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "response = correct_grammar(model, 'You are a grammar checker, correct the following user input.', 'That was so easy, it was a piece of pie!')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2c369",
   "metadata": {},
   "source": [
    "### weave.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarCorrectorModel(weave.Model):\n",
    "    model: str\n",
    "    system_message: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, user_input: str) -> Optional[str]:\n",
    "        \"\"\"Correct the grammar of a text.\"\"\"\n",
    "\n",
    "        response = sambanova_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{'role': 'system', 'content': self.system_message}, {'role': 'user', 'content': user_input}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Correct grammar\n",
    "corrector = GrammarCorrectorModel(\n",
    "    model=model, system_message='You are a grammar checker, correct the following user input.'\n",
    ")\n",
    "result = corrector.predict('That was so easy, it was a piece of pie!')\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
