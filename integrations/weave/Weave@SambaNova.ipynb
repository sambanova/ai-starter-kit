{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736bf04e",
   "metadata": {},
   "source": [
    "# Weave @SambaNova\n",
    "\n",
    "This notebook demonstrates how to use [W&B Weave](https://weave-docs.wandb.ai/) with [SambaNova](https://sambanova.ai/) as your fastest model provider of choice for open source models.\n",
    "\n",
    "`Weights & Biases (W&B) Weave` is a framework for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications. Designed for flexibility and scalability, Weave supports every stage of your LLM application development workflow:\n",
    "\n",
    "- Tracing & Monitoring: Track LLM calls and application logic to debug and analyze production systems.\n",
    "- Systematic Iteration: Refine and iterate on prompts, datasets, and models.\n",
    "- Experimentation: Experiment with different models and prompts in the LLM Playground.\n",
    "- Evaluation: Use custom or pre-built scorers alongside our comparison tools to systematically assess and enhance application performance.\n",
    "- Guardrails: Protect your application with pre- and post-safeguards for content moderation, prompt safety, and more.\n",
    "\n",
    "In order to use `Weave` @`SambaNova`, you need to set the environment variable `SAMBANOVA_API_KEY`: your API key for accessing the SambaNova Cloud. You can create your API key [here](https://cloud.sambanova.ai/apis).\n",
    "\n",
    "1. To get started, simply call `weave.init()` at the beginning of your script, with the project name as attribute.\n",
    "\n",
    "2. `Weave` ops make results reproducible by automatically versioning code as you experiment, and they capture their inputs and outputs.\n",
    "Simply create a function decorated with `@weave.op()` that calls into each completion function and `Weave` will track the inputs and outputs for you. \n",
    "\n",
    "3. By using the `weave.Model` class, you can capture and organize the experimental details of your app like your system prompt or the model you're using. This helps organize and compare different iterations of your app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad363b8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4330ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Optional\n",
    "\n",
    "import weave\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have SAMBANOVA_API_KEY in your .env file\n",
    "SAMBANOVA_API_KEY = os.getenv('SAMBANOVA_API_KEY')\n",
    "\n",
    "# Initialize Weave with your project name\n",
    "model = 'Meta-Llama-3.3-70B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe47689",
   "metadata": {},
   "source": [
    "## Via LangChain\n",
    "\n",
    "`Weave` is designed to make tracking and logging all calls made through the `LangChain` Python library effortless, after `weave.init()` is called.\n",
    "\n",
    "You can access all the features of the `LangChain` + `Weave` integration, by using our `LangChain` chat object, `langchain_sambanova.ChatSambaNovaCloud`.\n",
    "\n",
    "For more details on all the `Weave` features supported by `LangChain`, please refer to [Weave @LangChain](https://weave-docs.wandb.ai/guides/integrations/langchain/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_sambanova import ChatSambaNovaCloud\n",
    "\n",
    "# Initialize Weave project\n",
    "weave.init('weave_integration_sambanova_langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e94709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LangChain SambaNova Chat object\n",
    "llm = ChatSambaNovaCloud(\n",
    "    model=model,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# The prompt template\n",
    "prompt = PromptTemplate.from_template('1 + {number} = ')\n",
    "\n",
    "# The LLM chain\n",
    "llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d84b94",
   "metadata": {},
   "source": [
    "### Simple call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fcb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the LLM on the prompt\n",
    "output = llm_chain.invoke({'number': 2})\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e092f",
   "metadata": {},
   "source": [
    "### Tracking Call Metadata\n",
    "To track metadata from your `LangChain` calls, you can use the `weave.attributes` context manager. This context manager allows you to set custom metadata for a specific block of code, such as a chain or a single request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLM chain with Weave attributes\n",
    "with weave.attributes({'number_to_increment': 'value'}):\n",
    "    output = llm_chain.invoke({'number': 2})\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952e711",
   "metadata": {},
   "source": [
    "## Via LiteLLM\n",
    "\n",
    "`Weave` automatically tracks and logs LLM calls made via LiteLLM, after `weave.init()` is called.\n",
    "\n",
    "You can access all the features of the `Weave` + `LiteLLM` integration, by specifying the `SambaNova` model name in the `LiteLLM` constructor, as explained in [LiteLLM @SambaNova](https://docs.litellm.ai/docs/providers/sambanova).\n",
    "\n",
    "For more details on all the `Weave` features supported by `LiteLLM`, please refer to [Weave @LiteLLM](https://weave-docs.wandb.ai/guides/integrations/litellm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Initialize Weave project\n",
    "weave.init('weave_integration_sambanova_litellm')\n",
    "model_litellm = 'sambanova/' + model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7b868",
   "metadata": {},
   "source": [
    "### Simple call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranlsate\n",
    "response = litellm.completion(\n",
    "    model=model_litellm,\n",
    "    messages=[{'role': 'user', 'content': \"Translate 'Hello, how are you?' to French.\"}],\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6c9d7",
   "metadata": {},
   "source": [
    "### @weave.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a translation function\n",
    "@weave.op()\n",
    "def translate_litellm(model: str, text: str, target_language: str) -> Any:\n",
    "    response = litellm.completion(\n",
    "        model=model, messages=[{'role': 'user', 'content': f\"Translate '{text}' to {target_language}\"}], max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Translate\n",
    "translate_litellm(model_litellm, 'Hello, how are you?', 'French')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd70ba",
   "metadata": {},
   "source": [
    "### weave.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee86a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translator model\n",
    "class TranslatorModel(weave.Model):  # type: ignore\n",
    "    model: str\n",
    "    temperature: float\n",
    "\n",
    "    @weave.op()  # type: ignore\n",
    "    def predict(self, text: str, target_language: str) -> Any:\n",
    "        \"\"\"Translate the given text to target language.\"\"\"\n",
    "        \n",
    "        response = litellm.completion(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': f'You are a translator. Translate the given text to {target_language}.'},\n",
    "                {'role': 'user', 'content': text},\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Create an instance of the translator weave.Model\n",
    "translator = TranslatorModel(model=model_litellm, temperature=0.3)\n",
    "\n",
    "# Translate\n",
    "english_text = 'Hello, how are you today?'\n",
    "french_text = translator.predict(english_text, 'French')\n",
    "\n",
    "print(french_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674aac35",
   "metadata": {},
   "source": [
    "## Via the OpenAI SDK\n",
    "\n",
    "`SambaNova` supports the `OpenAI` SDK compatibility ([docs](https://docs.sambanova.ai/cloud/docs/capabilities/openai-compatibility)), which `Weave` automatically detects and integrates with.\n",
    "\n",
    "To use the SambaNova API, simply switch out the `api_key` to your SambaNova API key, `base_url` to https://api.sambanova.ai/v1, and `model` to one of our chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize Weave project\n",
    "weave.init('weave_integration_sambanova_openai_sdk')\n",
    "\n",
    "# SambaNova URL, e.g. https://api.sambanova.ai/v1\n",
    "SAMBANOVA_URL = os.getenv('SAMBANOVA_URL')\n",
    "\n",
    "# Set the sambanova client\n",
    "sambanova_client = OpenAI(base_url=SAMBANOVA_URL, api_key=SAMBANOVA_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f3d8c",
   "metadata": {},
   "source": [
    "### Simple call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646db090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct grammar\n",
    "response = sambanova_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a grammar checker, correct the following user input.'},\n",
    "        {'role': 'user', 'content': 'That was so easy, it was a piece of pie!'}],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e78b8",
   "metadata": {},
   "source": [
    "### @weave.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d61a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for grammar correction \n",
    "@weave.op()\n",
    "def correct_grammar(model: str, system_prompt: str, user_prompt: str) -> Optional[str]:\n",
    "    \"\"\"Correct the grammar of a text.\"\"\"\n",
    "    \n",
    "    response = sambanova_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': user_prompt}],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Correct grammar\n",
    "response = correct_grammar(model, 'You are a grammar checker, correct the following user input.', 'That was so easy, it was a piece of pie!')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2c369",
   "metadata": {},
   "source": [
    "### weave.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grammar corrector model\n",
    "class GrammarCorrectorModel(weave.Model):\n",
    "    model: str\n",
    "    system_message: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, user_input: str) -> Optional[str]:\n",
    "        \"\"\"Correct the grammar of a text.\"\"\"\n",
    "\n",
    "        response = sambanova_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{'role': 'system', 'content': self.system_message}, {'role': 'user', 'content': user_input}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Correct grammar\n",
    "corrector = GrammarCorrectorModel(\n",
    "    model=model, system_message='You are a grammar checker, correct the following user input.'\n",
    ")\n",
    "result = corrector.predict('That was so easy, it was a piece of pie!')\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
