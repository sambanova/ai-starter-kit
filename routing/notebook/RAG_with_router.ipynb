{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiwu/Documents/ai-starter-kit-personal/ai-starter-kit/routing/routing_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/feiwu/Documents/ai-starter-kit-personal/ai-starter-kit/routing/routing_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_API_KEY is not set. Weave initialization skipped.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from src.routing import Router\n",
    "from enterprise_knowledge_retriever.src.document_retrieval import DocumentRetrieval\n",
    "\n",
    "CONFIG_PATH = os.path.join(kit_dir,'config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "router = Router(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 17:17:48.296 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2024-09-19 17:18:02,296 [INFO] - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 17:18:07,977 [INFO] - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      "Loading collection name is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 17:18:10,188 [INFO] - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "document_retrieval =  DocumentRetrieval()\n",
    "embeddings = document_retrieval.load_embedding_model()\n",
    "# print(os.path.join(kit_dir, \"data/800_20\"))\n",
    "vectorstore = document_retrieval.load_vdb(db_path = os.path.join(kit_dir, \"data/800_20\"), embeddings=embeddings)\n",
    "document_retrieval.init_retriever(vectorstore)\n",
    "conversation = document_retrieval.get_qa_retrieval_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to measure performance using sambatune?\"\n",
    "# query = \"Who is the CEO of Sambanova?\"\n",
    "# query = \"What is LLM?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiwu/Documents/ai-starter-kit-personal/ai-starter-kit/routing/routing_env/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model. It's a type of artificial intelligence (AI) model that is trained on a massive corpus of text data to generate human-like language. LLMs are designed to understand and generate text, and they can be used for a wide range of applications, such as:\n",
      "\n",
      "1. **Language translation**: LLMs can translate text from one language to another.\n",
      "2. **Text summarization**: LLMs can summarize long pieces of text into shorter, more digestible versions.\n",
      "3. **Chatbots**: LLMs can be used to power chatbots that can have conversations with humans.\n",
      "4. **Content generation**: LLMs can generate text, such as articles, stories, and even entire books.\n",
      "5. **Question answering**: LLMs can answer questions based on the text they've been trained on.\n",
      "\n",
      "LLMs work by using a technique called **deep learning**, which involves training a neural network on a large dataset of text. The neural network learns to recognize patterns in the text and generate new text based on those patterns.\n",
      "\n",
      "Some of the key characteristics of LLMs include:\n",
      "\n",
      "1. **Scale**: LLMs are trained on massive datasets, often containing billions of words.\n",
      "2. **Complexity**: LLMs can understand and generate complex language, including nuances and subtleties.\n",
      "3. **Flexibility**: LLMs can be fine-tuned for specific tasks and domains.\n",
      "4. **Speed**: LLMs can generate text quickly, often in real-time.\n",
      "\n",
      "Examples of popular LLMs include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a widely used LLM that has achieved state-of-the-art results in many natural language processing tasks.\n",
      "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook, RoBERTa is a variant of BERT that has achieved even better results in some tasks.\n",
      "3. **LLaMA (Large Language Model Meta AI)**: Developed by Meta AI, LLaMA is a large language model that has achieved state-of-the-art results in many tasks.\n",
      "\n",
      "Overall, LLMs have the potential to revolutionize the way we interact with language and have many exciting applications in fields such as language translation, text summarization, and content generation, and more.\n"
     ]
    }
   ],
   "source": [
    "datasource = router.routing(query)\n",
    "if datasource == \"vectorstore\":\n",
    "    output = conversation.invoke({\"question\":query})\n",
    "    response = output['answer']\n",
    "else:\n",
    "    response = document_retrieval.llm(query)\n",
    "\n",
    "print(response)\n",
    "print(\"--------------------------\")\n",
    "print(f\"datasource: {datasource}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
