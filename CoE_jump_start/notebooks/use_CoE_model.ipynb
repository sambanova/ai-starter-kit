{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1234567",
   "metadata": {},
   "source": [
    "# Calling Composition Of Experts (CoE) Models\n",
    "\n",
    "This notebook demonstrates how to use the `use_coe_model.py` script to call the Composition Of Experts (CoE) models using different approaches. We'll explore three examples:\n",
    "\n",
    "1. Using Sambaverse to call CoE Model\n",
    "2. Using SambaStudio to call CoE with Named Expert\n",
    "3. Using SambaStudio to call CoE with Routing\n",
    "\n",
    "Before we begin, make sure you have the `use_coe_model.py` script in the same directory as this notebook.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497a4fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "CONFIG_PATH = os.path.join(current_dir, \"config.yaml\")\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from CoE_jump_start.src.use_CoE_model import SambaStudioEmbeddings, Sambaverse, SambaStudio, create_stuff_documents_chain, create_retrieval_chain, get_expert, get_expert_val\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(os.path.join(current_dir, \".env\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf862ab3",
   "metadata": {},
   "source": [
    "## Example 1: Using Sambaverse to call CoE Model\n",
    "\n",
    "In this example, we'll use Sambaverse to call the CoE model. Sambaverse provides the expert name and their API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "571edf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variables:\n",
      "SAMBASTUDIO_BASE_URL: https://sjc3-e6.sambanova.net\n",
      "SAMBASTUDIO_BASE_URI: Not set\n",
      "SAMBASTUDIO_PROJECT_ID: 6f6192be-d15a-49d3-80c9-133ecd8be8d6\n",
      "SAMBASTUDIO_ENDPOINT_ID: fc5badb8-d9ee-4da6-b644-b0c18b3a1da2\n",
      "SAMBASTUDIO_API_KEY: 96a13...f3864\n",
      "SAMBAVERSE_API_KEY: e300e...27823\n",
      "SAMBASTUDIO_EMBEDDINGS_BASE_URL: https://sjc3-demo2.sambanova.net\n",
      "SAMBASTUDIO_EMBEDDINGS_PROJECT_ID: 4e1e3d93-79b9-4694-bdfc-181b5a3e019b\n",
      "SAMBASTUDIO_EMBEDDINGS_ENDPOINT_ID: 5fc68ee8-2de8-429c-b4d9-b0a17a13ee87\n",
      "SAMBASTUDIO_EMBEDDINGS_API_KEY: 2634c...ba7d0\n"
     ]
    }
   ],
   "source": [
    "# List of key environment variables to check\n",
    "env_vars_to_check = [\n",
    "    \"SAMBASTUDIO_BASE_URL\",\n",
    "    \"SAMBASTUDIO_BASE_URI\",\n",
    "    \"SAMBASTUDIO_PROJECT_ID\",\n",
    "    \"SAMBASTUDIO_ENDPOINT_ID\",\n",
    "    \"SAMBASTUDIO_API_KEY\",\n",
    "    \"SAMBAVERSE_API_KEY\" , # Include this if you're using Sambaverse\n",
    "    \"SAMBASTUDIO_EMBEDDINGS_BASE_URL\",\n",
    "    \"SAMBASTUDIO_EMBEDDINGS_PROJECT_ID\",\n",
    "    \"SAMBASTUDIO_EMBEDDINGS_ENDPOINT_ID\",\n",
    "    \"SAMBASTUDIO_EMBEDDINGS_API_KEY\"\n",
    "    \n",
    "    ]\n",
    "\n",
    "# Print the values of the environment variables\n",
    "print(\"Environment Variables:\")\n",
    "for var in env_vars_to_check:\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        # Print only the first few characters of the API keys for security\n",
    "        if \"API_KEY\" in var:\n",
    "            print(f\"{var}: {value[:5]}...{value[-5:]}\")\n",
    "        else:\n",
    "            print(f\"{var}: {value}\")\n",
    "    else:\n",
    "        \n",
    "        print(f\"{var}: Not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234569",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1: Using Sambaverse to call CoE Model\n",
    "\n",
    "#In this example, we'll use Sambaverse to call the CoE model. Sambaverse provides the expert name and their API key.\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambaverse\n",
    "# llm:\n",
    "#   sambaverse_model_name: \"Mistral/Mistral-7B-Instruct-v0.2\"\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Since Embedding Models are only available on SambaStudio and not Sambaverse we create a local Hugging Face Embeddings Object\n",
    "# In the SambaStudio examples later, we use an Embeddings Models hosted on SambaStudio\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings, collection_name='sambaverse_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = Sambaverse(\n",
    "    sambaverse_model_name=llm_info[\"sambaverse_model_name\"],\n",
    "    sambaverse_api_key=os.getenv(\"SAMBAVERSE_API_KEY\"),\n",
    "    model_kwargs={\n",
    "        \"do_sample\": False,\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"process_prompt\": True,\n",
    "        \"select_expert\": llm_info[\"samabaverse_select_expert\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"How can you use langsmith for testing\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234570",
   "metadata": {},
   "source": [
    "## Example 2: Using SambaStudio to call CoE with Named Expert\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with a named expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1234571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "    Answer: 1. Install LangSmith‚ÄãPythonTypeScriptpip install -U langsmithyarn add langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãWe provide multiple ways to log traces to LangSmith. Below, we'll highlight\n",
      "\n",
      "    assistant[{'relatedText': \"1. Install LangSmith‚ÄãPythonTypeScriptpip install -U langsmithyarn add langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page.\", 'clause': ['API Key']}]\n"
     ]
    }
   ],
   "source": [
    "## Example 2: Using SambaStudio to call CoE with Named Expert\n",
    "\n",
    "#In this example, we'll use SambaStudio to call the CoE model with a named expert.\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Create a SambaStudioEmbeddings object\n",
    "snsdk_model = SambaStudioEmbeddings()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings, collection_name='sambastudio_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaStudio(\n",
    "    streaming=True,\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"select_expert\": llm_info[\"samabaverse_select_expert\"],\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"Tell me how I can use langsmith within applications\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234572",
   "metadata": {},
   "source": [
    "## Example 3: Using SambaStudio to call CoE with Routing\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with routing. The script will automatically determine the appropriate expert based on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1234573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router expert response: {'status': {'complete': True, 'exitCode': 0, 'elapsedTime': 0.5622470378875732, 'message': '', 'progress': 1, 'progressMessage': '', 'reason': ''}, 'predictions': [{'completion': '', 'logprobs': {'text_offset': [], 'top_logprobs': []}, 'prompt': '{\"conversation_id\": \"sambaverse-conversation-id\", \"messages\": [{\"message_id\": 0, \"role\": \"user\", \"content\": \"Tell me how I can use langsmith for testing\"}], \"prompt\": \"<s>[INST]\\\\nA message can be classified as only one of the following categories: \\'finance\\', \\'economics\\', \\'maths\\', \\'code generation\\', \\'legal\\', \\'medical\\', \\'history\\', \\'turkish language\\', \\'japanese language\\', \\'literature\\', \\'physics\\', \\'chemistry\\', \\'biology\\', \\'psychology\\', \\'sociology\\' or \\'None of the above\\'.\\\\n\\\\nExamples for these categories are given below:\\\\n- \\'finance\\': What is the current stock price of Apple?\\\\n- \\'economics\\': Explain the concept of supply and demand.\\\\n- \\'maths\\': Solve the equation 2x + 5 = 15.\\\\n- \\'code generation\\': Write a Python function to calculate the factorial of a number.\\\\n- \\'legal\\': What are the basic requirements for a valid contract?\\\\n- \\'medical\\': What are the symptoms of type 2 diabetes?\\\\n- \\'history\\': Describe the major events of World War II.\\\\n- \\'turkish language\\': Merhaba, nas\\\\u0131ls\\\\u0131n\\\\u0131z? (Hello, how are you?)\\\\n- \\'turkish language\\': T\\\\u00fcrk\\\\u00e7e \\\\u00f6\\\\u011frenmenin en iyi yolu nedir? (What\\'s the best way to learn Turkish?)\\\\n- \\'japanese language\\': \\\\u3053\\\\u3093\\\\u306b\\\\u3061\\\\u306f\\\\u3001\\\\u304a\\\\u5143\\\\u6c17\\\\u3067\\\\u3059\\\\u304b\\\\uff1f(Hello, how are you?)\\\\n- \\'japanese language\\': \\\\u65e5\\\\u672c\\\\u8a9e\\\\u306e\\\\u52a9\\\\u8a5e\\\\u306e\\\\u4f7f\\\\u3044\\\\u65b9\\\\u3092\\\\u8aac\\\\u660e\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002(Please explain the usage of particles in Japanese.)\\\\n- \\'literature\\': Analyze the themes in Shakespeare\\'s \\\\\"Hamlet\\\\\".\\\\n- \\'physics\\': Explain Newton\\'s laws of motion.\\\\n- \\'chemistry\\': What is the periodic table of elements?\\\\n- \\'biology\\': Describe the process of photosynthesis.\\\\n- \\'psychology\\': What are the stages of cognitive development according to Piaget?\\\\n- \\'sociology\\': Explain the concept of social stratification.\\\\n- \\'Generalist\\': Who are you?\\\\n- \\'Generalist\\': What are your capabilities?\\\\n- \\'Generalist\\': Can you tell me a joke?\\\\n\\\\nBased on the above categories, classify this message:\\\\nTell me how I can use langsmith for testing\\\\n\\\\nAlways remember the following instructions while classifying the given statement:\\\\n- Think carefully and if you are not highly certain then classify the given statement as \\'Generalist\\'\\\\n- For \\'turkish language\\' and \\'japanese language\\' categories, the input may be in Turkish or Japanese respectively. Classify based on the language used, not just the content.\\\\n- Always begin your response by putting the classified category of the given statement after \\'<<detected category>>:\\'\\\\n- Explain your answer\\\\n[/INST]\\\\n\"}', 'stop_reason': 'end_of_text', 'tokens': [], 'total_tokens_count': 725}]}\n",
      "Routing Named Expert: Generalist\n",
      "Named expert Model Name: nm-shi-0.1-Meta-Llama-3-70B-Instruct-coe\n",
      "Response:  and monitoring my LLM applications. assistant[{'relatedText': \"3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãWe provide multiple ways to log traces to LangSmith. Below, we'll highlight\", 'clause': ['Bildirim Yükümlülüğü']}]\n"
     ]
    }
   ],
   "source": [
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   coe_routing: true\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Create a SambaStudioEmbeddings object\n",
    "snsdk_model = SambaStudioEmbeddings()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings,collection_name='sambastudio_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "user_query = \"Tell me how I can use langsmith for testing\"\n",
    "\n",
    "# Get the expert by calling SambaStudio with a custom prompt workflow\n",
    "expert_response = get_expert(user_query, use_requests=True)\n",
    "print(f\"Router expert response: {expert_response}\")\n",
    "\n",
    "# Extract the expert name from the response\n",
    "expert = get_expert_val(expert_response)\n",
    "print(f\"Routing Named Expert: {expert}\")\n",
    "\n",
    "# Look up the model name based on the expert\n",
    "named_expert = config[\"coe_name_map\"][expert]\n",
    "print(f\"Named expert Model Name: {named_expert}\")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaStudio(\n",
    "    streaming=True,\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"select_expert\": named_expert,\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(f\"Response: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234574",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In each example, we walked through the following steps:\n",
    "\n",
    "1. Update the `config.yaml` file with the appropriate API information and LLM parameters\n",
    "2. Create a `SambaNovaEmbeddingModel` (or a `HuggingFaceEmbeddings`) object for embeddings\n",
    "3. Load documents from a URL and split them into chunks\n",
    "4. Create a vector database using Chroma\n",
    "5. Define the prompt template\n",
    "6. Set up the language model based on the example configuration\n",
    "7. Create the document chain and retrieval chain\n",
    "8. Invoke the retrieval chain with the user query\n",
    "9. Print the response\n",
    "\n",
    "For Example 3, we additionally:\n",
    "- Called `get_expert()` to determine the appropriate expert based on the user query.\n",
    "- Extracted the expert name using `get_expert_val()`.\n",
    "- Looked up the model name based on the expert.\n",
    "\n",
    "Feel free to explore and experiment with different configurations and queries to see how the CoE models respond!\n",
    "\n",
    "If you have any questions or need further assistance, please don't hesitate to ask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
