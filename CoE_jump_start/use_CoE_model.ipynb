{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1234567",
   "metadata": {},
   "source": [
    "# Calling Center of Excellence (CoE) Models\n",
    "\n",
    "This notebook demonstrates how to use the `use_coe_model.py` script to call the Center of Excellence (CoE) models using different approaches. We'll explore three examples:\n",
    "\n",
    "1. Using SambaVerse to call CoE Model\n",
    "2. Using SambaStudio to call CoE with Named Expert\n",
    "3. Using SambaStudio to call CoE with Routing\n",
    "\n",
    "Before we begin, make sure you have the `use_coe_model.py` script in the same directory as this notebook.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234568",
   "metadata": {},
   "source": [
    "## Example 1: Using SambaVerse to call CoE Model\n",
    "\n",
    "In this example, we'll use SambaVerse to call the CoE model. SambaVerse provides the expert name and their API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497a4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1234569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_community.document_loaders.web_base:fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Response.json of <Response [200]>>\n",
      "I'm unable to provide you with the exact code for creating a vector database in LangChain, as it depends on the specific version of LangChain and the configuration of your project.\n",
      "\n",
      "However, I can guide you through the general steps for creating a vector database in LangChain:\n",
      "\n",
      "1. Install LangChain: Before you can create a vector database in LangChain, you need to have LangChain installed in your project. You can install LangChain using pip or npm, depending on your project configuration.\n",
      "2. Create a new vector database: Once you have LangChain installed in your project, you can create a new vector database using the `VectorDatabase` class in LangChain. You can configure the properties of the new vector database, such as the name, the size, and the vectorization algorithm to use.\n",
      "3. Load data into the vector database: Once you have created a new vector database in LangChain, you can load data into the database using the `addDocument` method of the `VectorDatabase` class. You can pass the method a document object, which can be in the form of a JSON object, a Pandas DataFrame, or any other format that can be converted into a vector representation using the vectorization algorithm configured for the vector database.\n",
      "4. Perform vector database operations: Once you have loaded data into the vector database in LangChain, you can perform various vector database operations, such as querying the database using vector search algorithms, updating the database with new documents, and deleting documents from the database.\n",
      "\n",
      "I hope this information helps you get started with creating a vector database in LangChain. If you have any further questions or need more detailed guidance, please don't hesitate to ask.\n"
     ]
    }
   ],
   "source": [
    "from use_CoE_model import SambaNovaEmbeddingModel, SambaverseEndpoint, create_stuff_documents_chain, create_retrieval_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambaverse\n",
    "# llm:\n",
    "#   sambaverse_model_name: \"Mistral/Mistral-7B-Instruct-v0.2\"\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Create a SambaNovaEmbeddingModel object\n",
    "snsdk_model = SambaNovaEmbeddingModel()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaverseEndpoint(\n",
    "    sambaverse_model_name=\"Mistral/Mistral-7B-Instruct-v0.2\",\n",
    "    sambaverse_api_key=os.getenv(\"SAMBAVERSE_API_KEY\"),\n",
    "    model_kwargs={\n",
    "        \"do_sample\": False,\n",
    "        \"max_tokens_to_generate\": 1024,\n",
    "        \"temperature\": 0.1,\n",
    "        \"process_prompt\": True,\n",
    "        \"select_expert\": \"Mistral-7B-Instruct-v0.2\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"Give me the code for creating a vector db in langchain\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234570",
   "metadata": {},
   "source": [
    "## Example 2: Using SambaStudio to call CoE with Named Expert\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with a named expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1234571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_community.document_loaders.web_base:fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Response.json of <Response [200]>>\n",
      ".\n",
      "\n",
      "Answer: I'm unable to provide you with the exact code for creating a vector database in LangChain, as it depends on the specific use case and the chosen vector database implementation.\n",
      "\n",
      "However, I can guide you through the general steps for creating a vector database in LangChain:\n",
      "\n",
      "1. Install the required dependencies and packages for your LangChain project, including the vector database implementation you plan to use.\n",
      "\n",
      "2. Initialize your LangChain project and create a new API key for accessing your project's resources.\n",
      "\n",
      "3. Set up your environment for working with your vector database in LangChain. This may include configuring your database connection settings, setting up any necessary indexes or data structures, and defining any custom functions or scripts you may need to use with your vector database in LangChain.\n",
      "\n",
      "4. Once you have set up your environment for working with your vector database in LangChain, you can begin creating and managing your vector database resources within your LangChain project. This may include creating and managing vector collections, adding and removing vector documents from your collections, and performing various vector database operations, such as querying, indexing, and updating your vector database resources within your LangChain project.\n",
      "\n",
      "5. Finally, you can use the various LangChain APIs and tools to interact with and manage your vector database resources within your LangChain project, as well as to perform various data processing, analysis, and visualization tasks on your vector database data within your LangChain project.\n",
      "\n",
      "I hope this information helps you get started with creating a vector database in LangChain. If you have any further questions or need additional guidance, please don't hesitate to ask. Good luck with your LangChain project!\n"
     ]
    }
   ],
   "source": [
    "from use_CoE_model import SambaNovaEmbeddingModel, SambaNovaEndpoint, create_stuff_documents_chain, create_retrieval_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Create a SambaNovaEmbeddingModel object\n",
    "snsdk_model = SambaNovaEmbeddingModel()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaNovaEndpoint(\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens_to_generate\": 1024,\n",
    "        \"select_expert\": \"Mistral-7B-Instruct-v0.2\",\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"Give me the code for creating a vector db in langchain\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234572",
   "metadata": {},
   "source": [
    "## Example 3: Using SambaStudio to call CoE with Routing\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with routing. The script will automatically determine the appropriate expert based on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1234573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_community.document_loaders.web_base:fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Response.json of <Response [200]>>\n",
      "Expert response: {'data': [{'prompt': '{\"conversation_id\": \"sambaverse-conversation-id\", \"messages\": [{\"message_id\": 0, \"role\": \"user\", \"content\": \"Give me the code for creating a vector db in langchain\"}], \"prompt\": \"<s>[INST] \\\\n\\\\nA message can be classified as only one of the following categories: \\'finance\\',  \\'economics\\',  \\'maths\\',  \\'code generation\\', \\'legal\\', \\'medical\\', \\'history\\' or \\'None of the above\\'.  \\\\n\\\\nExamples for few of these categories are given below:\\\\n- \\'code generation\\': Write a python program\\\\n- \\'code generation\\': Debug the following code\\\\n- \\'None of the above\\': Who are you?\\\\n- \\'None of the above\\': What are you?\\\\n- \\'None of the above\\': Where are you?\\\\n\\\\nBased on the above categories, classify this message: \\\\n\\\\nGive me the code for creating a vector db in langchain\\\\n\\\\nAlways remember the following instructions while classifying the given statement:\\\\n- Think carefully and if you are not highly certain then classify the  given statement as \\'None of the above\\'\\\\n- Always begin your response by putting the classified category of the given statement after  \\'<<detected category>>:\\'\\\\n- Explain you answer\\\\n\\\\n[/INST]\\\\n\"}', 'completion': ' <<detected category>>: Code generation\\n\\nBased on the given message, it appears that the user is asking for code related to creating a vector database in Langchain. As a conversational AI language model, I cannot provide direct code without knowing more about the context and requirements of the project. However, I can offer some general guidance on how to approach this task.\\n\\nCreating a vector database in Langchain involves several steps, including data preparation, vectorization, and training. Here are some general steps you can follow:\\n\\n1. Data Preparation:\\n\\t* Collect and preprocess the data you want to use for training the vector database. This may involve cleaning the data, removing stop words, stemming or lemmatizing, and converting the data into a suitable format for training.\\n\\t* Split the data into training and validation sets to evaluate the performance of the model during training.\\n2. Vectorization:\\n\\t* Choose a suitable vectorization algorithm for your data, such as Word2Vec or Doc2Vec. These algorithms create vector representations of words or documents based on their context and co-occurrence patterns.\\n\\t* Train the vectorization model using the preprocessed data.\\n3. Training:\\n\\t* Train the vector database using the trained vectorization model and the preprocessed data. This involves feeding the preprocessed data into the model and adjusting the model parameters to minimize the error between the predicted and actual vectors.\\n4. Deployment:\\n\\t* Once the vector database is trained, you can use it to perform various natural language processing tasks, such as text classification, sentiment analysis, and information retrieval.\\n\\nI hope this general guidance helps you get started with creating a vector database in Langchain. If you have any specific questions or need further clarification, feel free to ask!', 'total_tokens_count': 702.0, 'stop_reason': 'end_of_text', 'logprobs': {'top_logprobs': None, 'text_offset': None}, 'tokens': ['', '<<', 'det', 'ected', 'category', '>>', ':', 'Code', 'generation', '\\n', '\\n', 'B', 'ased', 'on', 'the', 'given', 'message', ',', 'it', 'appears', 'that', 'the', 'user', 'is', 'asking', 'for', 'code', 'related', 'to', 'creating', 'a', 'vector', 'database', 'in', 'Lang', 'chain', '.', 'As', 'a', 'convers', 'ational', 'A', 'I', 'language', 'model', ',', 'I', 'cannot', 'provide', 'direct', 'code', 'without', 'knowing', 'more', 'about', 'the', 'context', 'and', 'requirements', 'of', 'the', 'project', '.', 'However', ',', 'I', 'can', 'offer', 'some', 'general', 'guidance', 'on', 'how', 'to', 'approach', 'this', 'task', '.', '\\n', '\\n', 'Cre', 'ating', 'a', 'vector', 'database', 'in', 'Lang', 'chain', 'involves', 'several', 'steps', ',', 'including', 'data', 'prepar', 'ation', ',', 'vector', 'ization', ',', 'and', 'training', '.', 'Here', 'are', 'some', 'general', 'steps', 'you', 'can', 'follow', ':', '\\n', '\\n', '1', '.', 'Data', 'Pre', 'par', 'ation', ':', '\\n', '\\t', '*', 'Collect', 'and', 'pre', 'process', 'the', 'data', 'you', 'want', 'to', 'use', 'for', 'training', 'the', 'vector', 'database', '.', 'This', 'may', 'involve', 'clean', 'ing', 'the', 'data', ',', 'removing', 'stop', 'words', ',', 'stem', 'ming', 'or', 'lem', 'mat', 'izing', ',', 'and', 'converting', 'the', 'data', 'into', 'a', 'suitable', 'format', 'for', 'training', '.', '\\n', '\\t', '*', 'Split', 'the', 'data', 'into', 'training', 'and', 'validation', 'sets', 'to', 'evaluate', 'the', 'performance', 'of', 'the', 'model', 'during', 'training', '.', '\\n', '2', '.', 'Vector', 'ization', ':', '\\n', '\\t', '*', 'Cho', 'ose', 'a', 'suitable', 'vector', 'ization', 'algorithm', 'for', 'your', 'data', ',', 'such', 'as', 'Word', '2', 'Vec', 'or', 'Doc', '2', 'Vec', '.', 'These', 'algorithms', 'create', 'vector', 'representations', 'of', 'words', 'or', 'documents', 'based', 'on', 'their', 'context', 'and', 'co', '-', 'occ', 'urrence', 'patterns', '.', '\\n', '\\t', '*', 'Train', 'the', 'vector', 'ization', 'model', 'using', 'the', 'pre', 'process', 'ed', 'data', '.', '\\n', '3', '.', 'Training', ':', '\\n', '\\t', '*', 'Train', 'the', 'vector', 'database', 'using', 'the', 'trained', 'vector', 'ization', 'model', 'and', 'the', 'pre', 'process', 'ed', 'data', '.', 'This', 'involves', 'feed', 'ing', 'the', 'pre', 'process', 'ed', 'data', 'into', 'the', 'model', 'and', 'adjust', 'ing', 'the', 'model', 'parameters', 'to', 'minim', 'ize', 'the', 'error', 'between', 'the', 'predicted', 'and', 'actual', 'vectors', '.', '\\n', '4', '.', 'Dep', 'loyment', ':', '\\n', '\\t', '*', 'Once', 'the', 'vector', 'database', 'is', 'trained', ',', 'you', 'can', 'use', 'it', 'to', 'perform', 'various', 'natural', 'language', 'processing', 'tasks', ',', 'such', 'as', 'text', 'classification', ',', 'sentiment', 'analysis', ',', 'and', 'information', 'retr', 'ieval', '.', '\\n', '\\n', 'I', 'hope', 'this', 'general', 'guidance', 'helps', 'you', 'get', 'started', 'with', 'creating', 'a', 'vector', 'database', 'in', 'Lang', 'chain', '.', 'If', 'you', 'have', 'any', 'specific', 'questions', 'or', 'need', 'further', 'clar', 'ification', ',', 'feel', 'free', 'to', 'ask', '!']}], 'status_code': 200}\n",
      "Expert: Code expert\n",
      "Named expert: deepseek-llm-67b-chat\n",
      "Response: .\n",
      "\n",
      "Based on the provided context, there is no information about creating a vector database in LangChain. The context mainly discusses issues related to sensitive data, private deployment, and self-hosting of LangSmith, as well as providing links to resources like the User Guide, FAQ, and community support.\n",
      "\n",
      "To find the code for creating a vector database in LangChain, you would need to refer to the LangChain documentation or search for relevant examples and tutorials online.\n"
     ]
    }
   ],
   "source": [
    "from use_CoE_model import SambaNovaEmbeddingModel, SambaNovaEndpoint, create_stuff_documents_chain, create_retrieval_chain, get_expert, get_expert_val\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   coe_routing: true\n",
    "\n",
    "# Create a SambaNovaEmbeddingModel object\n",
    "snsdk_model = SambaNovaEmbeddingModel()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "user_query = \"Give me the code for creating a vector db in langchain\"\n",
    "\n",
    "# Get the expert by calling SambaStudio with a custom prompt workflow\n",
    "expert_response = get_expert(user_query)\n",
    "print(f\"Router expert response: {expert_response}\")\n",
    "\n",
    "# Extract the expert name from the response\n",
    "expert = get_expert_val(expert_response)\n",
    "print(f\"Routing Named Expert: {expert}\")\n",
    "\n",
    "# Look up the model name based on the expert\n",
    "named_expert = {\n",
    "    \"Finance expert\": \"finance-chat\",\n",
    "    \"Math expert\": \"deepseek-llm-67b-chat\",\n",
    "    \"Code expert\": \"deepseek-llm-67b-chat\",\n",
    "    \"Medical expert\": \"medicine-chat\",\n",
    "    \"Legal expert\": \"law-chat\",\n",
    "    \"Generalist\": \"Mistral-7B-Instruct-v0.2\",\n",
    "}[expert]\n",
    "print(f\"Named expert Model Name: {named_expert}\")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaNovaEndpoint(\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens_to_generate\": 1024,\n",
    "        \"select_expert\": named_expert,\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(f\"Response: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234574",
   "metadata": {},
   "source": [
    "In each example, we walk through the following steps:\n",
    "\n",
    "1. Update the `config.yaml` file with the appropriate API information and LLM parameters.\n",
    "2. Create a `SambaNovaEmbeddingModel` object for embeddings.\n",
    "3. Load documents from a URL and split them into chunks.\n",
    "4. Create a vector database using Chroma.\n",
    "5. Define the prompt template.\n",
    "6. Set up the language model based on the example configuration.\n",
    "7. Create the document chain and retrieval chain.\n",
    "8. Invoke the retrieval chain with the user query.\n",
    "9. Print the response.\n",
    "\n",
    "For Example 3, we additionally:\n",
    "- Call `get_expert()` to determine the appropriate expert based on the user query.\n",
    "- Extract the expert name using `get_expert_val()`.\n",
    "- Look up the model name based on the expert.\n",
    "\n",
    "Feel free to explore and experiment with different configurations and queries to see how the CoE models respond!\n",
    "\n",
    "If you have any questions or need further assistance, please don't hesitate to ask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
