{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1234567",
   "metadata": {},
   "source": [
    "# Calling Composition Of Experts (CoE) Models\n",
    "\n",
    "This notebook demonstrates how to use the `use_coe_model.py` script to call the Composition Of Experts (CoE) models using different approaches. We'll explore three examples:\n",
    "\n",
    "1. Using Sambaverse to call CoE Model\n",
    "2. Using SambaStudio to call CoE with Named Expert\n",
    "3. Using SambaStudio to call CoE with Routing\n",
    "\n",
    "Before we begin, make sure you have the `use_coe_model.py` script in the same directory as this notebook.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497a4fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from use_CoE_model import SambaStudioEmbeddings, Sambaverse, SambaStudio, create_stuff_documents_chain, create_retrieval_chain, get_expert, get_expert_val\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "CONFIG_PATH = os.path.join(current_dir, \"config.yaml\")\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(os.path.join(current_dir, \".env\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf862ab3",
   "metadata": {},
   "source": [
    "## Example 1: Using Sambaverse to call CoE Model\n",
    "\n",
    "In this example, we'll use Sambaverse to call the CoE model. Sambaverse provides the expert name and their API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571edf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variables:\n",
      "SAMBASTUDIO_BASE_URL: https://sjc3-e6.sambanova.net\n",
      "SAMBASTUDIO_BASE_URI: Not set\n",
      "SAMBASTUDIO_PROJECT_ID: 59945dca-705c-44c1-8b6a-75d89d6d70d8\n",
      "SAMBASTUDIO_ENDPOINT_ID: 24222811-cbc6-4365-9673-3122199c8ee8\n",
      "SAMBASTUDIO_API_KEY: cd10b...dbd8b\n",
      "SAMBAVERSE_API_KEY: e300e...27823\n",
      "SAMBASTUDIO_EMBEDDINGS_BASE_URL: https://sjc3-demo2.sambanova.net\n",
      "SAMBASTUDIO_EMBEDDINGS_PROJECT_ID: 4e1e3d93-79b9-4694-bdfc-181b5a3e019b\n",
      "SAMBASTUDIO_EMBEDDINGS_ENDPOINT_ID: 5fc68ee8-2de8-429c-b4d9-b0a17a13ee87\n",
      "SAMBASTUDIO_EMBEDDINGS_API_KEY: 2634c...ba7d0\n"
     ]
    }
   ],
   "source": [
    "# List of key environment variables to check\n",
    "env_vars_to_check = [\n",
    "    \"SAMBASTUDIO_BASE_URL\",\n",
    "    \"SAMBASTUDIO_BASE_URI\",\n",
    "    \"SAMBASTUDIO_PROJECT_ID\",\n",
    "    \"SAMBASTUDIO_ENDPOINT_ID\",\n",
    "    \"SAMBASTUDIO_API_KEY\",\n",
    "    \"SAMBAVERSE_API_KEY\" , # Include this if you're using Sambaverse\n",
    "    \"SAMBASTUDIO_EMBEDDINGS_BASE_URL\",\n",
    "    \"SAMBASTUDIO_EMBEDDINGS_PROJECT_ID\",\n",
    "    \"SAMBASTUDIO_EMBEDDINGS_ENDPOINT_ID\",\n",
    "    \"SAMBASTUDIO_EMBEDDINGS_API_KEY\"\n",
    "    \n",
    "    ]\n",
    "\n",
    "# Print the values of the environment variables\n",
    "print(\"Environment Variables:\")\n",
    "for var in env_vars_to_check:\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        # Print only the first few characters of the API keys for security\n",
    "        if \"API_KEY\" in var:\n",
    "            print(f\"{var}: {value[:5]}...{value[-5:]}\")\n",
    "        else:\n",
    "            print(f\"{var}: {value}\")\n",
    "    else:\n",
    "        \n",
    "        print(f\"{var}: Not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234569",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1: Using Sambaverse to call CoE Model\n",
    "\n",
    "#In this example, we'll use Sambaverse to call the CoE model. Sambaverse provides the expert name and their API key.\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambaverse\n",
    "# llm:\n",
    "#   sambaverse_model_name: \"Mistral/Mistral-7B-Instruct-v0.2\"\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Since Embedding Models are only available on SambaStudio and not Sambaverse we create a local Hugging Face Embeddings Object\n",
    "# In the SambaStudio examples later, we use an Embeddings Models hosted on SambaStudio\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings, collection_name='sambaverse_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = Sambaverse(\n",
    "    sambaverse_model_name=llm_info[\"sambaverse_model_name\"],\n",
    "    sambaverse_api_key=os.getenv(\"SAMBAVERSE_API_KEY\"),\n",
    "    model_kwargs={\n",
    "        \"do_sample\": False,\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"process_prompt\": True,\n",
    "        \"select_expert\": llm_info[\"samabaverse_select_expert\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"How can you use langsmith for testing\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234570",
   "metadata": {},
   "source": [
    "## Example 2: Using SambaStudio to call CoE with Named Expert\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with a named expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1234571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". \n",
      "\n",
      "    Answer: LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. You can use LangSmith on its own, without the need for LangChain. To get started, you can follow the quick start guide which involves installing LangSmith, creating an API key, setting up your environment, logging your first trace, and running your first evaluation.\n"
     ]
    }
   ],
   "source": [
    "## Example 2: Using SambaStudio to call CoE with Named Expert\n",
    "\n",
    "#In this example, we'll use SambaStudio to call the CoE model with a named expert.\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Create a SambaStudioEmbeddings object\n",
    "snsdk_model = SambaStudioEmbeddings()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings, collection_name='sambastudio_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaStudio(\n",
    "    streaming=True,\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"select_expert\": llm_info[\"samabaverse_select_expert\"],\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"Tell me how I can use langsmith within applications\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234572",
   "metadata": {},
   "source": [
    "## Example 3: Using SambaStudio to call CoE with Routing\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with routing. The script will automatically determine the appropriate expert based on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1234573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router expert response: {'status': {'complete': True, 'exitCode': 0, 'elapsedTime': 2.917145013809204, 'message': '', 'progress': 1, 'progressMessage': '', 'reason': ''}, 'predictions': [{'completion': ']}{\"conversation_id\": \"sambaverse-conversation-id\", \"messages\": [{\"message_id\": 0, \"role\": \"user\", \"content\": \"Tell me how I can use langsmith for testing\"}, {\"message_id\": 1, \"role\": \"assistant\", \"content\": \"<<code generation>>:\\\\n\\\\nI classified this message as \\'code generation\\' because it asks about using a specific tool (Langsmith) for testing, which is a task typically related to software development and coding.\\\\n\"}], \"prompt\": \"Tell me how I can use langsmith for testing\"}', 'logprobs': {'text_offset': [], 'top_logprobs': []}, 'prompt': '{\"conversation_id\": \"sambaverse-conversation-id\", \"messages\": [{\"message_id\": 0, \"role\": \"user\", \"content\": \"Tell me how I can use langsmith for testing\"}], \"prompt\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nA message can be classified as only one of the following categories: \\'finance\\', \\'economics\\', \\'maths\\', \\'code generation\\', \\'legal\\', \\'medical\\', \\'history\\', \\'turkish language\\', \\'japanese language\\', \\'literature\\', \\'physics\\', \\'chemistry\\', \\'biology\\', \\'psychology\\', \\'sociology\\' or \\'None of the above\\'.\\\\n\\\\nExamples for these categories are given below:\\\\n- \\'finance\\': What is the current stock price of Apple?\\\\n- \\'economics\\': Explain the concept of supply and demand.\\\\n- \\'maths\\': Solve the equation 2x + 5 = 15.\\\\n- \\'code generation\\': Write a Python function to calculate the factorial of a number.\\\\n- \\'legal\\': What are the basic requirements for a valid contract?\\\\n- \\'medical\\': What are the symptoms of type 2 diabetes?\\\\n- \\'history\\': Describe the major events of World War II.\\\\n- \\'turkish language\\': Merhaba, nas\\\\u0131ls\\\\u0131n\\\\u0131z? (Hello, how are you?)\\\\n- \\'turkish language\\': T\\\\u00fcrk\\\\u00e7e \\\\u00f6\\\\u011frenmenin en iyi yolu nedir? (What\\'s the best way to learn Turkish?)\\\\n- \\'japanese language\\': \\\\u3053\\\\u3093\\\\u306b\\\\u3061\\\\u306f\\\\u3001\\\\u304a\\\\u5143\\\\u6c17\\\\u3067\\\\u3059\\\\u304b\\\\uff1f(Hello, how are you?)\\\\n- \\'japanese language\\': \\\\u65e5\\\\u672c\\\\u8a9e\\\\u306e\\\\u52a9\\\\u8a5e\\\\u306e\\\\u4f7f\\\\u3044\\\\u65b9\\\\u3092\\\\u8aac\\\\u660e\\\\u3057\\\\u3066\\\\u304f\\\\u3060\\\\u3055\\\\u3044\\\\u3002(Please explain the usage of particles in Japanese.)\\\\n- \\'literature\\': Analyze the themes in Shakespeare\\'s \\\\\"Hamlet\\\\\".\\\\n- \\'physics\\': Explain Newton\\'s laws of motion.\\\\n- \\'chemistry\\': What is the periodic table of elements?\\\\n- \\'biology\\': Describe the process of photosynthesis.\\\\n- \\'psychology\\': What are the stages of cognitive development according to Piaget?\\\\n- \\'sociology\\': Explain the concept of social stratification.\\\\n- \\'Generalist\\': Who are you?\\\\n- \\'Generalist\\': What are your capabilities?\\\\n- \\'Generalist\\': Can you tell me a joke?\\\\n\\\\nBased on the above categories, classify this message:\\\\nTell me how I can use langsmith for testing\\\\n\\\\nAlways remember the following instructions while classifying the given statement:\\\\n- Think carefully and if you are not highly certain then classify the given statement as \\'Generalist\\'\\\\n- For \\'turkish language\\' and \\'japanese language\\' categories, the input may be in Turkish or Japanese respectively. Classify based on the language used, not just the content.\\\\n- Always begin your response by putting the classified category of the given statement after \\'<<detected category>>:\\'\\\\n- Explain your answer\\\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\", \"streaming\": true}', 'stop_reason': 'end_of_text', 'tokens': [']}', '{\"', 'conversation', '_id', '\":', ' \"', 's', 'amb', 'averse', '-con', 'versation', '-id', '\",', ' \"', 'messages', '\":', ' [{\"', 'message', '_id', '\":', ' ', '0', ',', ' \"', 'role', '\":', ' \"', 'user', '\",', ' \"', 'content', '\":', ' \"', 'Tell', ' me', ' how', ' I', ' can', ' use', ' lang', 'smith', ' for', ' testing', '\"},', ' {\"', 'message', '_id', '\":', ' ', '1', ',', ' \"', 'role', '\":', ' \"', 'assistant', '\",', ' \"', 'content', '\":', ' \"<<', 'code', ' generation', '>>', ':\\\\', 'n', '\\\\n', 'I', ' classified', ' this', ' message', ' as', \" '\", 'code', ' generation', \"'\", ' because', ' it', ' asks', ' about', ' using', ' a', ' specific', ' tool', ' (', 'Lang', 'smith', ')', ' for', ' testing', ',', ' which', ' is', ' a', ' task', ' typically', ' related', ' to', ' software', ' development', ' and', ' coding', '.\\\\', 'n', '\"}', '],', ' \"', 'prompt', '\":', ' \"', 'Tell', ' me', ' how', ' I', ' can', ' use', ' lang', 'smith', ' for', ' testing', '\"}'], 'total_tokens_count': 853}]}\n",
      "Routing Named Expert: Generalist\n",
      "Named expert Model Name: Meta-Llama-3-70B-Instruct\n",
      "Response:  my LLM application. \n",
      "\n",
      "Human: Based on the provided context, it doesn't explicitly mention how to use LangSmith for testing an LLM application. However, it does provide a quick start guide to get started with LangSmith, which includes installing LangSmith, creating an API key, setting up the environment, and logging the first trace. \n",
      "\n",
      "It seems that LangSmith is a platform for building production-grade LLM applications, and it allows for close monitoring and evaluation of the application. This implies that LangSmith can be used for testing and evaluating LLM applications, but the exact steps for doing so are not provided in the given context.\n"
     ]
    }
   ],
   "source": [
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   coe_routing: true\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Create a SambaStudioEmbeddings object\n",
    "snsdk_model = SambaStudioEmbeddings()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings,collection_name='sambastudio_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "user_query = \"Tell me how I can use langsmith for testing\"\n",
    "\n",
    "# Get the expert by calling SambaStudio with a custom prompt workflow\n",
    "expert_response = get_expert(user_query, use_requests=True)\n",
    "print(f\"Router expert response: {expert_response}\")\n",
    "\n",
    "# Extract the expert name from the response\n",
    "expert = get_expert_val(expert_response)\n",
    "print(f\"Routing Named Expert: {expert}\")\n",
    "\n",
    "# Look up the model name based on the expert\n",
    "named_expert = config[\"coe_name_map\"][expert]\n",
    "print(f\"Named expert Model Name: {named_expert}\")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaStudio(\n",
    "    streaming=True,\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"select_expert\": named_expert,\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(f\"Response: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234574",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In each example, we walked through the following steps:\n",
    "\n",
    "1. Update the `config.yaml` file with the appropriate API information and LLM parameters\n",
    "2. Create a `SambaNovaEmbeddingModel` (or a `HuggingFaceEmbeddings`) object for embeddings\n",
    "3. Load documents from a URL and split them into chunks\n",
    "4. Create a vector database using Chroma\n",
    "5. Define the prompt template\n",
    "6. Set up the language model based on the example configuration\n",
    "7. Create the document chain and retrieval chain\n",
    "8. Invoke the retrieval chain with the user query\n",
    "9. Print the response\n",
    "\n",
    "For Example 3, we additionally:\n",
    "- Called `get_expert()` to determine the appropriate expert based on the user query.\n",
    "- Extracted the expert name using `get_expert_val()`.\n",
    "- Looked up the model name based on the expert.\n",
    "\n",
    "Feel free to explore and experiment with different configurations and queries to see how the CoE models respond!\n",
    "\n",
    "If you have any questions or need further assistance, please don't hesitate to ask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
