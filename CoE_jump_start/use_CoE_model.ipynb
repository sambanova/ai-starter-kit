{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1234567",
   "metadata": {},
   "source": [
    "# Calling Composition Of Experts (CoE) Models\n",
    "\n",
    "This notebook demonstrates how to use the `use_coe_model.py` script to call the Composition Of Experts (CoE) models using different approaches. We'll explore three examples:\n",
    "\n",
    "1. Using Sambaverse to call CoE Model\n",
    "2. Using SambaStudio to call CoE with Named Expert\n",
    "3. Using SambaStudio to call CoE with Routing\n",
    "\n",
    "Before we begin, make sure you have the `use_coe_model.py` script in the same directory as this notebook.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from use_CoE_model import SambaStudioEmbeddings, Sambaverse, SambaStudio, create_stuff_documents_chain, create_retrieval_chain, get_expert, get_expert_val\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "CONFIG_PATH = os.path.join(current_dir, \"config.yaml\")\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(os.path.join(current_dir, \".env\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf862ab3",
   "metadata": {},
   "source": [
    "## Example 1: Using Sambaverse to call CoE Model\n",
    "\n",
    "In this example, we'll use Sambaverse to call the CoE model. Sambaverse provides the expert name and their API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234569",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1: Using Sambaverse to call CoE Model\n",
    "\n",
    "#In this example, we'll use Sambaverse to call the CoE model. Sambaverse provides the expert name and their API key.\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambaverse\n",
    "# llm:\n",
    "#   sambaverse_model_name: \"Mistral/Mistral-7B-Instruct-v0.2\"\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Since Embedding Models are only available on SambaStudio and not Sambaverse we create a local Hugging Face Embeddings Object\n",
    "# In the SambaStudio examples later, we use an Embeddings Models hosted on SambaStudio\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings, collection_name='sambaverse_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = Sambaverse(\n",
    "    sambaverse_model_name=llm_info[\"sambaverse_model_name\"],\n",
    "    sambaverse_api_key=os.getenv(\"SAMBAVERSE_API_KEY\"),\n",
    "    model_kwargs={\n",
    "        \"do_sample\": False,\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"process_prompt\": True,\n",
    "        \"select_expert\": llm_info[\"samabaverse_select_expert\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"How can you use langsmith for testing\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234570",
   "metadata": {},
   "source": [
    "## Example 2: Using SambaStudio to call CoE with Named Expert\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with a named expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234571",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2: Using SambaStudio to call CoE with Named Expert\n",
    "\n",
    "#In this example, we'll use SambaStudio to call the CoE model with a named expert.\n",
    "\n",
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   samabaverse_select_expert: \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Create a SambaStudioEmbeddings object\n",
    "snsdk_model = SambaStudioEmbeddings()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings, collection_name='sambastudio_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaStudio(\n",
    "    streaming=True,\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"select_expert\": llm_info[\"samabaverse_select_expert\"],\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "user_query = \"Tell me how I can use langsmith within applications\"\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234572",
   "metadata": {},
   "source": [
    "## Example 3: Using SambaStudio to call CoE with Routing\n",
    "\n",
    "In this example, we'll use SambaStudio to call the CoE model with routing. The script will automatically determine the appropriate expert based on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the config.yaml file with the following:\n",
    "# api: sambastudio\n",
    "# llm:\n",
    "#   coe_routing: true\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "api_info = config[\"api\"]\n",
    "llm_info = config[\"llm\"]\n",
    "\n",
    "# Create a SambaStudioEmbeddings object\n",
    "snsdk_model = SambaStudioEmbeddings()\n",
    "embeddings = snsdk_model\n",
    "\n",
    "# Load documents and split into chunks\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector database using Chroma\n",
    "vector = Chroma.from_documents(documents, embeddings,collection_name='sambastudio_coe_aisk')\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "user_query = \"Tell me how I can use langsmith for testing\"\n",
    "\n",
    "# Get the expert by calling SambaStudio with a custom prompt workflow\n",
    "expert_response = get_expert(user_query, use_requests=True)\n",
    "print(f\"Router expert response: {expert_response}\")\n",
    "\n",
    "# Extract the expert name from the response\n",
    "expert = get_expert_val(expert_response)\n",
    "print(f\"Routing Named Expert: {expert}\")\n",
    "\n",
    "# Look up the model name based on the expert\n",
    "named_expert = config[\"coe_name_map\"][expert]\n",
    "print(f\"Named expert Model Name: {named_expert}\")\n",
    "\n",
    "# Set up the language model\n",
    "llm = SambaStudio(\n",
    "    streaming=True,\n",
    "    model_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": llm_info[\"temperature\"],\n",
    "        \"max_tokens_to_generate\": llm_info[\"max_tokens_to_generate\"],\n",
    "        \"select_expert\": named_expert,\n",
    "        \"process_prompt\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Invoke the retrieval chain with the user query\n",
    "response = retrieval_chain.invoke({\"input\": user_query})\n",
    "print(f\"Response: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234574",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In each example, we walked through the following steps:\n",
    "\n",
    "1. Update the `config.yaml` file with the appropriate API information and LLM parameters\n",
    "2. Create a `SambaNovaEmbeddingModel` (or a `HuggingFaceEmbeddings`) object for embeddings\n",
    "3. Load documents from a URL and split them into chunks\n",
    "4. Create a vector database using Chroma\n",
    "5. Define the prompt template\n",
    "6. Set up the language model based on the example configuration\n",
    "7. Create the document chain and retrieval chain\n",
    "8. Invoke the retrieval chain with the user query\n",
    "9. Print the response\n",
    "\n",
    "For Example 3, we additionally:\n",
    "- Called `get_expert()` to determine the appropriate expert based on the user query.\n",
    "- Extracted the expert name using `get_expert_val()`.\n",
    "- Looked up the model name based on the expert.\n",
    "\n",
    "Feel free to explore and experiment with different configurations and queries to see how the CoE models respond!\n",
    "\n",
    "If you have any questions or need further assistance, please don't hesitate to ask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
