{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../\")\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "load_dotenv(\"../../export.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sambanova_endpoint import SambaNovaEndpoint\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser, StructuredOutputParser, ResponseSchema\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from vectordb.vector_db import VectorDb\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import ReduceDocumentsChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "model = SambaNovaEndpoint(\n",
    "            model_kwargs={\n",
    "                \"do_sample\": True, \n",
    "                \"temperature\": 0.01,\n",
    "                \"max_tokens_to_generate\": 1500,\n",
    "            }\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_chunks(documents):\n",
    "    #split long document\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size= 800, chunk_overlap= 200)\n",
    "    return  splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce call method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_call(conversation):\n",
    "    reduce_prompt = load_prompt(\"../prompts/reduce.yaml\")\n",
    "    reduce_chain = LLMChain(llm=model, prompt=reduce_prompt)  \n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"transcription_chunks\"\n",
    "    )\n",
    "    # Combines and iteravely reduces the documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=1200,  \n",
    "    )\n",
    "    print(\"reducing call\")\n",
    "    new_document = reduce_documents_chain.invoke(conversation)[\"output_text\"]\n",
    "    print(\"call reduced\")\n",
    "    return new_document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sumarization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(conversation, model=model):\n",
    "    summarization_prompt=load_prompt(\"../prompts/summarization.yaml\")\n",
    "    output_parser = StrOutputParser()\n",
    "    summarization_chain = summarization_prompt | model | output_parser\n",
    "    input_variables={\"conversation\": conversation}\n",
    "    print(\"summarizing\")\n",
    "    summarization_response = summarization_chain.invoke(input_variables)\n",
    "    print(\"summarizing done\")\n",
    "    return summarization_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main topic classification method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_main_topic(conversation, classes, model=model):\n",
    "    topic_classification_prompt=load_prompt(\"../prompts/topic_classification.yaml\")\n",
    "    list_output_parser = CommaSeparatedListOutputParser()\n",
    "    list_format_instructions = list_output_parser.get_format_instructions()\n",
    "    topic_classifcation_chain = topic_classification_prompt | model | list_output_parser\n",
    "    input_variables={\"conversation\":conversation, \"topic_classes\" : \"\\n\\t- \".join(classes), \"format_instructions\": list_format_instructions}\n",
    "    print(\"cassification\")\n",
    "    topic_classifcation_response = topic_classifcation_chain.invoke(input_variables)\n",
    "    print(\"classification done\")\n",
    "    return topic_classifcation_response\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## named entity recognition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_entities(conversation, entities, model=model):\n",
    "    ner_prompt = load_prompt(\"../prompts/ner.yaml\")\n",
    "    response_schemas = []\n",
    "    for entity in entities:\n",
    "        response_schemas.append(ResponseSchema(name=entity, description=f\"{entity}s find in conversation\", type=\"list\"))\n",
    "    entities_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    ner_chain = ner_prompt | model | entities_output_parser\n",
    "    input_variables={\"conversation\":conversation,\n",
    "                     \"entities\" : \"\\n\\t- \".join(entities), \n",
    "                     \"format_instructions\":entities_output_parser.get_format_instructions()\n",
    "                    }\n",
    "    print(\"extracting entities\")\n",
    "    ner_response = ner_chain.invoke(input_variables)\n",
    "    print(\"extracting entities done\")\n",
    "    return ner_response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment analysis method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentiment(conversation, sentiments, model=model):\n",
    "    sentiment_analysis_prompt = load_prompt(\"../prompts/sentiment_analysis.yaml\")\n",
    "    list_output_parser = CommaSeparatedListOutputParser()\n",
    "    list_format_instructions = list_output_parser.get_format_instructions()\n",
    "    sentiment_analysis_chain = sentiment_analysis_prompt | model | list_output_parser\n",
    "    input_variables={\"conversation\":conversation, \"sentiments\":sentiments, \"format_instructions\":list_format_instructions}\n",
    "    print(\"sentiment analysis\")\n",
    "    sentiment_analysis_response = sentiment_analysis_chain.invoke(input_variables)\n",
    "    print(\"sentiment analysis done\")\n",
    "    return sentiment_analysis_response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## factual check method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_retriever(documents_path, urls):\n",
    "    print(\"setting retriever\") \n",
    "    vdb=VectorDb()\n",
    "    retriever = vdb.create_vdb(documents_path,1000,200,\"faiss\",None, load_txt=True, load_pdf=True, urls=urls).as_retriever()\n",
    "    print(\"retriever set\")\n",
    "    return retriever\n",
    "\n",
    "def factual_accuracy_analysis(conversation, retriever, model=model):\n",
    "    factual_accuracy_analysis_response_schemas = [ResponseSchema(name=\"correct\",\n",
    "                                                                 description=\"wether or not the provided information is correct\",\n",
    "                                                                 type=\"bool\"\n",
    "                                                                 ),\n",
    "                                                  ResponseSchema(name=\"errors\",\n",
    "                                                                 description=\"list of summarized errors made by the agent, if there is no errors, emplty list\" ,\n",
    "                                                                 type=\"list\"),\n",
    "                                                  ResponseSchema(name=\"score\",\n",
    "                                                                 description=\"puntuation from 1 to 100 of the overall quallity of the agent\" ,\n",
    "                                                                 type=\"int\")\n",
    "                                                ]\n",
    "    factual_accuracy_analysis_output_parser = StructuredOutputParser.from_response_schemas(factual_accuracy_analysis_response_schemas)\n",
    "    format_instructions=factual_accuracy_analysis_output_parser.get_format_instructions()\n",
    "    retrieval_qa_chat_prompt = load_prompt(\"../prompts/factual_accuracy_analysis.yaml\")\n",
    "    combine_docs_chain = create_stuff_documents_chain(\n",
    "        model, retrieval_qa_chat_prompt\n",
    "    )\n",
    "    retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "    input_variables={\"input\":conversation,\n",
    "                     \"format_instructions\":format_instructions\n",
    "                    }\n",
    "    print(\"factual check\")\n",
    "    model_response=retrieval_chain.invoke(input_variables)[\"answer\"]\n",
    "    factual_accuracy_analysis_response=factual_accuracy_analysis_output_parser.invoke(model_response)\n",
    "    print(\"factual check done\")\n",
    "    return factual_accuracy_analysis_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedural Analysis method\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procedural_accuracy_analysis(conversation, procedures_path, model=model):\n",
    "    \"\"\"\n",
    "    Analyse the procedural accuracy of the given conversation.\n",
    "\n",
    "    Args:\n",
    "        conversation (str): The conversation to analyse.\n",
    "        procedures_path (str): The path to the file containing the procedures.\n",
    "        model (Langchain LLM Model, optional): The language model to use for summarization and classification. Defaults to a SambaNovaEndpoint model.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the procedural accuracy analysis results. The keys are:\n",
    "            - \"correct\": A boolean indicating whether the agent followed all the procedures.\n",
    "            - \"errors\": A list of summarized errors made by the agent, if any.\n",
    "            - \"evaluation\": A list of booleans evaluating if the agent followed each one of the procedures listed.\n",
    "    \"\"\"\n",
    "    procedures_analysis_response_schemas = [ResponseSchema(name=\"correct\",\n",
    "                                                                 description=\"wether or not the agent followed all the procedures\",\n",
    "                                                                 type=\"bool\"\n",
    "                                                                 ),\n",
    "                                                  ResponseSchema(name=\"errors\",\n",
    "                                                                 description=\"list of summarized errors made by the agent, if there is no errors, emplty list\" ,\n",
    "                                                                 type=\"list\"),\n",
    "                                                  ResponseSchema(name=\"evaluation\",\n",
    "                                                                 description=\"list of booleans evaluating if the agent followed each one of the procedures listed\" ,\n",
    "                                                                 type=\"list[bool]\")\n",
    "                                                ]\n",
    "    procedures_analysis_output_parser = StructuredOutputParser.from_response_schemas(procedures_analysis_response_schemas)\n",
    "    format_instructions=procedures_analysis_output_parser.get_format_instructions()\n",
    "    procedures_prompt = load_prompt(\"../prompts/procedures_analysis.yaml\")\n",
    "    with open(procedures_path, 'r') as file:\n",
    "        procedures = file.readlines()\n",
    "    procedures_chain = procedures_prompt | model | procedures_analysis_output_parser\n",
    "    input_variables={\"input\":conversation,\n",
    "                     \"procedures\":procedures,\n",
    "                     \"format_instructions\":format_instructions\n",
    "                    }\n",
    "    print(\"proceduress check\")\n",
    "    procedures_analysis_response = procedures_chain.invoke(input_variables)\n",
    "    print(\"proceduress check done\")\n",
    "    return procedures_analysis_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPS prediction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nps(conversation, model=model):\n",
    "    nps_response_schemas = [ResponseSchema(name=\"description\",\n",
    "                                            description=\"reazoning\",\n",
    "                                            type=\"str\"\n",
    "                                            ),\n",
    "                            ResponseSchema(name=\"score\",\n",
    "                                            description=\"puntuation from 1 to 10 of the NPS\" ,\n",
    "                                            type=\"int\")\n",
    "                        ]\n",
    "    nps_output_parser = StructuredOutputParser.from_response_schemas(nps_response_schemas)\n",
    "    format_instructions=nps_output_parser.get_format_instructions()\n",
    "    nps_prompt = load_prompt(\"../prompts/nps.yaml\")\n",
    "    nps_chain = nps_prompt | model | nps_output_parser\n",
    "    input_variables={\"conversation\":conversation, \"format_instructions\":format_instructions}\n",
    "    print(f\"predicting nps\")\n",
    "    nps = nps_chain.invoke(input_variables)\n",
    "    print(f\"nps chain finished\")\n",
    "    return nps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quallity assement method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_call_quallity_assesment(conversation, factual_result, procedures_result):\n",
    "    total_score = 0\n",
    "    # predict a NPS of the call\n",
    "    nps = get_nps(conversation)\n",
    "    total_score += nps[\"score\"]*10\n",
    "    # include the factual analysis score\n",
    "    total_score += factual_result[\"score\"]\n",
    "    # include the procedures analysis score\n",
    "    if len(procedures_result[\"evaluation\"])==0:\n",
    "        total_score += 1\n",
    "    else:\n",
    "        total_score += procedures_result[\"evaluation\"].count(True)/len(procedures_result[\"evaluation\"])\n",
    "    # Simple average\n",
    "    overall_score = total_score / 3\n",
    "    return overall_score   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# complete analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"../data/conversations/transcription\"\n",
    "conversations = os.listdir(path)\n",
    "documents = []\n",
    "for conversation in conversations:\n",
    "    conversation_path=os.path.join(path, conversation)\n",
    "    loader = TextLoader(conversation_path)\n",
    "    documents.extend(loader.load())\n",
    "documents\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size= 800, chunk_overlap= 200)\n",
    "chunks = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting retrieverreducing call\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 54.03it/s]\n",
      "0it [00:00, ?it/s]\n",
      "2024-03-07 16:18:57,545 [INFO] - Total 1 files loaded\n",
      "2024-03-07 16:18:57,546 [INFO] - Splitter: splitting documents\n",
      "2024-03-07 16:18:57,547 [INFO] - Total 1 chunks created\n",
      "2024-03-07 16:18:57,549 [INFO] - Load pretrained SentenceTransformer: hkunlp/instructor-large\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 16:19:01,102 [INFO] - Use pytorch device: cpu\n",
      "2024-03-07 16:19:01,103 [INFO] - Processing embeddings using hkunlp/instructor-large. This could take time depending on the number of chunks ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 16:19:01,973 [INFO] - Vector store saved to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriever set\n",
      "call reduced\n",
      "summarizing\n",
      "extracting entities\n",
      "sentiment analysis\n",
      "cassification\n",
      "factual check\n",
      "proceduress check\n",
      "classification done\n",
      "extracting entities done\n",
      "summarizing done\n",
      "sentiment analysis done\n",
      "proceduress check done\n",
      "factual check done\n",
      "predicting nps\n",
      "nps chain finished\n",
      "{'classification': ['Undefined'],\n",
      " 'entities': {'address': [],\n",
      "              'city': [],\n",
      "              'customer_name': ['Kim'],\n",
      "              'payment_type': ['$35 charge on my checking account']},\n",
      " 'factual_analysis': {'correct': True, 'errors': [], 'score': 100},\n",
      " 'procedural_analysis': {'correct': True,\n",
      "                         'errors': [],\n",
      "                         'evaluation': [True, False, False, True, True, True]},\n",
      " 'quality_score': 63.55555555555555,\n",
      " 'sentiment': 'positive',\n",
      " 'summary': '\\n'\n",
      "            'Kim, a bank customer, called to inquire about a $35 charge on her '\n",
      "            'checking account. Adam, a customer service representative, '\n",
      "            'explained that the charge was for the monthly service fee for her '\n",
      "            'private client checking account. Kim was not aware of this fee '\n",
      "            'and requested that it be removed. Adam apologized for the '\n",
      "            'oversight and removed the fee for the current year. He also '\n",
      "            'offered to send Kim an email with the benefits of her private '\n",
      "            'client account, which she accepted. The conversation ended with '\n",
      "            'Adam asking if there was anything else he could assist Kim with, '\n",
      "            'and Kim responding that there was not.'}\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def call_analysis_parallel(conversation, documents_path, facts_urls, procedures_path, classes_list, entities_list, sentiment_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submitting tasks to executor\n",
    "        reduced_conversation_future = executor.submit(reduce_call, conversation=conversation)\n",
    "        retriever = set_retriever(documents_path=documents_path, urls=facts_urls)\n",
    "        reduced_conversation = reduced_conversation_future.result()\n",
    "        summary_future = executor.submit(get_summary, conversation=reduced_conversation)\n",
    "        classification_future = executor.submit(classify_main_topic, conversation=reduced_conversation, classes=classes_list)\n",
    "        entities_future = executor.submit(get_entities, conversation=reduced_conversation, entities=entities_list)\n",
    "        sentiment_future = executor.submit(get_sentiment, conversation=reduced_conversation, sentiments=sentiment_list)\n",
    "        factual_analysis_future = executor.submit(factual_accuracy_analysis, conversation=reduced_conversation, retriever = retriever)\n",
    "        procedural_analysis_future = executor.submit(procedural_accuracy_analysis, conversation=reduced_conversation, procedures_path=procedures_path)\n",
    "\n",
    "        # Retrieving results\n",
    "        summary = summary_future.result()\n",
    "        classification = classification_future.result()\n",
    "        entities = entities_future.result()\n",
    "        sentiment = sentiment_future.result()\n",
    "        factual_analysis = factual_analysis_future.result()\n",
    "        procedural_analysis = procedural_analysis_future.result()\n",
    "    quality_score = get_call_quallity_assesment(reduced_conversation, factual_analysis, procedural_analysis)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"classification\": classification,\n",
    "        \"entities\": entities,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"factual_analysis\": factual_analysis,\n",
    "        \"procedural_analysis\": procedural_analysis,\n",
    "        \"quality_score\": quality_score\n",
    "    }\n",
    "    \n",
    "classes = [\"medical emergecy\", \"animals emergency\", \"terrorism emergency\", \"fire emergency\", \"undefined\"]   \n",
    "entities = [\"city\", \"address\", \"customer_name\", \"payment_type\"]\n",
    "sentiments = [\"positive\", \"negative\", \"neutral\"]\n",
    "pprint(call_analysis_parallel(conversation=chunks,\n",
    "                              documents_path=\"../data/documents\", \n",
    "                              facts_urls=[],\n",
    "                              procedures_path=\"../data/documents/example_procedures.txt\",\n",
    "                              classes_list=classes,\n",
    "                              entities_list=entities, \n",
    "                              sentiment_list=sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
