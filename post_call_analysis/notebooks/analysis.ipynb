{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "load_dotenv(os.path.join(repo_dir, '.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser, StructuredOutputParser, ResponseSchema\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from utils.vectordb.vector_db import VectorDb\n",
    "from utils.model_wrappers.api_gateway import APIGateway\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import ReduceDocumentsChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "model = APIGateway.load_llm(type='sncloud', select_expert='llama3-8b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(documents):\n",
    "    # split long document\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "    return splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce call method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_call(conversation):\n",
    "    reduce_prompt = load_prompt(os.path.join(kit_dir, 'prompts/reduce.yaml'))\n",
    "    reduce_chain = LLMChain(llm=model, prompt=reduce_prompt)\n",
    "    combine_documents_chain = StuffDocumentsChain(llm_chain=reduce_chain, document_variable_name='transcription_chunks')\n",
    "    # Combines and iteravelly reduces the documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=1200,\n",
    "    )\n",
    "    print('reducing call')\n",
    "    new_document = reduce_documents_chain.invoke(conversation)['output_text']\n",
    "    print('call reduced')\n",
    "    return new_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sumarization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(conversation, model=model):\n",
    "    summarization_prompt = load_prompt(os.path.join(kit_dir, 'prompts/summarization.yaml'))\n",
    "    output_parser = StrOutputParser()\n",
    "    summarization_chain = summarization_prompt | model | output_parser\n",
    "    input_variables = {'conversation': conversation}\n",
    "    print('summarizing')\n",
    "    summarization_response = summarization_chain.invoke(input_variables)\n",
    "    print('summarizing done')\n",
    "    return summarization_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main topic classification method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_main_topic(conversation, classes, model=model):\n",
    "    topic_classification_prompt = load_prompt(os.path.join(kit_dir, 'prompts/topic_classification.yaml'))\n",
    "    list_output_parser = CommaSeparatedListOutputParser()\n",
    "    list_format_instructions = list_output_parser.get_format_instructions()\n",
    "    topic_classification_chain = topic_classification_prompt | model | list_output_parser\n",
    "    input_variables = {\n",
    "        'conversation': conversation,\n",
    "        'topic_classes': '\\n\\t- '.join(classes),\n",
    "        'format_instructions': list_format_instructions,\n",
    "    }\n",
    "    print('classification')\n",
    "    topic_classification_response = topic_classification_chain.invoke(input_variables)\n",
    "    print('classification done')\n",
    "    return topic_classification_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## named entity recognition method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(conversation, entities, model=model):\n",
    "    ner_prompt = load_prompt(os.path.join(kit_dir, 'prompts/ner.yaml'))\n",
    "    response_schemas = []\n",
    "    for entity in entities:\n",
    "        response_schemas.append(ResponseSchema(name=entity, description=f'{entity}s find in conversation', type='list'))\n",
    "    entities_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    ner_chain = ner_prompt | model | entities_output_parser\n",
    "    input_variables = {\n",
    "        'conversation': conversation,\n",
    "        'entities': '\\n\\t- '.join(entities),\n",
    "        'format_instructions': entities_output_parser.get_format_instructions(),\n",
    "    }\n",
    "    print('extracting entities')\n",
    "    ner_response = ner_chain.invoke(input_variables)\n",
    "    print('extracting entities done')\n",
    "    return ner_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment analysis method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(conversation, sentiments, model=model):\n",
    "    sentiment_analysis_prompt = load_prompt(os.path.join(kit_dir, 'prompts/sentiment_analysis.yaml'))\n",
    "    list_output_parser = CommaSeparatedListOutputParser()\n",
    "    list_format_instructions = list_output_parser.get_format_instructions()\n",
    "    sentiment_analysis_chain = sentiment_analysis_prompt | model | list_output_parser\n",
    "    input_variables = {\n",
    "        'conversation': conversation,\n",
    "        'sentiments': sentiments,\n",
    "        'format_instructions': list_format_instructions,\n",
    "    }\n",
    "    print('sentiment analysis')\n",
    "    sentiment_analysis_response = sentiment_analysis_chain.invoke(input_variables)\n",
    "    print('sentiment analysis done')\n",
    "    return sentiment_analysis_response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## factual check method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_retriever(documents_path, urls):\n",
    "    print('setting retriever')\n",
    "    vdb = VectorDb()\n",
    "    retriever = vdb.create_vdb(\n",
    "        documents_path, 1000, 200, 'faiss', None, load_txt=True, load_pdf=True, urls=urls\n",
    "    ).as_retriever()\n",
    "    print('retriever set')\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def factual_accuracy_analysis(conversation, retriever, model=model):\n",
    "    factual_accuracy_analysis_response_schemas = [\n",
    "        ResponseSchema(name='correct', description='wether or not the provided information is correct', type='bool'),\n",
    "        ResponseSchema(\n",
    "            name='errors',\n",
    "            description='list of summarized errors made by the agent, if there is no errors, empty list',\n",
    "            type='list',\n",
    "        ),\n",
    "        ResponseSchema(\n",
    "            name='score', description='punctuation from 1 to 100 of the overall quality of the agent', type='int'\n",
    "        ),\n",
    "    ]\n",
    "    factual_accuracy_analysis_output_parser = StructuredOutputParser.from_response_schemas(\n",
    "        factual_accuracy_analysis_response_schemas\n",
    "    )\n",
    "    format_instructions = factual_accuracy_analysis_output_parser.get_format_instructions()\n",
    "    retrieval_qa_chat_prompt = load_prompt(os.path.join(kit_dir, 'prompts/factual_accuracy_analysis.yaml'))\n",
    "    combine_docs_chain = create_stuff_documents_chain(model, retrieval_qa_chat_prompt)\n",
    "    retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "    input_variables = {'input': conversation, 'format_instructions': format_instructions}\n",
    "    print('factual check')\n",
    "    model_response = retrieval_chain.invoke(input_variables)['answer']\n",
    "    factual_accuracy_analysis_response = factual_accuracy_analysis_output_parser.invoke(model_response)\n",
    "    print('factual check done')\n",
    "    return factual_accuracy_analysis_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedural Analysis method\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procedural_accuracy_analysis(conversation, procedures_path, model=model):\n",
    "    \"\"\"\n",
    "    Analyse the procedural accuracy of the given conversation.\n",
    "\n",
    "    Args:\n",
    "        conversation (str): The conversation to analyse.\n",
    "        procedures_path (str): The path to the file containing the procedures.\n",
    "        model (Langchain LLM Model, optional): The language model to use for summarization and classification.\n",
    "            Defaults to a SambaNovaEndpoint model.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the procedural accuracy analysis results. The keys are:\n",
    "            - \"correct\": A boolean indicating whether the agent followed all the procedures.\n",
    "            - \"errors\": A list of summarized errors made by the agent, if any.\n",
    "            - \"evaluation\": A list of booleans evaluating if the agent followed each one of the procedures listed.\n",
    "    \"\"\"\n",
    "    procedures_analysis_response_schemas = [\n",
    "        ResponseSchema(name='correct', description='wether or not the agent followed all the procedures', type='bool'),\n",
    "        ResponseSchema(\n",
    "            name='errors',\n",
    "            description='list of summarized errors made by the agent, if there is no errors, empty list',\n",
    "            type='list',\n",
    "        ),\n",
    "        ResponseSchema(\n",
    "            name='evaluation',\n",
    "            description='list of booleans evaluating if the agent followed each one of the procedures listed',\n",
    "            type='list[bool]',\n",
    "        ),\n",
    "    ]\n",
    "    procedures_analysis_output_parser = StructuredOutputParser.from_response_schemas(\n",
    "        procedures_analysis_response_schemas\n",
    "    )\n",
    "    format_instructions = procedures_analysis_output_parser.get_format_instructions()\n",
    "    procedures_prompt = load_prompt(os.path.join(kit_dir, 'prompts/procedures_analysis.yaml'))\n",
    "    with open(procedures_path, 'r') as file:\n",
    "        procedures = file.readlines()\n",
    "    procedures_chain = procedures_prompt | model | procedures_analysis_output_parser\n",
    "    input_variables = {'input': conversation, 'procedures': procedures, 'format_instructions': format_instructions}\n",
    "    print('procedures check')\n",
    "    procedures_analysis_response = procedures_chain.invoke(input_variables)\n",
    "    print('procedures check done')\n",
    "    return procedures_analysis_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPS prediction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nps(conversation, model=model):\n",
    "    nps_response_schemas = [\n",
    "        ResponseSchema(name='description', description='reasoning', type='str'),\n",
    "        ResponseSchema(name='score', description='punctuation from 1 to 10 of the NPS', type='int'),\n",
    "    ]\n",
    "    nps_output_parser = StructuredOutputParser.from_response_schemas(nps_response_schemas)\n",
    "    format_instructions = nps_output_parser.get_format_instructions()\n",
    "    nps_prompt = load_prompt(os.path.join(kit_dir, 'prompts/nps.yaml'))\n",
    "    nps_chain = nps_prompt | model | nps_output_parser\n",
    "    input_variables = {'conversation': conversation, 'format_instructions': format_instructions}\n",
    "    print(f'predicting nps')\n",
    "    nps = nps_chain.invoke(input_variables)\n",
    "    print(f'nps chain finished')\n",
    "    return nps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quallity assement method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_call_quality_assessment(conversation, factual_result, procedures_result):\n",
    "    total_score = 0\n",
    "    # predict a NPS of the call\n",
    "    nps = get_nps(conversation)\n",
    "    total_score += nps['score'] * 10\n",
    "    # include the factual analysis score\n",
    "    total_score += factual_result['score']\n",
    "    # include the procedures analysis score\n",
    "    if len(procedures_result['evaluation']) == 0:\n",
    "        total_score += 1\n",
    "    else:\n",
    "        total_score += procedures_result['evaluation'].count(True) / len(procedures_result['evaluation'])\n",
    "    # Simple average\n",
    "    overall_score = total_score / 3\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# complete analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(kit_dir, 'data/conversations/transcription')\n",
    "conversations = os.listdir(path)\n",
    "documents = []\n",
    "for conversation in conversations:\n",
    "    conversation_path = os.path.join(path, conversation)\n",
    "    loader = TextLoader(conversation_path)\n",
    "    documents.extend(loader.load())\n",
    "documents\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def call_analysis_parallel(\n",
    "    conversation, documents_path, facts_urls, procedures_path, classes_list, entities_list, sentiment_list\n",
    "):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submitting tasks to executor\n",
    "        reduced_conversation_future = executor.submit(reduce_call, conversation=conversation)\n",
    "        retriever = set_retriever(documents_path=documents_path, urls=facts_urls)\n",
    "        reduced_conversation = reduced_conversation_future.result()\n",
    "        summary_future = executor.submit(get_summary, conversation=reduced_conversation)\n",
    "        classification_future = executor.submit(\n",
    "            classify_main_topic, conversation=reduced_conversation, classes=classes_list\n",
    "        )\n",
    "        entities_future = executor.submit(get_entities, conversation=reduced_conversation, entities=entities_list)\n",
    "        sentiment_future = executor.submit(get_sentiment, conversation=reduced_conversation, sentiments=sentiment_list)\n",
    "        factual_analysis_future = executor.submit(\n",
    "            factual_accuracy_analysis, conversation=reduced_conversation, retriever=retriever\n",
    "        )\n",
    "        procedural_analysis_future = executor.submit(\n",
    "            procedural_accuracy_analysis, conversation=reduced_conversation, procedures_path=procedures_path\n",
    "        )\n",
    "\n",
    "        # Retrieving results\n",
    "        summary = summary_future.result()\n",
    "        classification = classification_future.result()\n",
    "        entities = entities_future.result()\n",
    "        sentiment = sentiment_future.result()\n",
    "        factual_analysis = factual_analysis_future.result()\n",
    "        procedural_analysis = procedural_analysis_future.result()\n",
    "    quality_score = get_call_quality_assessment(reduced_conversation, factual_analysis, procedural_analysis)\n",
    "\n",
    "    return {\n",
    "        'summary': summary,\n",
    "        'classification': classification,\n",
    "        'entities': entities,\n",
    "        'sentiment': sentiment,\n",
    "        'factual_analysis': factual_analysis,\n",
    "        'procedural_analysis': procedural_analysis,\n",
    "        'quality_score': quality_score,\n",
    "    }\n",
    "\n",
    "\n",
    "classes = ['medical emergency', 'animals emergency', 'terrorism emergency', 'fire emergency', 'undefined']\n",
    "entities = ['city', 'address', 'customer_name', 'payment_type']\n",
    "sentiments = ['positive', 'negative', 'neutral']\n",
    "pprint(\n",
    "    call_analysis_parallel(\n",
    "        conversation=chunks,\n",
    "        documents_path=os.path.join(kit_dir, 'data/documents'),\n",
    "        facts_urls=[],\n",
    "        procedures_path=os.path.join(kit_dir, 'data/documents/example_procedures.txt'),\n",
    "        classes_list=classes,\n",
    "        entities_list=entities,\n",
    "        sentiment_list=sentiments,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
