llm: 
  "model": "Meta-Llama-3.3-70B-Instruct" 
  "max_tokens": 2048
  "temperature": 0.01
  "top_p": null
  "top_k": null


embedding_model: 
    "model": "E5-Mistral-7B-Instruct" 

prompts: 
    "generate_qa_prompt": "prompts/generate_Q&A.yaml"

splitting: 
  breakpoint_threshold_amount: 80
  min_doc_length: 80

generation:
  output_path: "./synthetic_data.jsonl"
  amount_per_document: 5
  include_context: true
  include_thoughts: true
  include_references: true
  system_prompt: "You are a helpful assistant for question-answering tasks.\n"