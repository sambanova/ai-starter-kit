llm: 
  "api": "sncloud"  # set either sambastudio, sncloud
  "do_sample": False
  "temperature": 0.01
  "max_tokens": 2048
  "model": "Meta-Llama-3.3-70B-Instruct" 

embedding_model: 
    "type": "sncloud" # set either sncloud sambastudio or cpu
    "model": "e5-mistral-7b-instruct" #set if using sncloud or SambaStudio Bundle embedding expert

prompts: 
    "generate_qa_prompt": "prompts/generate_Q&A.yaml"

splitting: 
  breakpoint_threshold_amount: 80
  min_doc_length: 80

generation:
  output_path: "./synthetic_data.jsonl"
  amount_per_document: 5
  include_context: true
  include_thoughts: true
  include_references: true
  system_prompt: "You are a helpful assistant for question-answering tasks.\n"