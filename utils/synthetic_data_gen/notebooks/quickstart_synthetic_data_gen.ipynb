{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Synthetic Data Generation\n",
    "\n",
    "This guide provides a quickstart for creating a synthetic QA and Retrieval-Augmented Generation (RAG) dataset using your own PDF document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "We recommend using Python 3.11. Make sure you have the necessary packages installed. If not, install them using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel==6.29.5\n",
    "!pip install langchain-sambanova==0.1.6\n",
    "!pip install \"unstructured[pdf,local-inference]\"\n",
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use [SambaNova Cloud](https://cloud.sambanova.ai) models, you'll need to set your API key. Run the following code to securely input your [SambaNova Cloud API Key](https://cloud.sambanova.ai/apis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if not os.getenv(\"SAMBANOVA_API_KEY\"):\n",
    "    os.environ[\"SAMBANOVA_API_KEY\"] = getpass.getpass(\n",
    "        \"Enter your SambaNova Cloud API key: \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the LLM to be used to generate the QA pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_sambanova import ChatSambaNova\n",
    "\n",
    "# Initialize the LLM and specify the model\n",
    "llm = ChatSambaNova(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    "    model=\"Meta-Llama-3.1-8B-Instruct\",\n",
    "    temperature=0.01,\n",
    "    max_tokens=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, specify the location of the PDF file to process and extracts elements from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location of the PDF file\n",
    "filename = \"./data/SambaNova_Dataflow.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will load the data from your source file using the [Unstructured](https://docs.unstructured.io/open-source/introduction/quick-start) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "def extract_pdf(file_path):\n",
    "    \"\"\"Extract text and tables from PDF file\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        List[Element]: A list of document elements (text, tables, etc.) extracted from the PDF.\n",
    "    \"\"\"\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=file_path,\n",
    "        extract_images_in_pdf=False, # Keep False\n",
    "        strategy='hi_res',\n",
    "        hi_res_model_name='yolox',\n",
    "        infer_table_structure=True, # Set to True to enable table detection; otherwise, set to False.\n",
    "        chunking_strategy='by_title',\n",
    "        max_characters=4096,\n",
    "        combine_text_under_n_chars=500)\n",
    "\n",
    "    return raw_pdf_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will store the extracted elements into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text_documents = []\n",
    "table_documents = []\n",
    "raw_pdf_elements = extract_pdf(filename)\n",
    "for document in raw_pdf_elements:\n",
    "    if document.category == 'Table':\n",
    "        #transform table documents into langchain documents\n",
    "        table_documents.append(Document(page_content=document.metadata.text_as_html))\n",
    "    else:\n",
    "        if document.metadata.text_as_html is not None:\n",
    "                table_documents.append(Document(page_content=document.metadata.text_as_html))\n",
    "        else:\n",
    "            text_documents.append(Document(page_content=document.text))\n",
    "\n",
    "print(len(table_documents))        \n",
    "print(len(text_documents))\n",
    "documents = text_documents + table_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate QA pairs\n",
    "\n",
    "With our granular documents ready, we can use a Large Language Model (LLM) to create QA pairs. Consider the following:\n",
    "\n",
    "- Depending on the dataset's purpose, you may want the model to include references used to generate the answer.\n",
    "- You might want the model to include reasoning steps from context to answer. A good strategy for this is [Chain of Thought (CoT)](https://www.promptingguide.ai/techniques/cot).\n",
    "- The model should generate a structured output from which we can extract the question, the thought process, the answer, and the references\n",
    "\n",
    "First, we'll define the schema for the QA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "class SyntheticDatum(BaseModel):\n",
    "    \"\"\"Model of a synthetic generated datum\"\"\"\n",
    "    question: str = Field(description='generated question')\n",
    "    answer: str = Field(description='generated answer')\n",
    "    references: list[str] = Field(description='references for generated answer')\n",
    "    thought: str = Field(description='thought for answer generation')\n",
    "\n",
    "\n",
    "class SyntheticData(BaseModel):\n",
    "    \"\"\"Model of a synthetic data generation\"\"\"\n",
    "    data: list[SyntheticDatum] = Field(description='synthetic data pairs')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a prompt instructing the model to generate QA pairs using the provided document and the specified number of QA pairs. The prompt will ask the model to generate a list of JSON objects containing the question, thought process, answer, and references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate([\n",
    "        (\"system\", \"You are a JSON generator who generates machine-readable JSON\"),\n",
    "        (\"human\", \"\"\"\n",
    "            Based on the following document, follow the instruction below\n",
    "            Document:\n",
    "            {document}\n",
    "            Instruction:\n",
    "            Generate {amount} of unique question, thought, answer, and references from the above document in the following JSON format. \n",
    "            The answers must avoid words that are not specific (e.g., \"many\", \"several\", \"few\", etc.). \n",
    "            The answers must contain specific, verbose, self-contained, grammatically correct sentences that answer the question comprehensively. \n",
    "            The answers must strictly contain content from the document and no content from outside the document. \n",
    "            There may be multiple references that contain verbatim text from the document to support the answers.             \n",
    "            JSON format:\n",
    "            [\n",
    "                {{\n",
    "                    \"question\": \"<generated question>\",            \n",
    "                    \"thought\": \"<generated thought on what is needed to answer the question. Start with 'To answer the question, I need'>\",\n",
    "                    \"answer\": \"<generated answer>\",\n",
    "                    \"references\": [\n",
    "                        \"<verbatim text from document that supports the answer>\",\n",
    "                        \"<verbatim text from document that supports the answer>\"\n",
    "                    ]\n",
    "                }}\n",
    "            ]\n",
    "            The first character of the response must be '[' and the last character must be ']'. No header text should be included.\n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the prompt defined, we can create a method to instantiate a LangChain chain, pass the input arguments (the context document and the number of QA pairs to generate), and process the model's response using the defined QA data schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(context, amount, include_context = False, include_thoughts = False, include_references = False):\n",
    "    \"\"\"Generate synthetic QA pairs from a given context using a LangChain chain.\n",
    "\n",
    "    Args:\n",
    "        context (str): The source text to generate questions and answers from.\n",
    "        amount (int): Number of QA pairs to generate.\n",
    "        include_context (bool): Whether to include the original context in each output entry.\n",
    "        include_thoughts (bool): Whether to include model 'thoughts' in each QA pair.\n",
    "        include_references (bool): Whether to include reference sources in each QA pair.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of dictionaries containing QA pairs (and optional metadata).\n",
    "    \"\"\"\n",
    "\n",
    "    synthetic_datum_parser = JsonOutputParser(pydantic_object=SyntheticData)\n",
    "    qa_generate_chain = prompt | llm | synthetic_datum_parser\n",
    "    qa_pairs = []\n",
    "    generation = qa_generate_chain.invoke({'document': context, 'amount': amount})\n",
    "    for datum in generation:\n",
    "        qa_pair = {\n",
    "            'question': datum['question'],\n",
    "            'context': context if include_context else None,\n",
    "            'answer': datum['answer'],\n",
    "            'thought': datum['thought'] if include_thoughts else None,\n",
    "            'references': datum['references'] if include_references else None,\n",
    "        }\n",
    "        qa_pair = {k: v for k, v in qa_pair.items() if v is not None}\n",
    "        qa_pairs.append(qa_pair)\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example where we create a series of synthetic data pairs, including the original context (useful for training models for Retrieval-Augmented Generation (RAG) applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc=\"\"\"Elephants are the largest living land animals. \n",
    "Three living species are currently recognised:\n",
    "the African bush elephant (Loxodonta africana),\n",
    "the African forest elephant (L. cyclotis), and the Asian elephant (Elephas maximus). \n",
    "They are the only surviving members of the family Elephantidae and the order Proboscidea;\n",
    "extinct relatives include mammoths and mastodons.\"\"\"\n",
    "\n",
    "generate_qa_pairs(sample_doc, 5, include_context = True, include_thoughts = True, include_references = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate full dataset\n",
    "\n",
    "We will create a simple method to convert each QA pair dictionary into a single string with the format required for the fine-tuning process. Then, we will iterate over each chunk of our source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_pairs_to_prompt_completion(qa_pairs):\n",
    "    \"\"\"Converts QA pair dictionaries into prompt-completion strings formatted for fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (Union[dict, List[dict]]): A single QA pair or a list of QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of JSON-formatted strings, each representing a prompt-completion example.\n",
    "    \"\"\"\n",
    "    # Ensure input is a list of QA pairs\n",
    "    if isinstance(qa_pairs, dict):\n",
    "        qa_pairs = [qa_pairs]\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    for pair in qa_pairs:\n",
    "        #line = {'prompt': f'{\"You are a helpful assistant for question-answering tasks.\"}{pair[\"question\"]}', 'completion': ''}\n",
    "        line = {'prompt': f'{pair[\"question\"]}', 'completion': ''}\n",
    "\n",
    "        # Optionally include context if available \n",
    "        if pair.get('context'):\n",
    "            line['prompt'] += f'\\nContext: {pair[\"context\"]}\\n'\n",
    "        \n",
    "        # Optionally include the model's \"thoughts\" before the answer\n",
    "        if pair.get('thought'):\n",
    "            line['completion'] += f'Thought: {pair[\"thought\"]}\\n'\n",
    "        \n",
    "        # Append the answer directly to the completion\n",
    "        line['completion'] += f'Answer: {pair[\"answer\"]}\\n'\n",
    "\n",
    "        # Optionally include references at the end\n",
    "        if pair.get('references'):\n",
    "            line['completion'] += f'References: {pair[\"references\"]}\\n'\n",
    "        \n",
    "        # Convert the prompt-completion pair to a JSONL line\n",
    "        lines.append(json.dumps(line))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for document in documents:\n",
    "    try: \n",
    "        qa_pairs = generate_qa_pairs(\n",
    "            context=document.page_content,\n",
    "            amount=5,\n",
    "            include_context=False,\n",
    "            include_thoughts=False,\n",
    "            include_references=False,\n",
    "        )\n",
    "        lines.extend(qa_pairs_to_prompt_completion(qa_pairs))\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Q&A pairs for document: {document.page_content}\")\n",
    "        print(e)\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the list of JSON strings into a jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.jsonl\", \"w\") as f:\n",
    "    for line in lines:\n",
    "        json_obj = json.loads(line)  # ensure it's valid JSON\n",
    "        f.write(json.dumps(json_obj) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syndatgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
