{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambanNova Langchain Wrappers Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_embeddings import SambaStudioEmbeddings\n",
    "from langchain_llms import SambaStudio, SambaNovaCloud\n",
    "from langchain_sambanova import ChatSambaNovaCloud, ChatSambaStudio\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "utils_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(utils_dir, '..'))\n",
    "\n",
    "load_dotenv(os.path.join(repo_dir, '.env'), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaStudio LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SambaStudio(\n",
    "    model_kwargs={\n",
    "        'do_sample': False,\n",
    "        'temperature': 0.01,\n",
    "        'max_tokens': 256,\n",
    "        'process_prompt': False,\n",
    "        'model': 'Meta-Llama-3-70B-Instruct-4096',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As the clock struck midnight, a lone figure crept into the abandoned mansion. Suddenly, a chandelier crashed down, and a ghostly figure materialized. \"You shouldn\\'t have come here,\" it whispered. The intruder froze, trapped in a century-old curse, forever doomed to roam the haunted halls.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('tell me a 50 word tale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SambaStudio(\n",
    "    model_kwargs={\n",
    "        'do_sample': False,\n",
    "        'max_tokens': 256,\n",
    "        'temperature': 0.01,\n",
    "        'process_prompt': False,\n",
    "        'model': 'Meta-Llama-3-70B-Instruct-4096',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the clock struck midnight, a lone figure crept into the abandoned mansion. Suddenly, a chandelier crashed down, and a ghostly figure materialized. \"You shouldn't have come here,\" it whispered. The intruder froze, trapped in a century-old curse, forever doomed to roam the haunted halls."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream('tell me a 50 word tale'):\n",
    "    print(chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaNovaCloud LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SambaNovaCloud(model='llama3-70b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello. How can I assist you today?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "llm.invoke(json.dumps([{'role': 'user', 'content': 'hello'}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello. How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here's a long story \n",
      "for you:\n",
      "\n",
      "Once upon \n",
      "a time, in a small village \n",
      "nestled in the rolling hills of \n",
      "rural France, there lived a \n",
      "young girl named Sophie. Sophie \n",
      "was a curious and adventurous \n",
      "child, with a mop of curly \n",
      "brown hair and a smile that \n",
      "could light up the darkest \n",
      "of rooms. She lived with \n",
      "her parents, Pierre and \n",
      "Colette, in a small stone cottage \n",
      "on the outskirts of \n",
      "the village.\n",
      "\n",
      "Sophie's village was \n",
      "a charming \n",
      "place, filled with narrow \n",
      "cobblestone streets, quaint shops, \n",
      "and \n",
      "bustling cafes. The villagers \n",
      "were a tight-knit \n",
      "community, and everyone knew each \n",
      "other's names and stories. Sophie \n",
      "loved listening to the villagers' \n",
      "tales of \n",
      "old, which \n",
      "often featured brave knights, \n",
      "beautiful princesses, and \n",
      "magical creatures.\n",
      "\n",
      "One day, while exploring \n",
      "the village, Sophie stumbled upon \n",
      "a small, mysterious shop tucked \n",
      "away on a quiet street. \n",
      "The sign above the door \n",
      "read \"Curios \n",
      "and Wonders,\" and the \n",
      "windows were filled \n",
      "with a dazzling array of strange \n",
      "and exotic objects. Sophie's \n",
      "curiosity was piqued, \n",
      "and she pushed open the door \n",
      "to venture inside.\n",
      "\n",
      "The shop \n",
      "was dimly lit, and \n",
      "the air was thick with the \n",
      "scent of old books and \n",
      "dust. Sophie's eyes \n",
      "adjusted slowly, and she \n",
      "saw that the shop was filled \n",
      "with all manner of curious \n",
      "objects: vintage \n",
      "clocks, rare coins, \n",
      "and even a \n",
      "taxidermied owl perched on \n",
      "a shelf. Behind the counter stood \n",
      "an old man with a kind \n",
      "face \n",
      "and a twinkle in his eye.\n",
      "\n",
      "\n",
      "\n",
      "\"Bonjour, mademoiselle,\" he \n",
      "said, his voice low and \n",
      "soothing. \"Welcome to Curios \n",
      "and Wonders. I \n",
      "am Monsieur LaFleur, \n",
      "the proprietor. How may I \n",
      "assist you \n",
      "today?\"\n",
      "\n",
      "Sophie wandered the aisles, \n",
      "running her fingers over \n",
      "the strange objects on \n",
      "display. She picked up \n",
      "a small, delicate music \n",
      "box and wound \n",
      "it up, listening \n",
      "as it played \n",
      "a soft, melancholy \n",
      "tune. Monsieur LaFleur \n",
      "smiled and nodded \n",
      "in approval.\n",
      "\n",
      "\"Ah, you have a \n",
      "good ear for \n",
      "music, mademoiselle,\" he \n",
      "said. \"That music box \n",
      "is a \n",
      "rare and precious item. It \n",
      "was crafted by a skilled artisan \n",
      "in the 18th century.\"\n",
      "\n",
      "\n",
      "As Sophie continued to \n",
      "explore the shop, \n",
      "she stumbled upon \n",
      "a large, leather-bound book \n",
      "with strange symbols etched into \n",
      "the cover. \n",
      "Monsieur LaFleur noticed her interest and \n",
      "approached \n",
      "her.\n",
      "\n",
      "\"Ah, you've found \n",
      "the infamous 'Livre \n",
      "\n",
      "des Secrets,'\" \n",
      "he said, his \n",
      "voice low and mysterious. \n",
      "\"That book is said to contain \n",
      "the secrets of the universe, \n",
      "hidden within its pages. But \n",
      "be \n",
      "warned, mademoiselle, \n",
      "the book is said to \n",
      "be cursed. Many have attempted \n",
      "to unlock its secrets, but \n",
      "none have \n",
      "succeeded.\"\n",
      "\n",
      "Sophie's eyes widened with \n",
      "excitement as she carefully opened \n",
      "the book. The pages \n",
      "were yellowed and \n",
      "crackling, and \n",
      "the text was written in a \n",
      "language she couldn't understand. \n",
      "But as she turned the \n",
      "pages, \n",
      "she felt a strange sensation, \n",
      "as if the book \n",
      "was calling \n",
      "to her.\n",
      "\n",
      "Monsieur \n",
      "LaFleur smiled \n",
      "and \n",
      "nodded. \"I see you have a \n",
      "connection to the \n",
      "book, mademoiselle. Perhaps you \n",
      "are the one who can unlock \n",
      "its secrets.\"\n",
      "\n",
      "Over the next \n",
      "few weeks, Sophie returned to \n",
      "the shop again and again, \n",
      "pouring over \n",
      "the pages of the Livre \n",
      "des Secrets. She spent hours \n",
      "studying \n",
      "the symbols and trying to decipher \n",
      "the text. \n",
      "Monsieur \n",
      "LaFleur watched her with a \n",
      "keen eye, offering guidance and encouragement \n",
      "whenever she needed it.\n",
      "\n",
      "As \n",
      "the days turned into weeks, \n",
      "Sophie began to notice strange occurrences \n",
      "happening around her. She would \n",
      "find objects moved from their \n",
      "usual places, and she would hear \n",
      "whispers in the night. She \n",
      "began \n",
      "to feel as though the book \n",
      "was exerting some kind of \n",
      "influence over her, drawing her \n",
      "deeper into \n",
      "its secrets.\n",
      "\n",
      "One \n",
      "night, Sophie had a vivid dream \n",
      "in which \n",
      "she saw herself standing in \n",
      "a \n",
      "grand, \n",
      "candlelit hall. \n",
      "The walls were lined with \n",
      "ancient tapestries, and the \n",
      "air was thick with the scent \n",
      "of \n",
      "incense. At the far end of \n",
      "the hall, she saw a \n",
      "figure cloaked in shadows.\n",
      "\n",
      "\n",
      "As she approached \n",
      "the figure, it stepped forward, \n",
      "revealing a woman \n",
      "with long, flowing hair and \n",
      "piercing green eyes. The woman \n",
      "spoke in a voice that was \n",
      "both familiar and yet \n",
      "completely alien.\n",
      "\n",
      "\"Sophie, you \n",
      "have been chosen to unlock the \n",
      "secrets of the Livre \n",
      "des Secrets,\" she \n",
      "said. \"But be warned, \n",
      "the \n",
      "journey will \n",
      "be difficult, and the cost \n",
      "will be high. Are you \n",
      "prepared to pay \n",
      "the price?\"\n",
      "\n",
      "Sophie woke up with \n",
      "a start, her heart racing and \n",
      "her mind reeling. She \n",
      "knew that she had \n",
      "to return to the shop and \n",
      "confront Monsieur LaFleur \n",
      "about the \n",
      "strange \n",
      "occurrences. But when she \n",
      "arrived at the shop, she \n",
      "found that it \n",
      "was closed, \n",
      "and \n",
      "a sign on the door \n",
      "read \"Gone on \n",
      "a \n",
      "journey. Will return \n",
      "soon.\"\n",
      "\n",
      "Sophie \n",
      "was devastated. \n",
      "She felt as though she had \n",
      "been abandoned, left \n",
      "to navigate the mysteries of \n",
      "the Livre des Secrets on \n",
      "her own. But as \n",
      "she turned to leave, she \n",
      "noticed a\n"
     ]
    }
   ],
   "source": [
    "for i in llm.stream('hello tell me a long story'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaStudio Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaStudio(\n",
    "    model=\"Meta-Llama-3.3-70B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    top_p=0.01,\n",
    "    do_sample = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What do you call a fake noodle?\\n\\nAn impasta.', additional_kwargs={}, response_metadata={'id': 'item0', 'partial': False, 'value': {'completion': 'What do you call a fake noodle?\\n\\nAn impasta.', 'completion_tokens_count': 13, 'logprobs': {'text_offset': [], 'top_logprobs': []}, 'model_execution_time': 0.43772339820861816, 'prompt': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ntell me a joke<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', 'prompt_tokens_count': 39, 'stop_reason': 'end_of_text', 'tokens': ['What', ' do', ' you', ' call', ' a', ' fake', ' nood', 'le', '?\\n\\n', 'An', ' imp', 'asta', '.'], 'total_tokens_count': 52}, 'params': {}, 'status': None}, id='item0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Yer lookin' fer a joke, eh? Alright then, matey, here be one fer ye:\\n\\nWhy did the pirate quit his job?\\n\\n(pause fer dramatic effect, savvy?)\\n\\nBecause he was sick o' all the arrrr-guments!\\n\\nYarrr, I hope that made ye laugh, me hearty!\", additional_kwargs={}, response_metadata={'id': 'item0', 'partial': False, 'value': {'completion': \"Yer lookin' fer a joke, eh? Alright then, matey, here be one fer ye:\\n\\nWhy did the pirate quit his job?\\n\\n(pause fer dramatic effect, savvy?)\\n\\nBecause he was sick o' all the arrrr-guments!\\n\\nYarrr, I hope that made ye laugh, me hearty!\", 'completion_tokens_count': 66, 'logprobs': {'text_offset': [], 'top_logprobs': []}, 'model_execution_time': 0.9974634647369385, 'prompt': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant with pirate accent<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ntell me a joke<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', 'prompt_tokens_count': 47, 'stop_reason': 'end_of_text', 'tokens': ['Y', 'er', ' look', 'in', \"'\", ' fer', ' a', ' joke', ',', ' eh', '?', ' Alright', ' then', ',', ' mate', 'y', ',', ' here', ' be', ' one', ' fer', ' ye', ':\\n\\n', 'Why', ' did', ' the', ' pirate', ' quit', ' his', ' job', '?\\n\\n', '(p', 'ause', ' fer', ' dramatic', ' effect', ',', ' savvy', '?)\\n\\n', 'Because', ' he', ' was', ' sick', ' o', \"'\", ' all', ' the', ' arr', 'rr', '-g', 'uments', '!\\n\\n', 'Y', 'ar', 'rr', ',', ' I', ' hope', ' that', ' made', ' ye', ' laugh', ',', ' me', ' hearty', '!'], 'total_tokens_count': 113}, 'params': {}, 'status': None}, id='item0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant with pirate accent\"),\n",
    "    HumanMessage(content=\"tell me a joke\")\n",
    "]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a joke for you:\\n\\nWhat do you call a fake noodle?\\n\\nAn impasta!\\n\\nHope that made you laugh! Do you want to hear another one?\", additional_kwargs={}, response_metadata={'id': 'item0', 'partial': False, 'value': {'completion': \"Here's a joke for you:\\n\\nWhat do you call a fake noodle?\\n\\nAn impasta!\\n\\nHope that made you laugh! Do you want to hear another one?\", 'completion_tokens_count': 34, 'logprobs': {'text_offset': [], 'top_logprobs': []}, 'model_execution_time': 0.6353631019592285, 'prompt': '<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|> tell me a joke <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n', 'prompt_tokens_count': 15, 'stop_reason': 'end_of_text', 'tokens': ['Here', \"'s\", ' a', ' joke', ' for', ' you', ':\\n\\n', 'What', ' do', ' you', ' call', ' a', ' fake', ' nood', 'le', '?\\n\\n', 'An', ' imp', 'asta', '!\\n\\n', 'Hope', ' that', ' made', ' you', ' laugh', '!', ' Do', ' you', ' want', ' to', ' hear', ' another', ' one', '?'], 'total_tokens_count': 49}, 'params': {}, 'status': None}, id='item0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_response = llm.ainvoke(\"tell me a joke\")\n",
    "await future_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh!\", additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 14, 'completion_tokens': 32, 'total_tokens': 46, 'throughput_after_first_token': 70.06468839908355, 'time_to_first_token': 0.2191941738128662, 'model_execution_time': 0.5902798175811768}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913510}, id='f6ef319e-4ba2-4117-8d63-f5823d0bc947'),\n",
       " AIMessage(content='The capital of the United Kingdom (UK) is London.', additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 17, 'completion_tokens': 12, 'total_tokens': 29, 'throughput_after_first_token': 62.08816650383397, 'time_to_first_token': 0.21888446807861328, 'model_execution_time': 0.315521240234375}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913509}, id='6145cc9d-7db4-4699-a47a-22445a7fdc49')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.batch([\"tell me a joke\", \"which is the capital of UK?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh!\", additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 14, 'completion_tokens': 32, 'total_tokens': 46, 'throughput_after_first_token': 70.03251686884015, 'time_to_first_token': 0.21899962425231934, 'model_execution_time': 0.5902557373046875}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913513}, id='33123501-e370-4a70-bda1-92c8af70865b'),\n",
       " AIMessage(content='The capital of the United Kingdom (UK) is London.', additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 17, 'completion_tokens': 12, 'total_tokens': 29, 'throughput_after_first_token': 62.36218692927395, 'time_to_first_token': 0.21871089935302734, 'model_execution_time': 0.3149230480194092}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913512}, id='d4d5e44a-c6eb-4298-be34-57b08e691931')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_responses = llm.abatch([\"tell me a joke\", \"which is the capital of UK?\"])\n",
    "await future_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"tell me a joke\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, \n",
      "listen up, matey! Here \n",
      "be a joke fer ye:\n",
      "\n",
      "\n",
      "Why did the pirate quit his job?\n",
      "\n",
      "\n",
      "Because he was \n",
      "sick o' all \n",
      "the arrrr-guments! (get \n",
      "it? arguments, but with a \n",
      "pirate \"arrr\" \n",
      "sound? Aye, I be \n",
      "a regular comedic genius, savvy?)\n",
      "\n",
      "\n",
      "So, did I make \n",
      "ye laugh, or did I \n",
      "walk the plank?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant with pirate accent\"),\n",
    "    HumanMessage(content=\"tell me a joke\"),\n",
    "]\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "\n",
      "Why couldn't the bicycle stand \n",
      "up by itself?\n",
      "\n",
      "(Wait \n",
      "for it...)\n",
      "\n",
      "Because it \n",
      "was two-tired!\n",
      "\n",
      "Hope that \n",
      "made you laugh!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream(\"tell me a joke\"):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "\n",
    "@tool()\n",
    "def get_time(kind: str = 'both') -> str:\n",
    "    \"\"\"Returns current date, current time or both.\n",
    "\n",
    "    Args:\n",
    "        kind: date, time or both\n",
    "    \"\"\"\n",
    "    if kind == 'date':\n",
    "        date = datetime.now().strftime('%d/%m/%Y')\n",
    "        return f'Current date: {date}'\n",
    "    elif kind == 'time':\n",
    "        time = datetime.now().strftime('%H:%M:%S')\n",
    "        return f'Current time: {time}'\n",
    "    else:\n",
    "        date = datetime.now().strftime('%d/%m/%Y')\n",
    "        time = datetime.now().strftime('%H:%M:%S')\n",
    "        return f'Current date: {date}, Current time: {time}'\n",
    "    \n",
    "@tool()  \n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "tools=[get_time, add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaStudio(    \n",
    "    model=\"Meta-Llama-3.3-70B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    top_p=0.01,\n",
    ")\n",
    "tool_llm=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_time',\n",
       "  'args': {'kind': 'time'},\n",
       "  'id': 'call_44a33627fb9442ebb9',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=tool_llm.invoke(\"what time is it?\")\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaStudio(\n",
    "    model=\"Meta-Llama-3.3-70B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    top_p=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join a band?', punchline='Because it wanted to be the purr-cussionist!')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pydantic\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'punchline': 'Because it wanted to be the purr-cussionist!',\n",
       " 'rating': 8,\n",
       " 'setup': 'Why did the cat join a band?'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# TypedDict\n",
    "class Joke(TypedDict):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
    "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
    "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'punchline': 'Because it wanted to be the purr-cussionist!',\n",
       " 'setup': 'Why did the cat join a band?'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json schema\n",
    "json_schema = {\n",
    "    \"title\": \"joke\",\n",
    "    \"description\": \"Joke to tell user.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"setup\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The setup of the joke\",\n",
    "        },\n",
    "        \"punchline\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The punchline to the joke\",\n",
    "        },\n",
    "        \"rating\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
    "            \"default\": None,\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"setup\", \"punchline\"],\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'They are the same weight',\n",
       " 'justification': 'A pound is a unit of weight or mass, so a pound of bricks and a pound of feathers both weigh the same amount, one pound. The difference is in their density and volume. A pound of feathers would take up more space than a pound of bricks due to the difference in their densities.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using json_mode method\n",
    "structured_llm = llm.with_structured_output(\n",
    "    method=\"json_mode\",\n",
    "    include_raw=False\n",
    ")\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Answer the following question. \"\n",
    "    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
    "    \"What's heavier a pound of bricks or a pound of feathers?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw': AIMessage(content='{\\n  \"answer\": \"They are the same weight\",\\n  \"justification\": \"A pound is a unit of weight or mass, so a pound of bricks and a pound of feathers both weigh the same amount, one pound. The difference is in their density and volume. A pound of feathers would take up more space than a pound of bricks due to the difference in their densities.\"\\n}', additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 70, 'completion_tokens': 79, 'total_tokens': 149, 'model_execution_time': 1.191061019897461}, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': '', 'created': 1732049872}, id='6ab145da-b369-498e-b928-67b5a09ec299'),\n",
       " 'parsed': AnswerWithJustification(answer='They are the same weight', justification='A pound is a unit of weight or mass, so a pound of bricks and a pound of feathers both weigh the same amount, one pound. The difference is in their density and volume. A pound of feathers would take up more space than a pound of bricks due to the difference in their densities.'),\n",
       " 'parsing_error': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "# Using json_schema method\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "structured_llm = llm.with_structured_output(\n",
    "    AnswerWithJustification,\n",
    "    method=\"json_schema\", \n",
    "    include_raw=True\n",
    "    )\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Answer the following question. \"\n",
    "    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
    "    \"What's heavier a pound of bricks or a pound of feathers?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaNova Cloud Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaNovaCloud(\n",
    "    model= \"Meta-Llama-3.3-70B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.01,\n",
    "    stream_options={'include_usage':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What do you call a fake noodle?\\n\\nAn impasta.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 19, 'completion_tokens': 13, 'completion_tokens_after_first_per_sec': 472.65089024115395, 'completion_tokens_after_first_per_sec_first_ten': 1673.3181312335958, 'completion_tokens_per_sec': 89.81238085801046, 'end_time': 1731021565.6574025, 'is_last_response': True, 'prompt_tokens': 39, 'start_time': 1731021565.4950366, 'time_to_first_token': 0.1369771957397461, 'total_latency': 0.14474619062323318, 'total_tokens': 52, 'total_tokens_per_sec': 359.2495234320418}, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1731021565}, id='d928193e-a158-43ee-a4b5-e01d74392ea5')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Yer lookin' fer a joke, eh? Alright then, matey, here be one fer ye:\\n\\nWhy did the pirate quit his job?\\n\\n(pause fer dramatic effect, savvy?)\\n\\nBecause he was sick o' all the arrrr-guments!\\n\\nYarrr, I hope that made ye laugh, me hearty!\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 8, 'completion_tokens': 66, 'completion_tokens_after_first_per_sec': 520.5848397648267, 'completion_tokens_after_first_per_sec_first_ten': 653.7256857855361, 'completion_tokens_per_sec': 268.51617358915166, 'end_time': 1731021567.2902174, 'is_last_response': True, 'prompt_tokens': 47, 'start_time': 1731021567.0205224, 'time_to_first_token': 0.1448354721069336, 'total_latency': 0.24579524993896484, 'total_tokens': 113, 'total_tokens_per_sec': 459.73223659960814}, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1731021567}, id='70258699-0c16-4a94-b6fc-1beffd0d8072')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant with pirate accent\"),\n",
    "    HumanMessage(content=\"tell me a joke\")\n",
    "    ]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What do you call a fake noodle?\\n\\nAn impasta.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 19, 'completion_tokens': 13, 'completion_tokens_after_first_per_sec': 489.30284647690155, 'completion_tokens_after_first_per_sec_first_ten': 1688.027451811057, 'completion_tokens_per_sec': 90.03422957386933, 'end_time': 1731021568.2752926, 'is_last_response': True, 'prompt_tokens': 39, 'start_time': 1731021568.1140797, 'time_to_first_token': 0.136688232421875, 'total_latency': 0.14438952897724353, 'total_tokens': 52, 'total_tokens_per_sec': 360.1369182954773}, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1731021568}, id='49a3ef85-5a04-48de-84b0-b30742ea6c55')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_response = llm.ainvoke(\"tell me a joke\")\n",
    "await(future_response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaNovaCloud(\n",
    "    model= \"Meta-Llama-3.3-70B-Instruct\",\n",
    "    streaming=False,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.01,\n",
    "    stream_options={'include_usage':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='What do you call a fake noodle?\\n\\nAn impasta.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 19, 'completion_tokens': 13, 'completion_tokens_after_first_per_sec': 491.99565986647247, 'completion_tokens_after_first_per_sec_first_ten': 1677.686280288836, 'completion_tokens_per_sec': 89.53407040415136, 'end_time': 1731021571.2208374, 'is_last_response': True, 'prompt_tokens': 39, 'start_time': 1731021571.0589995, 'time_to_first_token': 0.13744735717773438, 'total_latency': 0.14519612412703664, 'total_tokens': 52, 'total_tokens_per_sec': 358.13628161660546}, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1731021571}, id='2e51b075-b37b-4354-acc8-3609d73b3fa7'),\n",
       " AIMessage(content='The capital of the United Kingdom (UK) is London.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 17, 'completion_tokens': 12, 'completion_tokens_after_first_per_sec': 446.9373631696212, 'completion_tokens_after_first_per_sec_first_ten': 1502.922833716249, 'completion_tokens_per_sec': 82.13761971030976, 'end_time': 1731021571.2464256, 'is_last_response': True, 'prompt_tokens': 42, 'start_time': 1731021571.0837018, 'time_to_first_token': 0.13811182975769043, 'total_latency': 0.14609627162708955, 'total_tokens': 54, 'total_tokens_per_sec': 369.61928869639394}, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1731021571}, id='594e3bf7-1c7b-4730-b088-22196bfdf04f')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.batch([\"tell me a joke\",\"which is the capital of UK?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='What do you call a fake noodle?\\n\\nAn impasta.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 19, 'completion_tokens': 13, 'completion_tokens_after_first_per_sec': 451.44540317517266, 'completion_tokens_after_first_per_sec_first_ten': 1555.5078076202374, 'completion_tokens_per_sec': 84.31290126209767, 'end_time': 1731021572.4831522, 'is_last_response': True, 'prompt_tokens': 39, 'start_time': 1731021572.3107407, 'time_to_first_token': 0.1458301544189453, 'total_latency': 0.1541875538073088, 'total_tokens': 52, 'total_tokens_per_sec': 337.2516050483907}, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1731021572}, id='ac0c43e3-0129-4f7b-a778-817498d7d845'),\n",
       " AIMessage(content='The capital of the United Kingdom (UK) is London.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 17, 'completion_tokens': 12, 'completion_tokens_after_first_per_sec': 448.0267238951631, 'completion_tokens_after_first_per_sec_first_ten': 1497.713997647454, 'completion_tokens_per_sec': 82.86277349020392, 'end_time': 1731021572.4724288, 'is_last_response': True, 'prompt_tokens': 42, 'start_time': 1731021572.3110712, 'time_to_first_token': 0.13680553436279297, 'total_latency': 0.14481774498434627, 'total_tokens': 54, 'total_tokens_per_sec': 372.88248070591766}, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1731021572}, id='2fbb3597-107e-43e9-9b2a-8c7a436757af')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_responses = llm.abatch([\"tell me a joke\",\"which is the capital of UK?\"])\n",
    "await(future_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaNovaCloud(\n",
    "    model= \"Meta-Llama-3.3-70B-Instruct\",\n",
    "    streaming=True,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.01,\n",
    "    stream_options={'include_usage':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What do you call a fake \n",
      "\n",
      "noodle?\n",
      "\n",
      "An impasta.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"tell me a joke\"):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yer lookin' fer a \n",
      "joke, eh? Alright \n",
      "then, matey, here be \n",
      "one fer ye:\n",
      "\n",
      "Why did the pirate quit his job?\n",
      "\n",
      "\n",
      "\n",
      "(pause fer \n",
      "dramatic effect, savvy?)\n",
      "\n",
      "Because he was sick o' \n",
      "all the arrrr-guments!\n",
      "\n",
      "\n",
      "\n",
      "Yarrr, I hope that made \n",
      "ye \n",
      "laugh, \n",
      "me hearty!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant with pirate accent\"),\n",
    "    HumanMessage(content=\"tell me a joke\")\n",
    "    ]\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "\n",
      "\n",
      "An impasta.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream(\"tell me a joke\"):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "\n",
    "@tool()\n",
    "def get_time(kind: str = 'both') -> str:\n",
    "    \"\"\"Returns current date, current time or both.\n",
    "\n",
    "    Args:\n",
    "        kind: date, time or both\n",
    "    \"\"\"\n",
    "    if kind == 'date':\n",
    "        date = datetime.now().strftime('%d/%m/%Y')\n",
    "        return f'Current date: {date}'\n",
    "    elif kind == 'time':\n",
    "        time = datetime.now().strftime('%H:%M:%S')\n",
    "        return f'Current time: {time}'\n",
    "    else:\n",
    "        date = datetime.now().strftime('%d/%m/%Y')\n",
    "        time = datetime.now().strftime('%H:%M:%S')\n",
    "        return f'Current date: {date}, Current time: {time}'\n",
    "    \n",
    "@tool()  \n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "tools=[get_time, add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaNovaCloud(\n",
    "    model=\"Meta-Llama-3.3-70B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    top_p=0.01,\n",
    ")\n",
    "tool_llm=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_time',\n",
       "  'args': {'kind': 'time'},\n",
       "  'id': 'call_2f4673153d314dfdb2',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=tool_llm.invoke(\"what time is it?\")\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSambaNovaCloud(\n",
    "    model=\"Meta-Llama-3.3-70B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    top_p=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join a band?', punchline='Because it wanted to be the purr-cussionist!')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pydantic\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'punchline': 'Because it wanted to be the purr-cussionist!',\n",
       " 'rating': 8,\n",
       " 'setup': 'Why did the cat join a band?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# TypedDict\n",
    "class Joke(TypedDict):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
    "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
    "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'punchline': 'Because it wanted to be the purr-cussionist!',\n",
       " 'rating': None,\n",
       " 'setup': 'Why did the cat join a band?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json schema\n",
    "json_schema = {\n",
    "    \"title\": \"joke\",\n",
    "    \"description\": \"Joke to tell user.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"setup\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The setup of the joke\",\n",
    "        },\n",
    "        \"punchline\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The punchline to the joke\",\n",
    "        },\n",
    "        \"rating\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
    "            \"default\": None,\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"setup\", \"punchline\"],\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'They are the same weight',\n",
       " 'justification': 'One pound is a unit of weight or mass, so a pound of bricks and a pound of feathers both weigh the same amount, one pound. The difference is in their density and volume. A pound of feathers would take up more space than a pound of bricks due to the difference in their densities.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using json_mode method\n",
    "structured_llm = llm.with_structured_output(\n",
    "    method=\"json_mode\",\n",
    "    include_raw=False\n",
    ")\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Answer the following question. \"\n",
    "    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
    "    \"What's heavier a pound of bricks or a pound of feathers?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw': AIMessage(content='{\\n  \"answer\": \"They are the same weight\",\\n  \"justification\": \"A pound is a unit of weight or mass, so a pound of bricks and a pound of feathers both weigh the same amount, one pound. The difference is in their density and volume. A pound of feathers would take up more space than a pound of bricks due to the difference in their densities.\"\\n}', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 4.722222222222222, 'completion_tokens': 79, 'completion_tokens_after_first_per_sec': 358.09983493653596, 'completion_tokens_after_first_per_sec_first_ten': 418.25878333749813, 'completion_tokens_per_sec': 242.5663740319178, 'end_time': 1731530854.610591, 'is_last_response': True, 'prompt_tokens': 70, 'start_time': 1731530854.2559688, 'time_to_first_token': 0.13680577278137207, 'total_latency': 0.32568405375761145, 'total_tokens': 149, 'total_tokens_per_sec': 457.4986041867817}, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1731530854}, id='bcbc8178-d2c5-4b99-a52e-bf1b83561cec'),\n",
       " 'parsed': AnswerWithJustification(answer='They are the same weight', justification='A pound is a unit of weight or mass, so a pound of bricks and a pound of feathers both weigh the same amount, one pound. The difference is in their density and volume. A pound of feathers would take up more space than a pound of bricks due to the difference in their densities.'),\n",
       " 'parsing_error': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "# Using json_schema method\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "structured_llm = llm.with_structured_output(\n",
    "    AnswerWithJustification,\n",
    "    method=\"json_schema\", \n",
    "    include_raw=True\n",
    "    )\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Answer the following question. \"\n",
    "    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
    "    \"What's heavier a pound of bricks or a pound of feathers?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sambastudio Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = SambaStudioEmbeddings(batch_size=1, model_kwargs={'select_expert': 'e5-mistral-7b-instruct'})\n",
    "embedding.embed_documents(['tell me a 50 word tale', 'tell me a joke'])\n",
    "embedding.embed_query('tell me a 50 word tale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgep/Documents/ask_public_own/finetuning_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='tell me a 50 word tale'),\n",
       " Document(page_content='tell me a joke'),\n",
       " Document(page_content='give me 3 party activities'),\n",
       " Document(page_content='give me three healty dishes')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "docs = [\n",
    "    'tell me a 50 word tale',\n",
    "    'tell me a joke',\n",
    "    'when was America discoverd?',\n",
    "    'how to build an engine?',\n",
    "    'give me 3 party activities',\n",
    "    'give me three healty dishes',\n",
    "]\n",
    "docs = [Document(doc) for doc in docs]\n",
    "\n",
    "query = 'prompt for generating something fun'\n",
    "\n",
    "vectordb = Chroma.from_documents(docs, embedding)\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
