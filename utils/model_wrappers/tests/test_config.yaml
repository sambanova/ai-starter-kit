embedding_model: 
    type: "sncloud"
    batch_size: 1
    bundle: True
    model: "E5-mistral-7B-Instruct"

sn_llm:
    model: "Meta-Llama-3.3-70B-Instruct"
    max_tokens: 1200
    temperature: 0.0
    top_p: 0.0
    top_k: 1

sambastudio_llm:
    model_kwargs:
      model: "Meta-Llama-3-70B-Instruct-4096"
      temperature: 0.0
      do_sample: False
      max_tokens_to_generate: 1200

sn_chat_model:
    model: "Meta-Llama-3.3-70B-Instruct"
    streaming: True
    max_tokens: 1024
    temperature: 0.7
    top_k: 1
    top_p: 0.01
    stream_options : 
        include_usage: True

sambastudio_chat_model:
    model: "Meta-Llama-3-70B-Instruct-4096"
    streaming: true
    max_tokens: 1200
    temperature: 0.0
    top_p: null
    top_k: null
    do_sample: false
    process_prompt: false
    stream_options:
      include_usage: true
    special_tokens:
      start: "<|begin_of_text|>"
      start_role: "<|begin_of_text|><|start_header_id|>{role}<|end_header_id|>"
      end_role: "<|eot_id|>"
      end: "<|start_header_id|>assistant<|end_header_id|>\n"
    model_kwargs: null

bad_format_embeddings_model:
    type:
      - type: "string"
        batch_size: 1
        bundle: True
        model: "e5-mistral-7b-instruct"

    string:
      - type: "cpu"
        batch_size: 1
        bundle: True
        model: 1
      
      - type: "sambastudio"
        batch_size: 1
        bundle: True
        model: 1

    boolean:
      - type: "cpu"
        batch_size: 1
        bundle: "string"
        model: "e5-mistral-7b-instruct"
      
      - type: "sambastudio"
        batch_size: 1
        bundle: "string"
        model: "e5-mistral-7b-instruct"

    integer:
      - type: "cpu"
        batch_size: "string"
        bundle: True
        model: "e5-mistral-7b-instruct"
      
      - type: "sambastudio"
        batch_size: "string"
        bundle: True
        model: "e5-mistral-7b-instruct"

bad_format_llm_model:
    string:
      - type: "sncloud"
        model: 1
        max_tokens: 1024
        temperature: 0.0
        streaming: False

      - type: "sambastudio"
        model: 1
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        bundle: True
        select_expert: "Meta-Llama-3-70B-Instruct-4096"
    
    boolean:
      - type: "sncloud"
        model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: "string"

      - type: "sambastudio"
        model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: "string"
        bundle: True
        select_expert: "Meta-Llama-3-70B-Instruct-4096"

    integer:
      - type: "sncloud"
        model: "foo"
        max_tokens: "one"
        temperature: 0.0
        streaming: False

      - type: "sambastudio"
        model: "foo"
        max_tokens: "one"
        temperature: 0.0
        streaming: False
        bundle: True
        select_expert: "Meta-Llama-3-70B-Instruct-4096"

    number:
      - type: "sncloud"
        model: "foo"
        max_tokens: 1024
        temperature: "string"
        streaming: False

      - type: "sambastudio"
        model: "foo"
        max_tokens: 1024
        temperature: "string"
        streaming: False
        bundle: True
        select_expert: "Meta-Llama-3-70B-Instruct-4096"

bad_format_chat_model:
    string:
      - model: 1
        max_tokens: 1024
        temperature: 0.0
        streaming: False

    boolean:
      - model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: "string"

    integer:
      - model: "foo"
        max_tokens: "one"
        temperature: 0.0
        streaming: False
      
      - model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        top_k: "string"

    number:
      - model: "foo"
        max_tokens: 1024
        temperature: "string"
        streaming: False
      
      - model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        top_p: "string"

    dict:
      - model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        stream_options: "string"

    sambastudio_boolean:
      - model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        do_sample: "string"
      
      - model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        process_prompt: "string"
    
    sambastudio_dict:
      - model: "foo"
        max_tokens: 1024
        temperature: 0.0
        streaming: False
        special_tokens: "string"
