{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kit_dir: /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag\n",
      "repo_dir: /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petrojm/miniconda3/envs/complex_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle # remove\n",
    "import shutil\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\")) # absolute path for ekr_rag directory\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"../../../\")) # absolute path for starter-kit directory\n",
    "print('kit_dir: %s'%kit_dir)\n",
    "print('repo_dir: %s'%repo_dir)\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from src.document_retrieval import DocumentRetrieval\n",
    "\n",
    "CONFIG_PATH = os.path.join(kit_dir,'config.yaml')\n",
    "PERSIST_DIRECTORY = os.path.join(kit_dir,f\"data/my-vector-db\")\n",
    "#save_location = kit_dir + \"/data/my-vector-db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader: pypdf2\n",
      "Nb of chunks: 6\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# Specify PDF file\n",
    "pdf_file = kit_dir + '/data/tmp/2405.07518v1_1page.pdf'\n",
    "\n",
    "# Initialize DocumentRetrieval class\n",
    "documentRetrieval =  DocumentRetrieval()\n",
    "print('Loader: %s'%documentRetrieval.loaders['pdf'])\n",
    "\n",
    "# Get pdf text\n",
    "raw_text = []\n",
    "meta_data = []\n",
    "if documentRetrieval.loaders['pdf'] == \"unstructured\":\n",
    "    loader = UnstructuredPDFLoader(pdf_file)\n",
    "    docs_unstructured = loader.load()\n",
    "    for doc in docs_unstructured:\n",
    "        raw_text.append(doc.page_content)\n",
    "        meta_data.append({\"filename\": pdf_file})\n",
    "elif documentRetrieval.loaders['pdf'] == \"pypdf2\":\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    for page in pdf_reader.pages:\n",
    "        raw_text.append(page.extract_text())\n",
    "        meta_data.append({\"filename\": pdf_file})#, \"page\": page_number})\n",
    "else:\n",
    "    raise ValueError(f\"{self.documentRetrieval.loaders['pdf']} is not a valid pdf loader\")\n",
    "\n",
    "# Get the text chunks\n",
    "text_chunks = documentRetrieval.get_text_chunks_with_metadata(docs=raw_text, meta_data=meta_data) # lst of langchain_core.documents.base.Document\n",
    "#print(text_chunks[0].page_content)\n",
    "print('Nb of chunks: %d'%len(text_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petrojm/miniconda3/envs/complex_rag/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "2024-06-21 11:03:48,539 [INFO] - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 11:03:50,183 [INFO] - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      "The directory Chroma has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 11:03:50,754 [INFO] - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-06-21 11:03:57,501 [INFO] - Vector store saved to /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/my-vector-db\n"
     ]
    }
   ],
   "source": [
    "# Create vector store\n",
    "embeddings = documentRetrieval.load_embedding_model()\n",
    "if os.path.exists(PERSIST_DIRECTORY):\n",
    "    shutil.rmtree(PERSIST_DIRECTORY)\n",
    "    print(f\"The directory Chroma has been deleted.\")\n",
    "#vectorstore = documentRetrieval.create_vector_store(text_chunks, embeddings, output_db=None)\n",
    "vectorstore = documentRetrieval.create_vector_store(text_chunks, embeddings, output_db=PERSIST_DIRECTORY)\n",
    "\n",
    "# Create conversation chain\n",
    "documentRetrieval.init_retriever(vectorstore)\n",
    "conversation = documentRetrieval.get_qa_retrieval_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 11:04:00,848 [WARNING] - Number of requested results 15 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a monolithic model?\n",
      "A monolithic model is a large, single model that has billions or trillions of parameters, requiring significant computational resources and expertise to train and serve.\n",
      "\n",
      "Source #1:\n",
      "Source: /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405, Text: \n",
      "and updated on their data [ 1], [10], [16]. Finally, systems\n",
      "that cater to monolothic models have scaled compute TFLOPs\n",
      "much faster than memory bandwidth and capacity, creating the\n",
      "memory wall [ 35] where the memory system can no longer\n",
      "feed the compute efﬁciently.\n",
      "The ML research community has responded with\n",
      "ecosystems of much smaller, modular models that are\n",
      "just as capable, but are cheaper and easier to train and\n",
      "serve [ 5], [37], [43], [58]. Smaller models like the 7B-\n",
      "parameter Llama 3 [ 13], Llama 2 [ 68], and Mistral 7B [ 44]\n",
      "are often adequate. They might not match the performance\n",
      "of larger models over a general suite of tasks, but smaller\n",
      "models can deliver superior accuracy on a narrower set of\n",
      "specialized tasks for which they have been ﬁne-tuned [ 8],\n",
      "[65]. For example, Flan-T5-XL only has 3B parameters,\n",
      "but it surpasses the 175B-parameter GPT-3’s MMLU score\n",
      "by nearly 10% [ 31]. Proof points like these have bolstered\n",
      "community activity in building and training smaller models\n",
      "by specializing base models to a domain, by ﬁne-tuning base\n",
      "models to a speciﬁc task or group of tasks [ 66], [69], and by\n",
      "distilling or compressing larger models into smaller models.\n",
      "\n",
      "{'filename': '/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405.07518v1_1page.pdf'}\n",
      "\n",
      "Source #2:\n",
      "Source: /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405, Text: \n",
      "designed for enterprise inference and training applications. The\n",
      "chip introduces a new three-tier memory system with on-chip\n",
      "distributed SRAM, on-package HBM, and off-package DDR\n",
      "DRAM. A dedicated inter-RDU network enables scaling up and\n",
      "out over multiple sockets. We demonstrate speedups ranging from\n",
      "2⇥to13⇥on various benchmarks running on eight RDU sockets\n",
      "compared with an unfused baseline. We show that for CoE\n",
      "inference deployments, the 8-socket RDU Node reduces machine\n",
      "footprint by up to 19⇥, speeds up model switching time by 15⇥\n",
      "to31⇥, and achieves an overall speedup of 3.7⇥over a DGX\n",
      "H100 and 6.6⇥over a DGX A100.\n",
      "I. I NTRODUCTION\n",
      "Recent advances in the training and inference of large\n",
      "language models (LLMs) has taken the world by storm. State-\n",
      "of-the-art generative AI/ML applications like ChatGPT [ 7]\n",
      "and Gemini [ 3] are built on top of monolithic LLMs that\n",
      "can have billions or trillions of parameters. They are trained\n",
      "with curated datasets that consist of trillions of tokens scraped\n",
      "from the web. However, training and serving a state-of-\n",
      "the-art monolithic LLM is both an extraordinarily expensive\n",
      "affair and a complex systems engineering challenge. Training\n",
      "\n",
      "{'filename': '/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405.07518v1_1page.pdf'}\n",
      "\n",
      "Source #3:\n",
      "Source: /Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405, Text: \n",
      "from the web. However, training and serving a state-of-\n",
      "the-art monolithic LLM is both an extraordinarily expensive\n",
      "affair and a complex systems engineering challenge. Training\n",
      "requires building and operating a supercomputer composed ofthousands of hosts, purpose-built networks, power and cooling\n",
      "infrastructure, and thousands of accelerators – typically\n",
      "GPUs [ 29], [30] or TPUs [ 46]–[49]. The prohibitive cost\n",
      "and expertise required to train and serve 100s of billions of\n",
      "parameters put state-of-the-art AI capabilities out of reach\n",
      "for many academic researchers and smaller organizations,\n",
      "especially when on-premise deployments are needed. For\n",
      "instance, compute costs to train OpenAI’s GPT-4 is estimated\n",
      "to be $78 million USD, and Google’s Gemini Ultra to be $191\n",
      "million USD [ 57]. Building and deploying large monolithic\n",
      "models may not be sustainable for hyperscalers [ 14] or any\n",
      "organization needing capable AI models continuously trained\n",
      "and updated on their data [ 1], [10], [16]. Finally, systems\n",
      "that cater to monolothic models have scaled compute TFLOPs\n",
      "much faster than memory bandwidth and capacity, creating the\n",
      "memory wall [ 35] where the memory system can no longer\n",
      "\n",
      "{'filename': '/Users/petrojm/Documents/projects/workshops/ai_engineer_2024/forked/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/2405.07518v1_1page.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Ask questions about your data\n",
    "user_question = \"What is a monolithic model?\"\n",
    "\n",
    "response = conversation.invoke({\"question\":user_question})\n",
    "print(response['question'])\n",
    "print(response['answer'])\n",
    "#print(len(response['source_documents']))\n",
    "\n",
    "for i in range(0,len(response['source_documents'])):\n",
    "    print('\\nSource #%d:'%(i+1))\n",
    "    print(response['source_documents'][i].page_content)\n",
    "    print(response['source_documents'][i].metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complex_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
