{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kit_dir: /Users/varunk/repo/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag\n",
      "repo_dir: /Users/varunk/repo/ai-starter-kit\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle # remove\n",
    "import shutil\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\")) # absolute path for ekr_rag directory\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"../../../\")) # absolute path for starter-kit directory\n",
    "print('kit_dir: %s'%kit_dir)\n",
    "print('repo_dir: %s'%repo_dir)\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from src.document_retrieval import DocumentRetrieval\n",
    "\n",
    "CONFIG_PATH = os.path.join(kit_dir,'config.yaml')\n",
    "PERSIST_DIRECTORY = os.path.join(kit_dir,f\"data/my-vector-db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader: pypdf2\n",
      "Nb of chunks: 12\n"
     ]
    }
   ],
   "source": [
    "# Specify PDF file\n",
    "pdf_file = kit_dir + '/data/tmp/SN40LPaper2Pages.pdf'\n",
    "\n",
    "# Initialize DocumentRetrieval class\n",
    "documentRetrieval =  DocumentRetrieval()\n",
    "print('Loader: %s'%documentRetrieval.loaders['pdf'])\n",
    "\n",
    "# Get pdf text\n",
    "raw_text = []\n",
    "meta_data = []\n",
    "if documentRetrieval.loaders['pdf'] == \"unstructured\":\n",
    "    loader = UnstructuredPDFLoader(pdf_file)\n",
    "    docs_unstructured = loader.load()\n",
    "    for doc in docs_unstructured:\n",
    "        raw_text.append(doc.page_content)\n",
    "        meta_data.append({\"filename\": pdf_file})\n",
    "elif documentRetrieval.loaders['pdf'] == \"pypdf2\":\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    for page in pdf_reader.pages:\n",
    "        raw_text.append(page.extract_text())\n",
    "        meta_data.append({\"filename\": pdf_file})#, \"page\": page_number})\n",
    "else:\n",
    "    raise ValueError(f\"{self.documentRetrieval.loaders['pdf']} is not a valid pdf loader\")\n",
    "\n",
    "# Get the text chunks\n",
    "text_chunks = documentRetrieval.get_text_chunks_with_metadata(docs=raw_text, meta_data=meta_data) # lst of langchain_core.documents.base.Document\n",
    "#print(text_chunks[0].page_content)\n",
    "print('Nb of chunks: %d'%len(text_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunk/anaconda3/envs/ai_engineer_2024/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "2024-06-24 14:17:46,714 [INFO] - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:17:48,794 [INFO] - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      "The directory Chroma has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:17:52,183 [INFO] - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-06-24 14:18:11,095 [INFO] - Vector store saved to /Users/varunk/repo/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/my-vector-db\n"
     ]
    }
   ],
   "source": [
    "# Create vector store\n",
    "embeddings = documentRetrieval.load_embedding_model()\n",
    "if os.path.exists(PERSIST_DIRECTORY):\n",
    "    shutil.rmtree(PERSIST_DIRECTORY)\n",
    "    print(f\"The directory Chroma has been deleted.\")\n",
    "#vectorstore = documentRetrieval.create_vector_store(text_chunks, embeddings, output_db=None)\n",
    "vectorstore = documentRetrieval.create_vector_store(text_chunks, embeddings, output_db=PERSIST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation chain\n",
    "documentRetrieval.init_retriever(vectorstore)\n",
    "conversation = documentRetrieval.get_qa_retrieval_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a monolithic model?\n",
      "A monolithic model is a large language model that has billions or trillions of parameters and is trained with curated datasets that consist of trillions of tokens scraped from the web.\n",
      "\n",
      "Source #1:\n",
      "Source: /Users/varunk/repo/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/SN40LPaper2Pages, Text: \n",
      "from the web. However, training and serving a state-of-\n",
      "the-art monolithic LLM is both an extraordinarily expensive\n",
      "affair and a complex systems engineering challenge. Training\n",
      "requires building and operating a supercomputer composed ofthousands of hosts, purpose-built networks, power and cooling\n",
      "infrastructure, and thousands of accelerators – typically\n",
      "GPUs [ 29], [30] or TPUs [ 46]–[49]. The prohibitive cost\n",
      "and expertise required to train and serve 100s of billions of\n",
      "parameters put state-of-the-art AI capabilities out of reach\n",
      "for many academic researchers and smaller organizations,\n",
      "especially when on-premise deployments are needed. For\n",
      "instance, compute costs to train OpenAI’s GPT-4 is estimated\n",
      "to be $78 million USD, and Google’s Gemini Ultra to be $191\n",
      "million USD [ 57]. Building and deploying large monolithic\n",
      "models may not be sustainable for hyperscalers [ 14] or any\n",
      "organization needing capable AI models continuously trained\n",
      "and updated on their data [ 1], [10], [16]. Finally, systems\n",
      "that cater to monolothic models have scaled compute TFLOPs\n",
      "much faster than memory bandwidth and capacity, creating the\n",
      "memory wall [ 35] where the memory system can no longer\n",
      "\n",
      "{'filename': '/Users/varunk/repo/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/SN40LPaper2Pages.pdf'}\n",
      "\n",
      "Source #2:\n",
      "Source: /Users/varunk/repo/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/SN40LPaper2Pages, Text: \n",
      "by specializing base models to a domain, by ﬁne-tuning base\n",
      "models to a speciﬁc task or group of tasks [ 66], [69], and by\n",
      "distilling or compressing larger models into smaller models.\n",
      "Furthermore, compositions of such smaller models have been\n",
      "shown to demonstrate emergent behavior that matches large\n",
      "monolithic models [ 38], [45], [51], [55], [56]. They bring AI\n",
      "within reach to a broader community.\n",
      "We believe that successful AI systems of the future will host\n",
      "and execute many small models efﬁciently. This is reﬂected\n",
      "both in directions pursued successfully in academia [ 5], [38],\n",
      "[45], [51], [55], and new products that are being adopted in\n",
      "1arXiv:2405.07518v1  [cs.AR]  13 May 2024\n",
      "\n",
      "{'filename': '/Users/varunk/repo/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/SN40LPaper2Pages.pdf'}\n",
      "\n",
      "Source #3:\n",
      "Source: /Users/varunk/repo/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/SN40LPaper2Pages, Text: \n",
      "designed for enterprise inference and training applications. The\n",
      "chip introduces a new three-tier memory system with on-chip\n",
      "distributed SRAM, on-package HBM, and off-package DDR\n",
      "DRAM. A dedicated inter-RDU network enables scaling up and\n",
      "out over multiple sockets. We demonstrate speedups ranging from\n",
      "2⇥to13⇥on various benchmarks running on eight RDU sockets\n",
      "compared with an unfused baseline. We show that for CoE\n",
      "inference deployments, the 8-socket RDU Node reduces machine\n",
      "footprint by up to 19⇥, speeds up model switching time by 15⇥\n",
      "to31⇥, and achieves an overall speedup of 3.7⇥over a DGX\n",
      "H100 and 6.6⇥over a DGX A100.\n",
      "I. I NTRODUCTION\n",
      "Recent advances in the training and inference of large\n",
      "language models (LLMs) has taken the world by storm. State-\n",
      "of-the-art generative AI/ML applications like ChatGPT [ 7]\n",
      "and Gemini [ 3] are built on top of monolithic LLMs that\n",
      "can have billions or trillions of parameters. They are trained\n",
      "with curated datasets that consist of trillions of tokens scraped\n",
      "from the web. However, training and serving a state-of-\n",
      "the-art monolithic LLM is both an extraordinarily expensive\n",
      "affair and a complex systems engineering challenge. Training\n",
      "\n",
      "{'filename': '/Users/varunk/repo/ai-starter-kit/workshops/ai_engineer_2024/ekr_rag/data/tmp/SN40LPaper2Pages.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Ask questions about your data\n",
    "user_question = \"What is a monolithic model?\"\n",
    "\n",
    "response = conversation.invoke({\"question\":user_question})\n",
    "print(response['question'])\n",
    "print(response['answer'])\n",
    "#print(len(response['source_documents']))\n",
    "\n",
    "for i in range(0,len(response['source_documents'])):\n",
    "    print('\\nSource #%d:'%(i+1))\n",
    "    print(response['source_documents'][i].page_content)\n",
    "    print(response['source_documents'][i].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_engineer_2024",
   "language": "python",
   "name": "ai_engineer_2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
