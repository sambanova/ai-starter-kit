{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kit_dir: /Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever\n",
      "repo_dir: /Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/ekr_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\")) # absolute path for ekr_rag directory\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\")) # absolute path for starter-kit directory\n",
    "print('kit_dir: %s'%kit_dir)\n",
    "print('repo_dir: %s'%repo_dir)\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from src.document_retrieval import DocumentRetrieval\n",
    "from utils.parsing.sambaparse import SambaParse, parse_doc_universal\n",
    "\n",
    "CONFIG_PATH = os.path.join(kit_dir,'config.yaml')\n",
    "PERSIST_DIRECTORY = os.path.join(kit_dir,f\"data/my-vector-db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 16:53:32,732 [INFO] - Deleting contents of output directory: ./output\n",
      "2024-07-14 16:53:32,760 [INFO] - Running command: unstructured-ingest local --output-dir ./output --num-processes 2 --strategy auto --ocr-languages eng --encoding utf-8 --fields-include element_id,text,type,metadata,embeddings --metadata-exclude  --metadata-include  --pdf-infer-table-structure --input-path \"/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test\" --recursive --verbose --chunking-strategy by_title --chunk-max-characters 1500 --chunk-overlap 300 --chunk-combine-text-under-n-chars 1500 --chunk-elements\n",
      "2024-07-14 16:53:32,763 [INFO] - This may take some time depending on the size of your data. Please be patient...\n",
      "/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/ekr_venv/lib/python3.10/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type additional_partition_args detected when decoding CliPartitionConfig.\n",
      "  warnings.warn(\n",
      "2024-07-14 16:54:00,474 MainProcess INFO     running pipeline: DocFactory -> Reader -> Partitioner -> Chunker -> Copier with config: {\"reprocess\": false, \"verbose\": true, \"work_dir\": \"/Users/petrojm/.cache/unstructured/ingest/pipeline\", \"output_dir\": \"./output\", \"num_processes\": 2, \"raise_on_error\": false}\n",
      "2024-07-14 16:54:02,368 MainProcess INFO     Running doc factory to generate ingest docs. Source connector: {\"processor_config\": {\"reprocess\": false, \"verbose\": true, \"work_dir\": \"/Users/petrojm/.cache/unstructured/ingest/pipeline\", \"output_dir\": \"./output\", \"num_processes\": 2, \"raise_on_error\": false}, \"read_config\": {\"download_dir\": null, \"re_download\": false, \"preserve_downloads\": false, \"download_only\": false, \"max_docs\": null}, \"connector_config\": {\"input_path\": \"/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test\", \"recursive\": true, \"file_glob\": null}}\n",
      "2024-07-14 16:54:02,372 MainProcess INFO     processing 1 docs via 2 processes\n",
      "2024-07-14 16:54:02,373 MainProcess INFO     Calling Reader with 1 docs\n",
      "2024-07-14 16:54:02,373 MainProcess INFO     Running source node to download data associated with ingest docs\n",
      "2024-07-14 16:54:04,313 SpawnPoolWorker-3 INFO     File exists: /Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test/SN40LPaper2Pages.pdf, skipping download\n",
      "2024-07-14 16:54:04,334 MainProcess INFO     Calling Partitioner with 1 docs\n",
      "2024-07-14 16:54:04,334 MainProcess INFO     Running partition node to extract content from json files. Config: {\"pdf_infer_table_structure\": true, \"strategy\": \"auto\", \"ocr_languages\": [\"eng\"], \"encoding\": \"utf-8\", \"additional_partition_args\": null, \"skip_infer_table_types\": null, \"fields_include\": [\"element_id\", \"text\", \"type\", \"metadata\", \"embeddings\"], \"flatten_metadata\": false, \"metadata_exclude\": [\"--metadata-include\"], \"metadata_include\": [], \"partition_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"partition_by_api\": false, \"api_key\": null, \"hi_res_model_name\": null}, partition kwargs: {}]\n",
      "2024-07-14 16:54:04,335 MainProcess INFO     Creating /Users/petrojm/.cache/unstructured/ingest/pipeline/partitioned\n",
      "2024-07-14 16:54:06,334 SpawnPoolWorker-5 INFO     File exists: /Users/petrojm/.cache/unstructured/ingest/pipeline/partitioned/9ce7459afa898c1650e4fac094faac36.json, skipping partition\n",
      "2024-07-14 16:54:06,347 MainProcess INFO     Calling Chunker with 1 docs\n",
      "2024-07-14 16:54:06,348 MainProcess INFO     Running chunking node. Chunking config: {\"chunk_elements\": false, \"chunking_strategy\": \"by_title\", \"combine_text_under_n_chars\": 1500, \"include_orig_elements\": true, \"max_characters\": 1500, \"multipage_sections\": true, \"new_after_n_chars\": null, \"overlap\": 300, \"overlap_all\": false}]\n",
      "2024-07-14 16:54:06,348 MainProcess INFO     Creating /Users/petrojm/.cache/unstructured/ingest/pipeline/chunked\n",
      "2024-07-14 16:54:08,338 SpawnPoolWorker-6 DEBUG    File exists: /Users/petrojm/.cache/unstructured/ingest/pipeline/chunked/c953d9634ee9c7e162200129ab85caae.json, skipping chunking\n",
      "2024-07-14 16:54:08,350 MainProcess INFO     Calling Copier with 1 docs\n",
      "2024-07-14 16:54:08,351 MainProcess INFO     Running copy node to move content to desired output location\n",
      "2024-07-14 16:54:10,346 SpawnPoolWorker-9 INFO     Copying /Users/petrojm/.cache/unstructured/ingest/pipeline/chunked/c953d9634ee9c7e162200129ab85caae.json -> output/SN40LPaper2Pages.pdf.json\n",
      "2024-07-14 16:54:10,721 [INFO] - Ingest process completed successfully!\n",
      "2024-07-14 16:54:10,722 [INFO] - Performing additional processing...\n",
      "2024-07-14 16:54:10,726 [INFO] - Additional processing completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of chunks: 10\n"
     ]
    }
   ],
   "source": [
    "# Specify PDF folder location\n",
    "pdf_folder = kit_dir+'/data/test'\n",
    "\n",
    "# Initialize DocumentRetrieval class\n",
    "documentRetrieval =  DocumentRetrieval()\n",
    "\n",
    "# Parse and chunk the documents\n",
    "additional_metadata = {}\n",
    "_, _, text_chunks = parse_doc_universal(doc=pdf_folder, additional_metadata=additional_metadata)\n",
    "print('Nb of chunks: %d'%len(text_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/ekr_venv/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "2024-07-14 16:54:31,931 [INFO] - Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 16:54:35,989 [INFO] - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      "The directory Chroma has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 16:54:38,822 [INFO] - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-07-14 16:54:51,207 [INFO] - Vector store saved to /Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/my-vector-db\n"
     ]
    }
   ],
   "source": [
    "# Create vector store\n",
    "embeddings = documentRetrieval.load_embedding_model()\n",
    "if os.path.exists(PERSIST_DIRECTORY):\n",
    "    shutil.rmtree(PERSIST_DIRECTORY)\n",
    "    print(f\"The directory Chroma has been deleted.\")\n",
    "#vectorstore = documentRetrieval.create_vector_store(text_chunks, embeddings, output_db=None)\n",
    "vectorstore = documentRetrieval.create_vector_store(text_chunks, embeddings, output_db=PERSIST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation chain\n",
    "documentRetrieval.init_retriever(vectorstore)\n",
    "conversation = documentRetrieval.get_qa_retrieval_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a composition of experts?\n",
      "A composition of experts (CoE) is a modular approach that lowers the cost and complexity of training and serving AI models. It involves combining many smaller expert models, each with several orders of magnitude fewer parameters, to match or exceed the capabilities of monolithic large language models (LLMs).\n",
      "\n",
      "Source #1:\n",
      "SambaNova Systems, Inc. ﬁrst.last@sambanova.ai\n",
      "\n",
      "like GPT-4 have paved the way for modern generative AI serving, and maintaining monolithic applications. Training, LLMs at remains prohibitively expensive and challenging. The disproportionate increase in compute- to-memory ratio of modern AI accelerators have created a memory wall, necessitating new methods to deploy AI. Recent research has shown that a composition of many smaller expert models, each with several orders of magnitude fewer parameters, can match or exceed the capabilities of monolithic LLMs. Composition of Experts (CoE) is a modular approach that lowers the cost and complexity of training and serving. However, this approach presents two key challenges when using conventional hardware: (1) without fused operations, smaller models have lower operational intensity, which makes high utilization more challenging to achieve; and (2) hosting a large number of models can be either prohibitively expensive or slow when dynamically switching between them.\n",
      "{'data_source': '{\"url\": \"/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test/SN40LPaper2Pages.pdf\", \"date_created\": \"2024-07-03 13:39:13.690790\", \"date_modified\": \"2024-07-02 12:06:50.680944\", \"date_processed\": \"2024-07-03T20:40:11.536436\", \"permissions_data\": [{\"mode\": 33188}]}', 'element_id': '4d887e7c417d78d95ec0807f2c25cfc1', 'file_directory': '/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test', 'filename': 'SN40LPaper2Pages.pdf', 'filetype': 'application/pdf', 'languages': 'eng', 'last_modified': '2024-07-02T12:06:50', 'orig_elements': 'eJztVU2P2zYQ/SsDnbKALevTttxLgzRog6bBors9bQODksYWuxIpkPR6nSD/vY+U96NBgvbSnHLYDw5n3rx5M0PdfIy454GV28o22lDEVVUt03yVimKdsuC8bpfNrs2WbVEu67SMZhQN7EQrnID/x6jR2rRSCcc2nHtx0ge37VjuOwdLliUJYs7mo2xdB2u6CtZRS+V83M3NslzFqxmVaR6X72f0eF6tp3Oa5MmXDSEClsierOPBV3Ep77m/GkXD0SdceLJbqw8GZ88RZ942hvEnFJ0lWTFPVvMkpzTf5NUGkMsqWVVJNEXzdtCt3MnP3DNKs02y3JRJvFwnVVE8uo9GN2zt5/DXWbIpkk2axmW+LPKl9x/ZDNJaqZXdnlW9+Rghn+ea5+l6/ckXdzC9x1r8YdnYxcjO6L+GxU+6OfjuwYIzN/jn5dWvC741W8M70ThtpNovhJxbJ4xjM7+VboEANqORlre3Sh97bvcMf2ck37FZeBYL9NMtrt4VydtLAYrZpdizjcd2FxTdyZ63rTTsM5y+ObHoTEGJwasUfZHn2cedxuAjxrGXjXAQenG+7oXaH7y/1zxitY/eB6t1X+n39UO/Q+MQuVWHoWbjJzpYzNMmVW3FddXka1GlVYk2lvUqrcWqbFfLNbfLoKPjexcKEEMt3uk7QVdhiO2M3qgmpj8PuzpJjXWxZ/Wj9W4KbrGQnsJDbdfS9WHUP1/nIl1nnHAjMlFmZdaKbJevVyk3TVOk+VTH/7LOaVbEkGRZFHEVtnU6p2mBpfWGdV7GxRcMU8T3df6m69yyQzJw3jaYM+vrrUEviauirFbf9/2/7Pu/fjmf73svb5l+vryeF9SJO6YRv1pyHdNRnGinDfmJMYr2rNigCPi8fEOQ/A4yzkiolgaBbcMPDPBWupeukw09q9vGdG0mjxm9ffubJeHIsA+0hD51spYeuj8R34+srE/joZtO9FB7j0BAgFUrLfxHbTwsNoKk8gtn/T/U6GE8OJ6T0/OBBwwIecqa9O6hDHAXTYO3CRfa2Kno88qSoHPUEVlnpNjvmnSAQGWKj7h2nW4t8Knlsdcn4MX0O/yUr8eyME0HTEu200cFHVGnCLw0gEA5UBHqRHbwlZlQr3GBXo/HlgUAjhAQEmPyRE94BjHfU9xeSXdomXZ8RCi6jjHE4CKuEQr3DsHaYzZ87mIjRlFLdETyhPHUH9+HmF79k9vrQMfSi1f69QVJ6zXR7aEXxvfTaE8vVNXro6cVcmjrpm4Bq+d76U4eyp07Hq7OAxPTL4hDYTNEevQHzNGrh60ld9R0y6fHzoP2sWNFB+uhGq3u4OZ730Nn0x4x+Rt6kV4E0fApoN0BzyPpMUwrRm/2KPWk8dTyQP/JC2iYYT947jRDQglKg7hF8g7fGzo4SPgheALF8PO59MOACvxD8UMo9UV2QR0kCaUTlNszTdv6MIcg4ftVMzFI+05+ZQXQSwumkwLtCa8OFqqHi0W1yIkMNbsjsx81HuLnn+J3wkz7eu1X/dP7vwEmJ8RW', 'page': '1', 'page_number': '1', 'type': 'CompositeElement'}\n",
      "\n",
      "Source #2:\n",
      "II. BACKGROUND: COMPOSITION OF EXPERTS\n",
      "\n",
      "We ﬁrst describe the Samba-CoE, a trillion parameter CoE system with 150 7B expert models, and how running it efﬁciently requires hardware support for aggressive operator the fusion and a novel memory system. We present SambaNova SN40L Reconﬁgurable Dataﬂow Unit (RDU), a commercial dataﬂow accelerator that combines streaming dataﬂow parallelism with a novel three-tier memory system\n",
      "\n",
      "In this section, we describe one instance of a CoE built and deployed on the SN40L, called Samba-CoE. Figure 2 shows the Samba-CoE pipeline from prompt to response.\n",
      "\n",
      "Samba-CoE consists of several expert models and a router model. Each expert is ﬁne-tuned in a speciﬁc domain. We leveraged several excellent expert models ﬁne-tuned on\n",
      "\n",
      "2\n",
      "{'data_source': '{\"url\": \"/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test/SN40LPaper2Pages.pdf\", \"date_created\": \"2024-07-03 13:39:13.690790\", \"date_modified\": \"2024-07-02 12:06:50.680944\", \"date_processed\": \"2024-07-03T20:40:11.536436\", \"permissions_data\": [{\"mode\": 33188}]}', 'element_id': '9b38c3884ae06cae824269c8abe9ca36', 'file_directory': '/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test', 'filename': 'SN40LPaper2Pages.pdf', 'filetype': 'application/pdf', 'languages': 'eng', 'last_modified': '2024-07-02T12:06:50', 'orig_elements': 'eJztmNtu4zYQhl+F0FULJDKps3y3m2SLYNskyAEtkC4MihrZ7EqkSlJxgmDfvUNZaQ51UQTYFLnwVaJfM+TMkPN57Ov7AFroQLmFrIM5CSomKgFFxhrKmoqxMuIihypKOKtoKmiwR4IOHK+542h/HwitTS0Vd2DH55bf6cEtViCXK4dKFFGKPpO8lrVbocryUe21VM77XV+XcRrGe/gio2H0ZY88CkUeFl5gSZmGdLsyOqEU2DvroPOZnMlbaC96LiD4hi98wAurB4PPPk58hoUwgH/GxCMaJfs036cxYfE8LucsDrOS5uWY8mjd6Vo28oV5RFg0p9k8pWFW0DJJ/jbvjRZg7cvlLyM6T+icsTCNsyTOvH0PppPWSq3sYqrs9X2A+/lY45gVxTef3GBav9bsyoKxsx6c0X90s0MtBn+CqOAzCPznw8XnGXw1CwMNF04bqZYzLvet48aB2f8q3QwdwPRGWlh8VXrdQr0EtHdGwg2YmY9ihmfqZhcnCf35jGOI0Rlfgg37utlUFBxuhjEvRMut9flWGB4Nc5ZShgaNbGFRSwM+hLv/PfJgCkHxzpcx2JrIZOPu+tGG930rBfdZzabXLVfLwdv7QwlALYMvo2rdv1yIy4cLMZ4sei7U0FVgfDP4ujm49Y0RHB+H5OOHg88/nZ9enRzOycHpL2enF8eXx6cn5PQTOfrt7Oj88sIv8hDdpXTteJtfdm2dF6yq0ioWTcVBUEaLMkuwiYuyKcsmf7OuZdikzDdgTkM6tuQklGUZZl4o4jyMtgkbl13Pvo+eLZOyjHc9u61nR8W84iPyaZP/CuT3oakoM9aRGqwwsgLiVkAueFfx/QN9tEc4wUK0LSZAcCdMHYtF8A3ZdAZZS7ciCFWSfyRwi8VwxN+z1qKrqslKr4kZlMK6E+kINJsdhcSY2zti4M8BD9SSFTf1GjMhduh7jYs02hC+XOI7K2+AaFyZ4/mN4TWDv9rj+pwofQMt6aDDOzEFFRJMrUdX3GSTy4m+4WQ8MHIOQqtNFMvB8KoFcohHPCoRRnulMM4fzg+vfvTJC911YITkLamfWnEhEHMPIXHnDSupMBPrEAKdz/eZgy9e20Ir7VSzh8jdygDsO4llfZbEU7iecINbYR0u/dFtg2yRc8YhYpxyiqiNkLgAeRTXUEElsjeDbJGlYYrELGg8IfRBKIuJumnORuxuUUanHWbfCWZj/CDcYfa7Y/ZYYY9LBMOm7HtkDY+01QqIVFgnJfChQSp4tlaDbN3Itxr6Vt9BjYYbMvtq7BHhWVI/cjoknyTSDEhELCLXPqc46WWP5MGtGqM7JKPuekecRvzaHi8vhK+CDSR1WScpA0ROXWVpw1NI4jQp8kRgHZo3hE0WZn5eQ24UG9hMQpmH5QYtWTJ+69qijE472LwX2ERRsYPNd4fNY8/jlGOlddZTxfqscYR5NqBN85PBnvOzh9dCcsTF6sEMmbWZkxTOJ4NC3kgcuojtQchpjCO17rhU48DVjpss0exxOxySWj+EPd/4xapavW7WSfI4hZhHWQosAwAaN2mZs6Iqq7Sm8dvhJ2EhfkBGtGATfiaBsel3oSLN/e9C/xQ2Ljv4vA/4pEXOoh18/utHoOhpW37SGrMKvn35C5CI3to=', 'page': '2', 'page_number': '2', 'type': 'CompositeElement'}\n",
      "\n",
      "Source #3:\n",
      "industry [4], [6], [24]. CoE-like compound AI systems play a pivotal role in advancing the AI frontier [60], [72]. In this paper, we refer to such modular systems with compositions of specialized smaller models as Composition of Experts (CoE). A CoE consists of several small expert models working in tandem on a task. Outputs from one expert determine which expert(s) to execute next. Running an expert involves loading model parameter weights to the accelerator’s main memory, and then executing the model. Consequently, executing a CoE involves a sequence of model switching and model execution. Current state-of-the-art AI accelerators do not handle this sequence of operations efﬁciently, as shown in Figure 1.\n",
      "\n",
      "We quantify and discuss the impact of streaming dataﬂow showing real world benchmarks, parallelism on several speedups ranging from 2 over an optimized baseline. to 13 We deploy Samba-CoE on a single SN40L Node that contains eight SN40L RDU sockets and a host. We discuss the performance of Samba-CoE on the SN40L Node compared to DGX A100 and DGX H100. We show that for CoE inference deployments, the SN40L reduces machine footprint by up to , and 19 ⇥ over DGX H100 achieves an overall speedup of 3.7 and DGX A100, respectively.\n",
      "\n",
      "Efﬁciently accelerating a CoE boils down to executing expert models efﬁciently while minimizing model switching costs. We break this down into three key requirements:\n",
      "{'data_source': '{\"url\": \"/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test/SN40LPaper2Pages.pdf\", \"date_created\": \"2024-07-03 13:39:13.690790\", \"date_modified\": \"2024-07-02 12:06:50.680944\", \"date_processed\": \"2024-07-03T20:40:11.536436\", \"permissions_data\": [{\"mode\": 33188}]}', 'element_id': 'f13265076c06e53b822f14472d463b80', 'file_directory': '/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test', 'filename': 'SN40LPaper2Pages.pdf', 'filetype': 'application/pdf', 'languages': 'eng', 'last_modified': '2024-07-02T12:06:50', 'orig_elements': 'eJztVt1v2zYQ/1cIPbWALevDli2/BWu3FRu6ommxAWlgUOTJ5iyRKknF8Yr+77ujZCcNshV7aLGHvCTW8e7H++Lv7upTBA20oP1GyWjNoqLMCslhnhTlPC8BirLKlgmsKr7Iq0KsogmLWvBccs9R/1MkjLFSae7Bhe+GH03vNztQ251HSZYlCdqM4oOSfofSdBmknVHak93VVZoXMYqWZRHn1xN2+k7TVTkIVnkezx8RDBYoiNzReWgpijfqFprLjguIPuMBObtxprf4TT7iN2yEBfwXgs6SbD5NltMkZ2m+zst1msdFmSzLJBqsYdMaqWr1QD1jabZOivUiiYtVUs7nZ/XOGgHOPYR/lyXrebJO03iRF/O8IP0ObKucU0a7zZjVq08R3ke+5nm6Wn2m4HrbENbsvQPrZh14a/5sZy+M6Kl6KMFvEPjj4vKXGeztxkLNhTdW6e2Mq6nz3Hqw073yMzQA21nlYLPX5tCA3ALqe6vgBuyMvJhhPf3s8vU8+fUNRxezN3wLLu5kHTJaqwY2UlmgG47f3bFodEHzlrIUPernqOOPXdDhXdcowT0mejYeN1xve9KnnEegt9F1kDr/D/V+d6p3KBxabnTfVmCpzyktHm6p5yOlZe+8PbKrOXVqQX+y+XXMfjAvp43aAxOm7UyvJbt4xYa+dazDR8I469SN8bxh1jTAlGZc3nAtMFvM74D0a2u0V2AROCHkZYbIrzQeKwShLEzYARjmGXW8Ya4XO4YB9Q2358sOyu8GL5yinDhmauY6EIo36i+QzLW8aRCAOrFxjDt0/qxNyi9v8Sbv2DMM6nnMLig4RNROOT+gUdEwkIDEIKif4A7G7ikkjM9zLaFlCMrxt9vH7Lfedz1iYKAkh5OtBE+PBQWHncKYBvEz95yihFsQvQemsQYxe9trTfhcn4yVvjHNDTjWGC7pKHiC+bLYRIiLKSPKcoRFieZCIDVajp36oc+StHSs5ehuCy22/ASRJenp8eJTfQIqFVo7+NhjQzeoe6fCQ5bOvnA2aAmghA0eOSyN2A3Oy1E2AhiNyL21CMvw3XiYmnqKt07xCVFn3PPZMWmYNp7tEAUbKTTH/ctMR4qh8lB/6OsqSYUa/cViu505aCrPj2rbW2BpTD1/ekyvuSXjG3hHHY+t/3COQF6naSqKVNTIc2md10shk1qKfC7zrFp8szmyKoq4wDGRJMs4C2NiFOTFMl6FybIoVnH2qGQwepol33WW0KsW1IgbgdTrKN4K3UvicpEvi6dh87Vh8zuwjz3HiVAfA2NI5UTvXGAj1WLj+kDGHnu0JVahwMJ7z8whPHMS4mFDpNxIViFD7Fpu924S2BGnQKNcIOgzo3cAsu8csxgUmQemzpjBY6Jc03nVhilScYfWGmKi1TRn6KyErjFHdsnbik+JDQPzO4RBmgrJZa+xY9F/7mmeeGRd5CjigfH47Yv3zBmxB2RripiznXFI+gR+L3isUG1sy0fC++JCOr93Fw1CbtFf9PLFT3+wC6SCAE0fP+NHAKdkDW4h7kjkOGMDow5RhS6c3ENHzB5fL44OonRAQ+Ox55C/qyPrO7pvGCVpyXDMpLAYkni6l5EdhFGhwwmN0jH9FFQeL89+ktMTvJGmOHFzc/xvnM2lXJRVmi3mwHnK81WJU09mXBSyrkSSfOvdP03LeVzcX/4z3PWTYdcv48VjgsHkibH/J4yNHi+eGPtrjP3yy33rbm272xAroxpa4XAHO++2dPrlFv1gcaO1GDkUWZ7Y927HvdsoBfKkC1xW4QPYD1uhHDa9sPdaALaHI7LIxx7rF2q0/hcWuf4br99S2g==', 'page': '2', 'page_number': '2', 'type': 'CompositeElement'}\n",
      "\n",
      "Source #4:\n",
      "4 2 0 2\n",
      "\n",
      "y a M 3 1 ] R A . s c [\n",
      "\n",
      "1 v 8 1 5 7 0 . 5 0 4 2 : v i X r a\n",
      "\n",
      "SambaNova SN40L: Scaling the AI Memory Wall with Dataﬂow and Composition of Experts\n",
      "\n",
      "Raghu Prabhakar, Ram Sivaramakrishnan, Darshan Gandhi, Yun Du, Mingran Wang, Xiangyu Song, Kejie Zhang, Tianren Gao, Angela Wang, Karen Li, Yongning Sheng, Joshua Brot, Denis Sokolov, Apurv Vivek, Calvin Leung, Arjun Sabnis, Jiayu Bai, Tuowen Zhao, Mark Gottscho, David Jackson, Mark Luttrell, Manish K. Shah, Edison Chen, Kaizhao Liang, Swayambhoo Jain, Urmish Thakker, Dawei Huang, Sumti Jairath, Kevin J. Brown, Kunle Olukotun\n",
      "{'data_source': '{\"url\": \"/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test/SN40LPaper2Pages.pdf\", \"date_created\": \"2024-07-03 13:39:13.690790\", \"date_modified\": \"2024-07-02 12:06:50.680944\", \"date_processed\": \"2024-07-03T20:40:11.536436\", \"permissions_data\": [{\"mode\": 33188}]}', 'element_id': '9ea41eb00daf1282ed1316ea9ba1910b', 'file_directory': '/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test', 'filename': 'SN40LPaper2Pages.pdf', 'filetype': 'application/pdf', 'languages': 'eng', 'last_modified': '2024-07-02T12:06:50', 'orig_elements': 'eJztVtty20YM/RUMn2Vql3fqzblM0jhOPZHSpHU9GoiExI1Irma5lKJk8u/BSnKrcdxJM1OnedATBSwuB9g9gK4/eVRTQ62dqtIbgSfSogzFPJlneRpHGBSBTCOZZ5nAXMYy8gbgNWSxRIts/8krtDalatFSt5Nr3OreTitSi8qyJgiEYJ+DeqNKW7FWpjvtSqvWOr/r6yj1gwHESeaLmwEcxCTNfelEKQI/+1rem7PC67adpcZVcKU+UD1eYUHeZz5wQKed7g3LDh/LNC0M8WdXcCCC6EykZyIEGY7CfCRDP8lFmgtv703TRpdqru6YByCDkUhGsfCTTORR9Jf5yuiCuu5u+EkgRpEYSenHYRKFibNfkWlU1ynddtNDR68/eZzPYQ1DmWWfXXG9qV2s4ZuOTDdckTX6fTN8oove3RxrWKaCf5yPL4a0NFNDcyysNqpdDFGddRaNJXO2VHbIDmRWRnU0XbZ6U1O5ILa3RtGazNChGPJd2uH4VSReXiFDDK5wQZ2/Kue7js5VTdNSGXIZtj8cmHeA0GLjuuTdi/NgY7ernQ2uVrUq0HKjh4fjGttF7+xdzz1qF97NTtvZf7jvye197y6OPadt38zIuNfs2mLpg3vvXgQBCAic1W36Ny3npgWX/ZHKibNjh7vMy8ucZnkRZky0POarj2epnGEal2mSUZk8MPOSJPCjv5kno/DAxAPVvlLsHU7c+6HcK8lyMsY8Lfipdq7eGcMTfpRIkZ7I+S1ybgHhEvi5wQ28hnPwoYOCUxyR9TlhyY73MDSJAsriQqZUijJjpoqISilnMhbzLJHZAzNU8kv3gyOKxnHsJ8cU/Uqx9zhx9LQf/18K7jTmO3bdMWclrCFjxsaQ8mL1+SvALdkR6xW8AwP43cs2TPMiisMiLamMwkDETOO4iGXI/3X5rQYPRmXJWzNx21Tut+1BDpio++Uah6kf3aPYe5yo/HOs2yRNo9O6/e+5PsZmhq/0GmFXygjGBdbcILAVwfkvcEkN9xneYl3DRtkKnnAr/uznMxHoDWBbwmPdrHSnXHmg5/D0AzeDuXc0ICbK1nTfUKBoHs8CTIo8LXI5Z5ihDEVUyHCeEsrZww2FVPjxAHj47Pf7QY7SWzkOeBrco9h7nIbCzzEU8jCJ49NQ+DdD4Zv793govMZF1cOVwVmFSzQDeI0NjNUaDTa45IZULbYDHgWmq7CFZzwGKjWA3/sWnvQDuOReGta/5YIG8E7xZ9vDWDvpgt4rgj+q3dGEjxgjB9ADOG8XVOPB6cJhh5cuKLu1biCNK3InL3RX9QiPjLaMgFrVceSlrvWaQ6x6s4bf1JqWA3iM9VpxDOqd27l5z+jGOGMHDqKQET1Cjj/p9YZTMSLGcIlmCc+0tV1RaVfgWpXwAotlp9vD6cveWkN17USOVcGFz9CwGsDTUrEZPGacrgD1kUNyCbt6xhvc8qSttOZwis/fOB5WMOEGL8m4VBtS8LzfW/eNVc7QoK1cz1whL3xX9MbF7tua4Ne6X2rbt8eT9hUaduH693/Dbr4A5VZluA==', 'page': '1', 'page_number': '1', 'type': 'CompositeElement'}\n",
      "\n",
      "Source #5:\n",
      "responded with ecosystems of much smaller, modular models that are just as capable, but are cheaper and easier to train and serve [5], [37], [43], [58]. Smaller models like the 7B- parameter Llama 3 [13], Llama 2 [68], and Mistral 7B [44] are often adequate. They might not match the performance of larger models over a general suite of tasks, but smaller models can deliver superior accuracy on a narrower set of specialized tasks for which they have been ﬁne-tuned [8], [65]. For example, Flan-T5-XL only has 3B parameters, but it surpasses the 175B-parameter GPT-3’s MMLU score by nearly 10% [31]. Proof points like these have bolstered community activity in building and training smaller models by specializing base models to a domain, by ﬁne-tuning base models to a speciﬁc task or group of tasks [66], [69], and by distilling or compressing larger models into smaller models. Furthermore, compositions of such smaller models have been shown to demonstrate emergent behavior that matches large monolithic models [38], [45], [51], [55], [56]. They bring AI within reach to a broader community.\n",
      "\n",
      "We believe that successful AI systems of the future will host and execute many small models efﬁciently. This is reﬂected both in directions pursued successfully in academia [5], [38], [45], [51], [55], and new products that are being adopted in\n",
      "\n",
      "1\n",
      "{'data_source': '{\"url\": \"/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test/SN40LPaper2Pages.pdf\", \"date_created\": \"2024-07-03 13:39:13.690790\", \"date_modified\": \"2024-07-02 12:06:50.680944\", \"date_processed\": \"2024-07-03T20:40:11.536436\", \"permissions_data\": [{\"mode\": 33188}]}', 'element_id': '165b5d5a3291da7f65692a83be760fca', 'file_directory': '/Users/petrojm/Documents/projects/ASK/ekr_refactoring/ai-starter-kit/enterprise_knowledge_retriever/data/test', 'filename': 'SN40LPaper2Pages.pdf', 'filetype': 'application/pdf', 'languages': 'eng', 'last_modified': '2024-07-02T12:06:50', 'orig_elements': 'eJztVktv20YQ/isLAr1JFN8idUtQpCjqBAbsoAUUQ1guh9LGJJfdh2U1yH/vzFKyHEM9FGiKHHyRuLPzfnw76y8BdNDDYDeyCVYsKPOyKETM2zzjeZM2ZVuWUSpynnBRtHUezFjQg+UNtxz5vwRCKd3IgVsw/tzxg3J2swO53VmkJEkUocyRvJeN3SE1XnrqqORgSW69RrNhMWNxEhVhdjdjT4SyyMOYCHGOlPwiZRJCUmAOxkJPkVzLR+huRi4g+IoX5PDGKKfxTH7iGTZCA/75wJMoyebRch6lLE5XabWK07CoomUVBZM0bHrVyFa+YE/Q+ioqVnkUFmVUZdkT+6iVAGNeqr9NolUWreI4zNMiSwviH0H30hipBrM5Znb9JUB75GuaxmX5lYJzuiNdi48GtFmMYLX63C9+VsJRBZGCZxD48ebmtwXc642GlgurtBy2Cy7nxnJtQc/vpV2gAOhRSwOb+0HtO2i2gPxWS3gAvSAvFlhTu7j5kEVX1xxdTK75Fkw4Nu2UUbBoDH3eiI4bQ/HW6F4UVlmRVMjQyg42jdRALhz+d8+DowsD7ymNwcVAjjz2MHoePo6dFJyiWhyvOz5sHfFTUQIYtsGdpxr7Dw1xe2oIX1mU3Ayur0FT23uKPo8bVFVVxOky5lkZA4e0bgrRNknRZHlRx7lPtIVHmqRAgxnV0EDD9tLuGAg1dbthqmW9Eztmet51oGcMPXMd1/QPnWF2xy1Du+yzM/hhmOAjrzuYsdpNF2IHlBnGh4YBNxI/rWJWczl4GtbtAdg6p7FLl/SbpfSbl3chu5nMnqx18h7QJLDl2znDcDH/WDF21fGesxTHliSnU8LWRYknMvFeGrTXoRQqz+68W6q1gA408KfDkQrZ7Q4OrCdoYYOyrOcWoyZT6HurdM8HQUIMY9+eHVIPFBnbwgBkwDhpPZfl5t5MOTDfhiD4wPBDkqBxqFwq1CCE01wcmEKX2MC1Vnu6B0vKzAhC8k7+hfXxihk6xPY7OXl4YDuOGawB4/nk2jqKB5hbNyD3mjKwLnLM5DsUgUfej1Sbd9h689t8/scVWuxIgWHp23NGj65L9N7pEUcQjM9FvMzfzs95/+X6dp5+ckkUV4a9f3/1kRmhMLf1gQ3ANSqOo5+wqjHav9YKQ5lg+amOBo6+qw7bTaPHQvW9G6Q9YE6sfKAP7JPayQ6fgq2vpm8dOrzILFp9yhRd1xzVn9pUYV4b1aPkjBif5+kir9c0cQmfdIb522rlxqfyYmILn97q2Gaot8FOk11HKpEfgxlxtAwdv+0bzIJ64T+WyGnMie4xhTMvq4wkvPBTaJ5N4UnLue5mp/YDed5AjwKYIuxDfH3R5mCRBTmpz/y0+tbGgnqPUNWgOhx6KU5a16nvmsxPZO6fw3z6Lu6Oc1ITerI3v3q4wPrgY0e9SImrtcKh0udChgRWJxT8gK2NGPgAt4Q8CEEvt4SmqZZclE2bxkWdZxXwtm3jrEnatiiKKPnuW0K5xOf2+ZZQVcuwetoJ0ouUSeh1S/hBtoS0jLLXLeE/3xJ+J7DpKMoJSBCSqMFa1xEUPNsY6KlonXX4FOwRDdlO0W5A7/8jCIfIhK/pYQKzE+hAewRbic51B8IZiTBpEFr8RYLVweehVrieIOBMVfXYODptHF6dven8m8EF4lAv+Wm1uAxq5NUAe4YN0DhsgPNCU4N/cBo1kmE5/CsYK+qyqJNl2rZp1gqIqwYpUQZtm/O0zZffD8ayhFAricok9GvUiRDHUZh4Qr4Ml5cIk8griP0YIFbEUfEKYhdB7Dkmxc/H8p1SGFXw9e5vWuRaLw==', 'page': '1', 'page_number': '1', 'type': 'CompositeElement'}\n"
     ]
    }
   ],
   "source": [
    "# Ask questions about your data\n",
    "user_question = \"What is a composition of experts?\"\n",
    "\n",
    "response = conversation.invoke({\"question\":user_question})\n",
    "print(response['question'])\n",
    "print(response['answer'])\n",
    "\n",
    "for i in range(0,len(response['source_documents'])):\n",
    "    print('\\nSource #%d:'%(i+1))\n",
    "    print(response['source_documents'][i].page_content)\n",
    "    print(response['source_documents'][i].metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
