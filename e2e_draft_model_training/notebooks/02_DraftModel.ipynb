{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft Model Training: The Target Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our draft model is also available in HuggingFace: [meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct), as well as SambaStudio.\n",
    "\n",
    "The `Llama 3.2` collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The `Llama 3.2` instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n",
    "\n",
    "In this notebook we will detail the following points:\n",
    "1. Download the draft model from Hugging Face in your local directory.\n",
    "2. Configure the draft model checkpoint.\n",
    "3. Upload the draft model on SambaStudio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- [1. Setup](#1-setup)\n",
    "    - [1.1 Imports](#11-imports)\n",
    "    - [1.2 Instantiate the SambaStudio client for BYOC](#12-instantiate-the-sambastudio-client-for-byoc)\n",
    "- [2. Target model: `Llama-3.2-3B-Instruct`](#2-target-model-llama-3b-instruct)\n",
    "    - [2.1 Download the target model from HuggingFace](#21-download-the-target-model-from-huggingface)\n",
    "    - [2.2 Configure checkpoint](#22-configure-checkpoint)\n",
    "    - [2.3 Set and check chat template (optional)](#23-set-and-check-chat-template-optional)\n",
    "    - [2.4 Set padding token (required for training)](#24-set-padding-token-required-for-training)\n",
    "    - [2.5 Get model params and Sambastudio suitable Apps](#25-get-model-params-and-sambastudio-suitable-apps)\n",
    "- [3. Upload the checkpoint to SambaStudio](#3-upload-the-checkpoint-to-sambastudio)\n",
    "    - [3.1 Using the checkpoint dictionary](#31-using-the-checkpoint-dictionary)\n",
    "    - [3.2 Using the `draft_checkpoint_config.yaml`](#32-using-the-target_checkpoint_configyaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "\n",
    "# Get absolute paths for kit_dir and repo_dir\n",
    "current_dir = os.getcwd()\n",
    "kit_dir =  os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "\n",
    "# Adding directories to the Python module search path\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# Bring Your Own Checkpoint (BYOC)\n",
    "from utils.byoc.src.snsdk_byoc_wrapper import BYOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Instantiate the SambaStudio client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 21:12:17,376 [INFO] Using variables from .snapi config to set up Snsdk.\n"
     ]
    }
   ],
   "source": [
    "# Initialize BYOC (Bring Your Own Checkpoint)\n",
    "byoc = BYOC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target model: `Llama-3.2-3B-Instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Download the target model from HuggingFace\n",
    "You can use your own fine-tuned models or you can download and use [Huggingface model checkpoints](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).\n",
    "\n",
    "In our example we will use an available model in HuggingFace: [meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct), as well as SambaStudio.\n",
    "\n",
    "The `Llama 3.2` collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The `Llama 3.2` instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model name\n",
    "# xxx\n",
    "# hf_model = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "hf_model = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "# Our local target directory\n",
    "target_dir = os.path.join(kit_dir, 'data', 'models') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target dir if it does not exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Download checkpoint to your target directory\n",
    "repo_files = HfApi().list_repo_files(hf_model)\n",
    "for file_name in repo_files:\n",
    "    hf_hub_download(repo_id=hf_model, filename=file_name, cache_dir=target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint folder:  /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n"
     ]
    }
   ],
   "source": [
    "# Find the snapshot folder inside your target directory\n",
    "for root, dirs, files in os.walk(target_dir):\n",
    "    if \"snapshots\" in root and hf_model.replace(\"/\", \"--\") in root:\n",
    "        checkpoint_folder = os.path.join(root, dirs[0])\n",
    "        break\n",
    "\n",
    "print(\"Checkpoint folder: \", checkpoint_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Configure checkpoint\n",
    "\n",
    "Some parameters should be provided as a checkpoint dictionary, in order to upload a previously created checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the target dictionary\n",
    "# xxx\n",
    "# checkpoint = {\n",
    "#     'model_name':'Llama-3.2-3B-Instruct',\n",
    "#     'publisher': \"meta-llama\",\n",
    "#     'description': \"The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).\",\n",
    "#     'param_count': 3,  # number in billions of parameters\n",
    "#     'checkpoint_path': checkpoint_folder\n",
    "# }\n",
    "checkpoint = {\n",
    "    'model_name':'Llama-3.1-8B-Instruct',\n",
    "    'publisher': \"meta-llama\",\n",
    "    'description': \"The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).\",\n",
    "    'param_count': 8,  # number in billions of parameters\n",
    "    'checkpoint_path': checkpoint_folder\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set and check chat template (optional) \n",
    "\n",
    "If you want to use chat templates (roles structures), you need to include or update the existing chat template.\n",
    "\n",
    "This should be formatted as a Jinja2 String template as in the following `llama` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinjia chat template\n",
    "jinja_chat_template = \"\"\" \n",
    "{% for message in messages %}\n",
    "    {% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\\n' + message['content'] | trim + '<|eot_id|>'+'\\n' %}\n",
    "    {% if loop.index0 == 0 %}{% set content = bos_token + content %}\n",
    "    {% endif %}\n",
    "    {{content}}\n",
    "{% endfor %}\n",
    "{{'<|start_header_id|>assistant<|end_header_id|>'+'\\n'}}\n",
    "\"\"\"\n",
    "# Delete escape characters\n",
    "jinja_chat_template = re.sub(r\"(?<!')\\n(?!')\", \"\", jinja_chat_template).strip().replace('  ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and update your tokenizer config from the checkpoint path\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'tokenizer_config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['chat_template'] = jinja_chat_template\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 21:12:37,820 [INFO] Raw chat template for checkpoint in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659:\n",
      "{% for message in messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\n",
      "' + message['content'] | trim + '<|eot_id|>'+'\n",
      "' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{content}}{% endfor %}{{'<|start_header_id|>assistant<|end_header_id|>'+'\n",
      "'}}\n",
      "\n",
      "2025-03-31 21:12:37,829 [INFO] Rendered template with input test messages:\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "This is a system prompt.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "This is a user prompt.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "This is a response from the assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "This is an user follow up<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Render template when using a roles / chat structure\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"This is a system prompt.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a user prompt.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"This is a response from the assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is an user follow up\"}\n",
    "    ]\n",
    "byoc.check_chat_templates(test_messages, checkpoint_paths=checkpoint['checkpoint_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Set padding token (required for training)\n",
    "\n",
    "If `pad_token_id` is not set in your checkpoint configuration yet and you want to do a further fine-tuning over your checkpoint, you need to set `pad_token_id` in your checkpoint `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pad_token_id to checkpoint config\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['pad_token_id'] = None\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Get model params and Sambastudio suitable apps\n",
    "\n",
    "Extra parameters are required to upload your checkpoint, including model architecture, sequence length, and vocabulary size.\n",
    "\n",
    "These parameters can be extracted from your checkpoint configuration, and included in your checkpoint dictionary parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 21:12:41,574 [INFO] Params for checkpoint in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659:\n",
      "[{'model_arch': 'llama', 'seq_length': 131072, 'vocab_size': 128256}]\n"
     ]
    }
   ],
   "source": [
    "# Extract all the checkpoint parameters\n",
    "checkpoint_config_params = byoc.find_config_params(checkpoint_paths=checkpoint['checkpoint_path'])[0]\n",
    "# Update your checkpoint dictionary\n",
    "checkpoint.update(checkpoint_config_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to upload a model checkpoint, you need to set a SambaStudio App.\n",
    "\n",
    "You can search for suitable apps using the checkpoint parameters, and then select the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 21:12:49,711 [INFO] Checkpoint Llama-3.1-8B-Instruct suitable apps:\n",
      "[{'id': 'eb0aaad1-694f-41b6-958a-b974737635c4', 'name': 'Samba1 Llama3.1 Experts'}]\n"
     ]
    }
   ],
   "source": [
    "# Look for suitable apps in SambaStudio\n",
    "suitable_apps = byoc.get_suitable_apps(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have one suitable app\n",
    "checkpoint[\"app_id\"] = suitable_apps[0][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Llama-3.1-8B-Instruct', 'publisher': 'meta-llama', 'description': 'The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).', 'param_count': 8, 'checkpoint_path': '/Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', 'model_arch': 'llama', 'seq_length': 131072, 'vocab_size': 128256, 'app_id': 'eb0aaad1-694f-41b6-958a-b974737635c4'}\n"
     ]
    }
   ],
   "source": [
    "# We can see here all the parameters required to upload the checkpoint\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload the checkpoint to SambaStudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using the checkpoint dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 21:39:55,906 [INFO] Model with name 'Llama-3.1-8B-Instruct' not found\n",
      "2025-03-31 21:39:56,116 [INFO] App with name 'eb0aaad1-694f-41b6-958a-b974737635c4' found with id eb0aaad1-694f-41b6-958a-b974737635c4\n",
      "2025-03-31 21:39:56,116 [INFO] running snapi upload command:\n",
      " snapi import model create --model-name Llama-3.1-8B-Instruct --app eb0aaad1-694f-41b6-958a-b974737635c4 --source-type LOCAL --source-path /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659 --model-arch llama --parameter-count 8b --sequence-length 131072 --vocab-size 128256 -ni --publisher meta-llama --description The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).\n",
      "This could take a while\n",
      "2025-03-31 22:36:08,269 [INFO] Model with name 'Llama-3.1-8B-Instruct' found with id b11219b8-84ba-4c5a-833b-edd6ffd5c0d6\n",
      "2025-03-31 22:36:08,273 [INFO] Model checkpoint with name 'Llama-3.1-8B-Instruct' created it with id b11219b8-84ba-4c5a-833b-edd6ffd5c0d6\n"
     ]
    }
   ],
   "source": [
    "# Call the `upload_checkpoint method` from client with your checkpoint parameters (this can take a while)\n",
    "model_id=byoc.upload_checkpoint(\n",
    "    model_name=checkpoint['model_name'],\n",
    "    checkpoint_path=checkpoint['checkpoint_path'],\n",
    "    description=checkpoint['description'],\n",
    "    publisher=checkpoint['publisher'],\n",
    "    param_count=checkpoint['param_count'],\n",
    "    model_arch=checkpoint['model_arch'],\n",
    "    seq_length=checkpoint['seq_length'],\n",
    "    vocab_size=checkpoint['vocab_size'],\n",
    "    app_id=checkpoint['app_id'],\n",
    "    retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check the status of the uploaded checkpoint \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m byoc.get_checkpoints_status(\u001b[43mmodel_id\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_id' is not defined"
     ]
    }
   ],
   "source": [
    "# Check the status of the uploaded checkpoint \n",
    "byoc.get_checkpoints_status(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using the `target_checkpoint_config.yaml`\n",
    "Alternatively, the checkpoint upload can be done more straightforwardly by setting all the checkpoints parameters in a config file as in [target_checkpoints_config.yaml](../target_checkpoints_config.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 15:14:00,389 [INFO] Using config file located in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/draft_checkpoint_config.yaml\n",
      "2025-03-31 15:14:00,390 [INFO] Using variables from .snapi config to set up Snsdk.\n",
      "2025-03-31 15:14:01,105 [INFO] Params for checkpoint in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95:\n",
      "[{'model_arch': 'llama', 'seq_length': 131072, 'vocab_size': 128256}]\n",
      "2025-03-31 15:14:01,106 [INFO] config updated with checkpoints parameters\n",
      "2025-03-31 15:14:01,345 [INFO] Model with name 'Llama-3.2-3B-Instruct-Francesca' not found\n",
      "2025-03-31 15:14:01,551 [INFO] App with name '49683c7f-3e42-4217-96dd-6f975d17c393' found with id 49683c7f-3e42-4217-96dd-6f975d17c393\n",
      "2025-03-31 15:14:01,552 [INFO] running snapi upload command:\n",
      " snapi import model create --model-name Llama-3.2-3B-Instruct-Francesca --app 49683c7f-3e42-4217-96dd-6f975d17c393 --source-type LOCAL --source-path /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95 --model-arch llama --parameter-count 3b --sequence-length 131072 --vocab-size 128256 -ni --publisher meta-llama --description The `Llama 3.2` collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).\n",
      "This could take a while\n",
      "2025-03-31 15:14:03,222 [ERROR] ================================================================================\n",
      "====================\n",
      "\u001b[34mSelected Config Information:\u001b[0m\n",
      "================================================================================\n",
      "====================\n",
      "Parameter Count               : 3b\n",
      "Sequence Length               : \u001b[1;36m131072\u001b[0m\n",
      "Vocab size                    : \u001b[1;36m128256\u001b[0m\n",
      "\u001b[34mPrompt template in tokenizer_config.json will be used.\u001b[0m\n",
      "\u001b[34mTo edit your chat template, please update your tokenizer_config.json file with \u001b[0m\n",
      "\u001b[34mthe chat_template field before uploading the model.\u001b[0m\n",
      "{\n",
      "    \"message\": \"The configuration provided is not supported at the moment.\",\n",
      "    \"code\": \"5\",\n",
      "    \"correlation_id\": \"8a8c65a5-2635-456e-824d-0cf638ebd998\"\n",
      "}\n",
      "\u001b[31mImport Model cancelled!!\u001b[0m\n",
      "\u001b[31mImport Model cancelled!!\u001b[0m\n",
      "\n",
      "2025-03-31 15:14:03,223 [ERROR] Error uploading model checkpoint Process returned a non-zero exit code.\n",
      "2025-03-31 15:14:03,223 [ERROR] Traceback (most recent call last):\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/snapi/commands/import_export.py\", line 456, in model_import_create\n",
      "    raise typer.Abort()\n",
      "click.exceptions.Abort\n",
      "\u001b[31mAborted.\u001b[0m\n",
      "\n",
      "2025-03-31 15:14:03,224 [INFO] Retrying upload for 1 time...\n",
      "2025-03-31 15:14:03,224 [INFO] running snapi upload command:\n",
      " snapi import model create --model-name Llama-3.2-3B-Instruct-Francesca --app 49683c7f-3e42-4217-96dd-6f975d17c393 --source-type LOCAL --source-path /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95 --model-arch llama --parameter-count 3b --sequence-length 131072 --vocab-size 128256 -ni --publisher meta-llama --description The `Llama 3.2` collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).\n",
      "This could take a while\n",
      "2025-03-31 15:14:04,752 [ERROR] ================================================================================\n",
      "====================\n",
      "\u001b[34mSelected Config Information:\u001b[0m\n",
      "================================================================================\n",
      "====================\n",
      "Parameter Count               : 3b\n",
      "Sequence Length               : \u001b[1;36m131072\u001b[0m\n",
      "Vocab size                    : \u001b[1;36m128256\u001b[0m\n",
      "\u001b[34mPrompt template in tokenizer_config.json will be used.\u001b[0m\n",
      "\u001b[34mTo edit your chat template, please update your tokenizer_config.json file with \u001b[0m\n",
      "\u001b[34mthe chat_template field before uploading the model.\u001b[0m\n",
      "{\n",
      "    \"message\": \"The configuration provided is not supported at the moment.\",\n",
      "    \"code\": \"5\",\n",
      "    \"correlation_id\": \"4d3dc291-bfed-4e90-84ee-7ba76f5f3851\"\n",
      "}\n",
      "\u001b[31mImport Model cancelled!!\u001b[0m\n",
      "\u001b[31mImport Model cancelled!!\u001b[0m\n",
      "\n",
      "2025-03-31 15:14:04,753 [ERROR] Error uploading model checkpoint Process returned a non-zero exit code.\n",
      "2025-03-31 15:14:04,754 [ERROR] Traceback (most recent call last):\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/snapi/commands/import_export.py\", line 456, in model_import_create\n",
      "    raise typer.Abort()\n",
      "click.exceptions.Abort\n",
      "\u001b[31mAborted.\u001b[0m\n",
      "\n",
      "2025-03-31 15:14:04,754 [INFO] Retrying upload for 2 time...\n",
      "2025-03-31 15:14:04,754 [INFO] running snapi upload command:\n",
      " snapi import model create --model-name Llama-3.2-3B-Instruct-Francesca --app 49683c7f-3e42-4217-96dd-6f975d17c393 --source-type LOCAL --source-path /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95 --model-arch llama --parameter-count 3b --sequence-length 131072 --vocab-size 128256 -ni --publisher meta-llama --description The `Llama 3.2` collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).\n",
      "This could take a while\n",
      "2025-03-31 15:14:06,305 [ERROR] ================================================================================\n",
      "====================\n",
      "\u001b[34mSelected Config Information:\u001b[0m\n",
      "================================================================================\n",
      "====================\n",
      "Parameter Count               : 3b\n",
      "Sequence Length               : \u001b[1;36m131072\u001b[0m\n",
      "Vocab size                    : \u001b[1;36m128256\u001b[0m\n",
      "\u001b[34mPrompt template in tokenizer_config.json will be used.\u001b[0m\n",
      "\u001b[34mTo edit your chat template, please update your tokenizer_config.json file with \u001b[0m\n",
      "\u001b[34mthe chat_template field before uploading the model.\u001b[0m\n",
      "{\n",
      "    \"message\": \"The configuration provided is not supported at the moment.\",\n",
      "    \"code\": \"5\",\n",
      "    \"correlation_id\": \"79fe94f1-8a0e-446e-915e-9dfcf5c1929e\"\n",
      "}\n",
      "\u001b[31mImport Model cancelled!!\u001b[0m\n",
      "\u001b[31mImport Model cancelled!!\u001b[0m\n",
      "\n",
      "2025-03-31 15:14:06,306 [ERROR] Error uploading model checkpoint Process returned a non-zero exit code.\n",
      "2025-03-31 15:14:06,306 [ERROR] Traceback (most recent call last):\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/snapi/commands/import_export.py\", line 456, in model_import_create\n",
      "    raise typer.Abort()\n",
      "click.exceptions.Abort\n",
      "\u001b[31mAborted.\u001b[0m\n",
      "\n",
      "2025-03-31 15:14:06,306 [INFO] Retrying upload for 3 time...\n",
      "2025-03-31 15:14:06,306 [INFO] running snapi upload command:\n",
      " snapi import model create --model-name Llama-3.2-3B-Instruct-Francesca --app 49683c7f-3e42-4217-96dd-6f975d17c393 --source-type LOCAL --source-path /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95 --model-arch llama --parameter-count 3b --sequence-length 131072 --vocab-size 128256 -ni --publisher meta-llama --description The `Llama 3.2` collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).\n",
      "This could take a while\n",
      "2025-03-31 15:14:07,846 [ERROR] ================================================================================\n",
      "====================\n",
      "\u001b[34mSelected Config Information:\u001b[0m\n",
      "================================================================================\n",
      "====================\n",
      "Parameter Count               : 3b\n",
      "Sequence Length               : \u001b[1;36m131072\u001b[0m\n",
      "Vocab size                    : \u001b[1;36m128256\u001b[0m\n",
      "\u001b[34mPrompt template in tokenizer_config.json will be used.\u001b[0m\n",
      "\u001b[34mTo edit your chat template, please update your tokenizer_config.json file with \u001b[0m\n",
      "\u001b[34mthe chat_template field before uploading the model.\u001b[0m\n",
      "{\n",
      "    \"message\": \"The configuration provided is not supported at the moment.\",\n",
      "    \"code\": \"5\",\n",
      "    \"correlation_id\": \"2432cd1e-4026-467d-8219-548ddf2f0ddb\"\n",
      "}\n",
      "\u001b[31mImport Model cancelled!!\u001b[0m\n",
      "\u001b[31mImport Model cancelled!!\u001b[0m\n",
      "\n",
      "2025-03-31 15:14:07,847 [ERROR] Error uploading model checkpoint Process returned a non-zero exit code.\n",
      "2025-03-31 15:14:07,848 [ERROR] Traceback (most recent call last):\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/snapi/commands/import_export.py\", line 456, in model_import_create\n",
      "    raise typer.Abort()\n",
      "click.exceptions.Abort\n",
      "\u001b[31mAborted.\u001b[0m\n",
      "\n",
      "2025-03-31 15:14:07,848 [ERROR] Error uploading checkpoint for model Llama-3.2-3B-Instruct-Francesca: Error uploading model checkpoint after 3 attempts\n",
      "max retries exceed\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/utils/byoc/src/snsdk_byoc_wrapper.py\", line 421, in upload_checkpoints\n",
      "    result = future.result()  # This will raise the exception if the thread raised one\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/francescar/Documents/ai-starter-kit/utils/byoc/src/snsdk_byoc_wrapper.py\", line 358, in upload_checkpoint\n",
      "    raise Exception(\n",
      "Exception: Error uploading model checkpoint after 3 attempts\n",
      "max retries exceed\n",
      "2025-03-31 15:14:08,057 [INFO] model Llama-3.2-3B-Instruct-Francesca status: \n",
      " {'code': 5, 'message': 'No import status found for model ID Llama-3.2-3B-Instruct-Francesca', 'details': [], 'status_code': 404, 'headers': {'access-control-allow-headers': 'Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, ResponseType, Access-Control-Allow-Origin', 'access-control-allow-methods': 'GET, POST, PATCH, DELETE', 'access-control-allow-origin': 'https://sjc3-demo2.sambanova.net', 'content-security-policy': \"default-src 'self'\", 'content-type': 'application/json,application/grpc', 'permissions-policy': 'none', 'referrer-policy': 'no-referrer', 'strict-transport-security': 'max-age=31536000; includeSubDomains, max-age=31536000; includeSubDomains', 'x-content-type-options': 'nosniff', 'x-correlation-id': '2a1b6362-84d6-4061-9b32-70e94471a346', 'x-frame-options': 'DENY', 'date': 'Mon, 31 Mar 2025 14:14:07 GMT', 'content-length': '103', 'x-envoy-upstream-service-time': '55', 'server': 'istio-envoy'}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'status'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Wait until all checkpoints are in available status\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     statuses = \u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstatus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_checkpoints_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(x == \u001b[33m\"\u001b[39m\u001b[33mAvailable\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m statuses):\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Wait until all checkpoints are in available status\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     statuses = [\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstatus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m byoc.get_checkpoints_status()]\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(x == \u001b[33m\"\u001b[39m\u001b[33mAvailable\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m statuses):\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'status'"
     ]
    }
   ],
   "source": [
    "config_file = os.path.join(kit_dir, 'draft_checkpoint_config.yaml')\n",
    "byoc = BYOC(config_file)\n",
    "byoc.find_config_params()\n",
    "byoc.upload_checkpoints()\n",
    "# Wait until all checkpoints are in available status\n",
    "while True:\n",
    "    statuses = [model['status'] for model in byoc.get_checkpoints_status()]\n",
    "    if all(x == \"Available\" for x in statuses):\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
