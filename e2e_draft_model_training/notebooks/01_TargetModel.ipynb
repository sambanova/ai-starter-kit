{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. The Target Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target model is available in HuggingFace: [aaditya/Llama3-OpenBioLLM-8B](https://huggingface.co/aaditya/Llama3-OpenBioLLM-8B).\n",
    "\n",
    "`OpenBioLLM-8B` is an advanced open source language model designed specifically for the biomedical domain. Developed by Saama AI Labs, this model leverages cutting-edge techniques to achieve state-of-the-art performance on a wide range of biomedical tasks.\n",
    "\n",
    "Biomedical Specialization: `OpenBioLLM-8B` is tailored for the unique language and knowledge requirements of the medical and life sciences fields. It was fine-tuned on a vast corpus of high-quality biomedical data, enabling it to understand and generate text with domain-specific accuracy and fluency.\n",
    "\n",
    "In this notebook we will detail the following points:\n",
    "1. Download the target model from Hugging Face in your local directory.\n",
    "2. Configure the target model checkpoint.\n",
    "3. Upload the target model on SambaStudio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- [1. Setup](#1-setup)\n",
    "    - [1.1 Imports](#11-imports)\n",
    "    - [1.2 Instantiate the SambaStudio client for BYOC](#12-instantiate-the-sambastudio-client-for-byoc)\n",
    "- [2. Target model: `OpenBioLLM-8B`](#2-target-model-openbiollm-8b)\n",
    "    - [2.1 Download the target model from HuggingFace](#21-download-the-target-model-from-huggingface)\n",
    "    - [2.2 Configure checkpoint](#22-configure-checkpoint)\n",
    "    - [2.3 Set and check chat template (optional)](#23-set-and-check-chat-template-optional)\n",
    "    - [2.4 Set padding token (required for training)](#24-set-padding-token-required-for-training)\n",
    "    - [2.5 Get model params and Sambastudio suitable Apps](#25-get-model-params-and-sambastudio-suitable-apps)\n",
    "- [3. Upload the checkpoint to SambaStudio](#3-upload-the-checkpoint-to-sambastudio)\n",
    "    - [3.1 Using the checkpoint dictionary](#31-using-the-checkpoint-dictionary)\n",
    "    - [3.2 Using the `target_checkpoint_config.yaml`](#32-using-the-target_checkpoint_configyaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "\n",
    "# Get absolute paths for kit_dir and repo_dir\n",
    "current_dir = os.getcwd()\n",
    "kit_dir =  os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "\n",
    "# Adding directories to the Python module search path\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# Bring Your Own Checkpoint (BYOC)\n",
    "from utils.byoc.src.snsdk_byoc_wrapper import BYOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 22:59:53,413 [INFO] Using variables from .snapi config to set up Snsdk.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the BYOC (Bring Your Own Checkpoint) SambaStudio client\n",
    "byoc = BYOC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'The Meta Llama 3.1 collection of multilingual large language '\n",
      "                'models (LLMs) is a collection of pretrained and instruction '\n",
      "                'tuned generative models in 8B, 70B and 405B sizes (text '\n",
      "                'in/text out).',\n",
      " 'model_name': 'Llama-3.1-8B-Instruct',\n",
      " 'param_count': 8,\n",
      " 'publisher': 'meta-llama'}\n"
     ]
    }
   ],
   "source": [
    "# Load the target model config\n",
    "config_target_yaml = '../01_config_target.yaml'\n",
    "\n",
    "# Open and load the YAML file into a dictionary\n",
    "with open(config_target_yaml, 'r') as file:\n",
    "    config_target = yaml.safe_load(file)['target']['model']\n",
    "\n",
    "pprint(config_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target model: `OpenBioLLM-8B`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Download the target model from HuggingFace\n",
    "You can use your own fine-tuned models or you can download and use [Huggingface model checkpoints](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).\n",
    "\n",
    "In our example we will use an available model in HuggingFace: [aaditya/Llama3-OpenBioLLM-8B](https://huggingface.co/aaditya/Llama3-OpenBioLLM-8B).\n",
    "\n",
    "`OpenBioLLM-8B` is an advanced open source language model designed specifically for the biomedical domain. Developed by Saama AI Labs, this model leverages cutting-edge techniques to achieve state-of-the-art performance on a wide range of biomedical tasks.\n",
    "\n",
    "Biomedical Specialization: `OpenBioLLM-8B` is tailored for the unique language and knowledge requirements of the medical and life sciences fields. It was fine-tuned on a vast corpus of high-quality biomedical data, enabling it to understand and generate text with domain-specific accuracy and fluency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model name\n",
    "hf_model = config_target['publisher'] + '/' + config_target['model_name']\n",
    "# Our local target directory\n",
    "target_dir = os.path.join(kit_dir, 'data', 'models') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target dir if it does not exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Download checkpoint to your target directory\n",
    "repo_files = HfApi().list_repo_files(hf_model)\n",
    "for file_name in repo_files:\n",
    "    hf_hub_download(repo_id=hf_model, filename=file_name, cache_dir=target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint folder:  /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n"
     ]
    }
   ],
   "source": [
    "# Find the snapshot folder inside your target directory\n",
    "for root, dirs, files in os.walk(target_dir):\n",
    "    if \"snapshots\" in root and hf_model.replace(\"/\", \"--\") in root:\n",
    "        checkpoint_folder = os.path.join(root, dirs[0])\n",
    "        break\n",
    "\n",
    "print(\"Checkpoint folder: \", checkpoint_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Configure checkpoint\n",
    "\n",
    "Some parameters should be provided as a checkpoint dictionary, in order to upload a previously created checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the checkpoint dictionary\n",
    "checkpoint = {\n",
    "    'model_name': config_target['model_name'],\n",
    "    'publisher': config_target['publisher'],\n",
    "    'description': config_target['description'],\n",
    "    'param_count': config_target['param_count'],\n",
    "    'checkpoint_path': checkpoint_folder\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set and check chat template (optional) \n",
    "\n",
    "If you want to use chat templates (roles structures), you need to include or update the existing chat template.\n",
    "\n",
    "This should be formatted as a Jinja2 String template as in the following `llama` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinjia chat template\n",
    "jinja_chat_template = \"\"\" \n",
    "{% for message in messages %}\n",
    "    {% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\\n' + message['content'] | trim + '<|eot_id|>'+'\\n' %}\n",
    "    {% if loop.index0 == 0 %}{% set content = bos_token + content %}\n",
    "    {% endif %}\n",
    "    {{content}}\n",
    "{% endfor %}\n",
    "{{'<|start_header_id|>assistant<|end_header_id|>'+'\\n'}}\n",
    "\"\"\"\n",
    "# Delete escape characters\n",
    "jinja_chat_template = re.sub(r\"(?<!')\\n(?!')\", \"\", jinja_chat_template).strip().replace('  ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and update your tokenizer config from the checkpoint path\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'tokenizer_config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['chat_template'] = jinja_chat_template\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 22:59:56,528 [INFO] Raw chat template for checkpoint in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659:\n",
      "{% for message in messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\n",
      "' + message['content'] | trim + '<|eot_id|>'+'\n",
      "' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{content}}{% endfor %}{{'<|start_header_id|>assistant<|end_header_id|>'+'\n",
      "'}}\n",
      "\n",
      "2025-04-01 22:59:56,540 [INFO] Rendered template with input test messages:\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "This is a system prompt.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "This is a user prompt.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "This is a response from the assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "This is an user follow up<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Render template when using a roles / chat structure\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"This is a system prompt.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a user prompt.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"This is a response from the assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is an user follow up\"}\n",
    "    ]\n",
    "byoc.check_chat_templates(test_messages, checkpoint_paths=checkpoint['checkpoint_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Set padding token (required for training)\n",
    "\n",
    "If `pad_token_id` is not set in your checkpoint configuration yet and you want to do a further fine-tuning over your checkpoint, you need to set `pad_token_id` in your checkpoint `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pad_token_id to checkpoint config\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['pad_token_id'] = None\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Get model params and Sambastudio suitable apps\n",
    "\n",
    "Extra parameters are required to upload your checkpoint, including model architecture, sequence length, and vocabulary size.\n",
    "\n",
    "These parameters can be extracted from your checkpoint configuration, and included in your checkpoint dictionary parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 22:59:56,557 [INFO] Params for checkpoint in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659:\n",
      "[{'model_arch': 'llama', 'seq_length': 131072, 'vocab_size': 128256}]\n"
     ]
    }
   ],
   "source": [
    "# Extract all the checkpoint parameters\n",
    "checkpoint_config_params = byoc.find_config_params(checkpoint_paths=checkpoint['checkpoint_path'])[0]\n",
    "# Update your checkpoint dictionary\n",
    "checkpoint.update(checkpoint_config_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to upload a model checkpoint, you need to set a SambaStudio App.\n",
    "\n",
    "You can search for suitable apps using the checkpoint parameters, and then select the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 23:00:00,412 [INFO] Checkpoint Llama-3.1-8B-Instruct suitable apps:\n",
      "[{'id': 'eb0aaad1-694f-41b6-958a-b974737635c4', 'name': 'Samba1 Llama3.1 Experts'}]\n"
     ]
    }
   ],
   "source": [
    "# Look for suitable apps in SambaStudio\n",
    "suitable_apps = byoc.get_suitable_apps(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the three suitable apps found, we will use the last one as it is more generic\n",
    "checkpoint[\"app_id\"] = suitable_apps[-1][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Llama-3.1-8B-Instruct', 'publisher': 'meta-llama', 'description': 'The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).', 'param_count': 8, 'checkpoint_path': '/Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', 'model_arch': 'llama', 'seq_length': 131072, 'vocab_size': 128256, 'app_id': 'eb0aaad1-694f-41b6-958a-b974737635c4'}\n"
     ]
    }
   ],
   "source": [
    "# We can see here all the parameters required to upload the checkpoint\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload the checkpoint to SambaStudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using the checkpoint dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 23:00:00,711 [INFO] Model with name 'Llama-3.1-8B-Instruct' found with id b11219b8-84ba-4c5a-833b-edd6ffd5c0d6\n",
      "2025-04-01 23:00:00,713 [INFO] Model checkpoint with name 'Llama-3.1-8B-Instruct' not created it already exist with id b11219b8-84ba-4c5a-833b-edd6ffd5c0d6\n"
     ]
    }
   ],
   "source": [
    "# Call the `upload_checkpoint method` from client with your checkpoint parameters (this can take a while)\n",
    "model_id=byoc.upload_checkpoint(\n",
    "    model_name=checkpoint['model_name'],\n",
    "    checkpoint_path=checkpoint['checkpoint_path'],\n",
    "    description=checkpoint['description'],\n",
    "    publisher=checkpoint['publisher'],\n",
    "    param_count=checkpoint['param_count'],\n",
    "    model_arch=checkpoint['model_arch'],\n",
    "    seq_length=checkpoint['seq_length'],\n",
    "    vocab_size=checkpoint['vocab_size'],\n",
    "    app_id=checkpoint['app_id'],\n",
    "    retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 23:00:00,945 [INFO] model b11219b8-84ba-4c5a-833b-edd6ffd5c0d6 status: \n",
      " {'model_id': 'b11219b8-84ba-4c5a-833b-edd6ffd5c0d6', 'status': 'Available', 'progress': 100, 'stage': 'convert', 'status_code': 200, 'headers': {'access-control-allow-headers': 'Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, ResponseType, Access-Control-Allow-Origin', 'access-control-allow-methods': 'GET, POST, PATCH, DELETE', 'access-control-allow-origin': 'https://sjc3-demo2.sambanova.net', 'content-security-policy': \"default-src 'self'\", 'content-type': 'application/json', 'permissions-policy': 'none', 'referrer-policy': 'no-referrer', 'strict-transport-security': 'max-age=31536000; includeSubDomains, max-age=31536000; includeSubDomains', 'x-content-type-options': 'nosniff', 'x-correlation-id': 'f953649d-f252-4ebb-a6b2-77b4f7314e82', 'x-frame-options': 'DENY', 'date': 'Tue, 01 Apr 2025 22:00:00 GMT', 'x-envoy-upstream-service-time': '69', 'server': 'istio-envoy', 'content-encoding': 'gzip', 'vary': 'Accept-Encoding', 'transfer-encoding': 'chunked'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model_id': 'b11219b8-84ba-4c5a-833b-edd6ffd5c0d6', 'status': 'Available', 'progress': 100, 'stage': 'convert', 'status_code': 200, 'headers': {'access-control-allow-headers': 'Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, ResponseType, Access-Control-Allow-Origin', 'access-control-allow-methods': 'GET, POST, PATCH, DELETE', 'access-control-allow-origin': 'https://sjc3-demo2.sambanova.net', 'content-security-policy': \"default-src 'self'\", 'content-type': 'application/json', 'permissions-policy': 'none', 'referrer-policy': 'no-referrer', 'strict-transport-security': 'max-age=31536000; includeSubDomains, max-age=31536000; includeSubDomains', 'x-content-type-options': 'nosniff', 'x-correlation-id': 'f953649d-f252-4ebb-a6b2-77b4f7314e82', 'x-frame-options': 'DENY', 'date': 'Tue, 01 Apr 2025 22:00:00 GMT', 'x-envoy-upstream-service-time': '69', 'server': 'istio-envoy', 'content-encoding': 'gzip', 'vary': 'Accept-Encoding', 'transfer-encoding': 'chunked'}}]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the status of the uploaded checkpoint \n",
    "byoc.get_checkpoints_status(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using the `target_checkpoint_config.yaml`\n",
    "Alternatively, the checkpoint upload can be done more straightforwardly by setting all the checkpoints parameters in a config file as in [target_checkpoints_config.yaml](../target_checkpoints_config.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(kit_dir, 'target_checkpoint_config.yaml')\n",
    "byoc = BYOC(config_file)\n",
    "byoc.find_config_params()\n",
    "byoc.upload_checkpoints()\n",
    "# Wait until all checkpoints are in available status\n",
    "while True:\n",
    "    statuses = [model['status'] for model in byoc.get_checkpoints_status()]\n",
    "    if all(x == \"Available\" for x in statuses):\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
