{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. The Target Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target model is available in HuggingFace: [TsinghuaC3I/Llama-3.1-8B-UltraMedical](https://huggingface.co/TsinghuaC3I/Llama-3.1-8B-UltraMedical).\n",
    "\n",
    "`Llama-3.1-8B-UltraMedical` is an open-access large language model (LLM) specialized in biomedicine. Developed by the Tsinghua C3I Lab, this model aims to enhance medical examination access, literature comprehension, and clinical knowledge.\n",
    "\n",
    "Building on the foundation of Meta's `Llama-3.1-8B`, `Llama-3.1-8B-UltraMedical` is trained on our UltraMedical collection with supervised fine-tuning (SFT), iterative preference learning (like DPO and KTO). The UltraMedical collection is a large-scale, high-quality dataset of biomedical instructions, comprising 410,000 synthetic and manually curated samples, along with more than 100,000 preference data.\n",
    "\n",
    "In this notebook we will detail the following points:\n",
    "1. Download the target model from Hugging Face in your local directory.\n",
    "2. Configure the target model checkpoint.\n",
    "3. Upload the target model on SambaStudio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- [1. Setup](#1-setup)\n",
    "    - [1.1 Imports](#11-imports)\n",
    "    - [1.2 Instantiate the SambaStudio client for BYOC](#12-instantiate-the-sambastudio-client-for-byoc)\n",
    "- [2. Target model: `OpenBioLLM-8B`](#2-target-model-openbiollm-8b)\n",
    "    - [2.1 Download the target model from HuggingFace](#21-download-the-target-model-from-huggingface)\n",
    "    - [2.2 Configure checkpoint](#22-configure-checkpoint)\n",
    "    - [2.3 Set and check chat template (optional)](#23-set-and-check-chat-template-optional)\n",
    "    - [2.4 Set padding token (required for training)](#24-set-padding-token-required-for-training)\n",
    "    - [2.5 Get model params and Sambastudio suitable Apps](#25-get-model-params-and-sambastudio-suitable-apps)\n",
    "- [3. Upload the checkpoint to SambaStudio](#3-upload-the-checkpoint-to-sambastudio)\n",
    "    - [3.1 Using the checkpoint dictionary](#31-using-the-checkpoint-dictionary)\n",
    "    - [3.2 Using the `target_checkpoint_config.yaml`](#32-using-the-target_checkpoint_configyaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "\n",
    "# Get absolute paths for kit_dir and repo_dir\n",
    "current_dir = os.getcwd()\n",
    "kit_dir =  os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "\n",
    "# Adding directories to the Python module search path\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# Bring Your Own Checkpoint (BYOC)\n",
    "from utils.byoc.src.snsdk_byoc_wrapper import BYOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:49:17,046 [INFO] Using variables from .snapi config to set up Snsdk.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the BYOC (Bring Your Own Checkpoint) SambaStudio client\n",
    "byoc = BYOC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Llama-3.1-8B-UltraMedical is an open-access large language '\n",
      "                'model (LLM) specialized in biomedicine. Developed by the '\n",
      "                'Tsinghua C3I Lab, this model aims to enhance medical '\n",
      "                'examination access, literature comprehension, and clinical '\n",
      "                'knowledge.',\n",
      " 'hf_name': 'TsinghuaC3I/Llama-3.1-8B-UltraMedical',\n",
      " 'model_name': 'Llama-3.1-8B-UltraMedical',\n",
      " 'param_count': 8,\n",
      " 'publisher': 'TsinghuaC3I'}\n"
     ]
    }
   ],
   "source": [
    "# Load the target model config\n",
    "config_target_yaml = '01_config_target.yaml'\n",
    "\n",
    "# Open and load the YAML file into a dictionary\n",
    "with open(config_target_yaml, 'r') as file:\n",
    "    config_target = yaml.safe_load(file)['model']\n",
    "\n",
    "pprint(config_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target model: `OpenBioLLM-8B`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Download the target model from HuggingFace\n",
    "You can use your own fine-tuned models or you can download and use [Huggingface model checkpoints](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).\n",
    "\n",
    "In our example we will use an available model in HuggingFace: [TsinghuaC3I/Llama-3.1-8B-UltraMedical](https://huggingface.co/TsinghuaC3I/Llama-3.1-8B-UltraMedical).\n",
    "\n",
    "Llama-3.1-8B-UltraMedical is an open-access large language model (LLM) specialized in biomedicine. Developed by the Tsinghua C3I Lab, this model aims to enhance medical examination access, literature comprehension, and clinical knowledge.\n",
    "\n",
    "Building on the foundation of Meta's Llama-3.1-8B, Llama-3.1-8B-UltraMedical is trained on our UltraMedical collection with supervised fine-tuning (SFT), iterative preference learning (like DPO and KTO). The UltraMedical collection is a large-scale, high-quality dataset of biomedical instructions, comprising 410,000 synthetic and manually curated samples, along with more than 100,000 preference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model name\n",
    "hf_model = config_target['hf_name']\n",
    "# Our local target directory\n",
    "target_dir = os.path.join(kit_dir, 'data', 'models') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target dir if it does not exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Download checkpoint to your target directory\n",
    "repo_files = HfApi().list_repo_files(hf_model)\n",
    "for file_name in repo_files:\n",
    "    hf_hub_download(repo_id=hf_model, filename=file_name, cache_dir=target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint folder:  /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--TsinghuaC3I--Llama-3.1-8B-UltraMedical/snapshots/c982b9e34f7cf396f41627e7d851629f9e4e5792\n"
     ]
    }
   ],
   "source": [
    "# Find the snapshot folder inside your target directory\n",
    "for root, dirs, files in os.walk(target_dir):\n",
    "    if \"snapshots\" in root and hf_model.replace(\"/\", \"--\") in root:\n",
    "        checkpoint_folder = os.path.join(root, dirs[0])\n",
    "        break\n",
    "\n",
    "print(\"Checkpoint folder: \", checkpoint_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Configure checkpoint\n",
    "\n",
    "Some parameters should be provided as a checkpoint dictionary, in order to upload a previously created checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the checkpoint dictionary\n",
    "checkpoint = {\n",
    "    'model_name': config_target['model_name'],\n",
    "    'publisher': config_target['publisher'],\n",
    "    'description': config_target['description'],\n",
    "    'param_count': config_target['param_count'],\n",
    "    'checkpoint_path': checkpoint_folder\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set and check chat template (optional) \n",
    "\n",
    "If you want to use chat templates (roles structures), you need to include or update the existing chat template.\n",
    "\n",
    "This should be formatted as a Jinja2 String template as in the following `llama` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinjia chat template\n",
    "jinja_chat_template = \"\"\" \n",
    "{% for message in messages %}\n",
    "    {% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\\n' + message['content'] | trim + '<|eot_id|>'+'\\n' %}\n",
    "    {% if loop.index0 == 0 %}{% set content = bos_token + content %}\n",
    "    {% endif %}\n",
    "    {{content}}\n",
    "{% endfor %}\n",
    "{{'<|start_header_id|>assistant<|end_header_id|>'+'\\n'}}\n",
    "\"\"\"\n",
    "# Delete escape characters\n",
    "jinja_chat_template = re.sub(r\"(?<!')\\n(?!')\", \"\", jinja_chat_template).strip().replace('  ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and update your tokenizer config from the checkpoint path\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'tokenizer_config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['chat_template'] = jinja_chat_template\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:49:20,293 [INFO] Raw chat template for checkpoint in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--TsinghuaC3I--Llama-3.1-8B-UltraMedical/snapshots/c982b9e34f7cf396f41627e7d851629f9e4e5792:\n",
      "{% for message in messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\n",
      "' + message['content'] | trim + '<|eot_id|>'+'\n",
      "' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{content}}{% endfor %}{{'<|start_header_id|>assistant<|end_header_id|>'+'\n",
      "'}}\n",
      "\n",
      "2025-04-30 11:49:20,298 [INFO] Rendered template with input test messages:\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "This is a system prompt.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "This is a user prompt.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "This is a response from the assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "This is an user follow up<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Render template when using a roles / chat structure\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"This is a system prompt.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a user prompt.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"This is a response from the assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is an user follow up\"}\n",
    "    ]\n",
    "byoc.check_chat_templates(test_messages, checkpoint_paths=checkpoint['checkpoint_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Set padding token (required for training)\n",
    "\n",
    "If `pad_token_id` is not set in your checkpoint configuration yet and you want to do a further fine-tuning over your checkpoint, you need to set `pad_token_id` in your checkpoint `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pad_token_id to checkpoint config\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['pad_token_id'] = None\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Get model params and Sambastudio suitable apps\n",
    "\n",
    "Extra parameters are required to upload your checkpoint, including model architecture, sequence length, and vocabulary size.\n",
    "\n",
    "These parameters can be extracted from your checkpoint configuration, and included in your checkpoint dictionary parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:49:20,306 [INFO] Params for checkpoint in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--TsinghuaC3I--Llama-3.1-8B-UltraMedical/snapshots/c982b9e34f7cf396f41627e7d851629f9e4e5792:\n",
      "[{'model_arch': 'llama', 'seq_length': 131072, 'vocab_size': 128256}]\n"
     ]
    }
   ],
   "source": [
    "# Extract all the checkpoint parameters\n",
    "checkpoint_config_params = byoc.find_config_params(checkpoint_paths=checkpoint['checkpoint_path'])[0]\n",
    "# Update your checkpoint dictionary\n",
    "checkpoint.update(checkpoint_config_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to upload a model checkpoint, you need to set a SambaStudio App.\n",
    "\n",
    "You can search for suitable apps using the checkpoint parameters, and then select the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:49:21,181 [INFO] Checkpoint Llama-3.1-8B-UltraMedical suitable apps:\n",
      "[{'id': 'eb0aaad1-694f-41b6-958a-b974737635c4', 'name': 'Samba1 Llama3.1 Experts'}]\n"
     ]
    }
   ],
   "source": [
    "# Look for suitable apps in SambaStudio\n",
    "suitable_apps = byoc.get_suitable_apps(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the three suitable apps found, we will use the last one as it is more generic\n",
    "checkpoint[\"app_id\"] = suitable_apps[-1][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Llama-3.1-8B-UltraMedical', 'publisher': 'TsinghuaC3I', 'description': 'Llama-3.1-8B-UltraMedical is an open-access large language model (LLM) specialized in biomedicine. Developed by the Tsinghua C3I Lab, this model aims to enhance medical examination access, literature comprehension, and clinical knowledge.', 'param_count': 8, 'checkpoint_path': '/Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--TsinghuaC3I--Llama-3.1-8B-UltraMedical/snapshots/c982b9e34f7cf396f41627e7d851629f9e4e5792', 'model_arch': 'llama', 'seq_length': 131072, 'vocab_size': 128256, 'app_id': 'eb0aaad1-694f-41b6-958a-b974737635c4'}\n"
     ]
    }
   ],
   "source": [
    "# We can see here all the parameters required to upload the checkpoint\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload the checkpoint to SambaStudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using the checkpoint dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:49:21,390 [INFO] Model with name 'Llama-3.1-8B-UltraMedical' not found\n",
      "2025-04-30 11:49:21,556 [INFO] App with name 'eb0aaad1-694f-41b6-958a-b974737635c4' found with id eb0aaad1-694f-41b6-958a-b974737635c4\n",
      "2025-04-30 11:49:21,556 [INFO] running snapi upload command:\n",
      " snapi import model create --model-name Llama-3.1-8B-UltraMedical --app eb0aaad1-694f-41b6-958a-b974737635c4 --source-type LOCAL --source-path /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--TsinghuaC3I--Llama-3.1-8B-UltraMedical/snapshots/c982b9e34f7cf396f41627e7d851629f9e4e5792 --model-arch llama --parameter-count 8b --sequence-length 131072 --vocab-size 128256 -ni --publisher TsinghuaC3I --description Llama-3.1-8B-UltraMedical is an open-access large language model (LLM) specialized in biomedicine. Developed by the Tsinghua C3I Lab, this model aims to enhance medical examination access, literature comprehension, and clinical knowledge.\n",
      "This could take a while\n",
      "2025-04-30 12:19:50,971 [INFO] Model with name 'Llama-3.1-8B-UltraMedical' found with id b5ca8c6a-e69e-434f-a72e-b75e0d19bda6\n",
      "2025-04-30 12:19:50,974 [INFO] Model checkpoint with name 'Llama-3.1-8B-UltraMedical' created it with id b5ca8c6a-e69e-434f-a72e-b75e0d19bda6\n"
     ]
    }
   ],
   "source": [
    "# Call the `upload_checkpoint method` from client with your checkpoint parameters (this can take a while)\n",
    "model_id=byoc.upload_checkpoint(\n",
    "    model_name=checkpoint['model_name'],\n",
    "    checkpoint_path=checkpoint['checkpoint_path'],\n",
    "    description=checkpoint['description'],\n",
    "    publisher=checkpoint['publisher'],\n",
    "    param_count=checkpoint['param_count'],\n",
    "    model_arch=checkpoint['model_arch'],\n",
    "    seq_length=checkpoint['seq_length'],\n",
    "    vocab_size=checkpoint['vocab_size'],\n",
    "    app_id=checkpoint['app_id'],\n",
    "    retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:19:51,184 [INFO] model b5ca8c6a-e69e-434f-a72e-b75e0d19bda6 status: \n",
      " {'model_id': 'b5ca8c6a-e69e-434f-a72e-b75e0d19bda6', 'status': '', 'progress': 0, 'stage': 'Copy', 'status_code': 200, 'headers': {'Date': 'Wed, 30 Apr 2025 11:19:51 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'access-control-allow-headers': 'Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, ResponseType, Access-Control-Allow-Origin', 'access-control-allow-methods': 'GET, POST, PATCH, DELETE', 'access-control-allow-origin': 'http://sn-studio-internal-1.cloud.snova.ai', 'content-security-policy': \"default-src 'self'\", 'permissions-policy': 'none', 'referrer-policy': 'no-referrer', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-content-type-options': 'nosniff', 'x-correlation-id': '41262a7d-bb14-4ad1-81f5-afe3aa43bc18', 'x-frame-options': 'DENY', 'x-envoy-upstream-service-time': '20', 'content-encoding': 'gzip', 'vary': 'Accept-Encoding'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model_id': 'b5ca8c6a-e69e-434f-a72e-b75e0d19bda6',\n",
       "  'status': '',\n",
       "  'progress': 0,\n",
       "  'stage': 'Copy',\n",
       "  'status_code': 200,\n",
       "  'headers': {'Date': 'Wed, 30 Apr 2025 11:19:51 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'access-control-allow-headers': 'Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, ResponseType, Access-Control-Allow-Origin', 'access-control-allow-methods': 'GET, POST, PATCH, DELETE', 'access-control-allow-origin': 'http://sn-studio-internal-1.cloud.snova.ai', 'content-security-policy': \"default-src 'self'\", 'permissions-policy': 'none', 'referrer-policy': 'no-referrer', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-content-type-options': 'nosniff', 'x-correlation-id': '41262a7d-bb14-4ad1-81f5-afe3aa43bc18', 'x-frame-options': 'DENY', 'x-envoy-upstream-service-time': '20', 'content-encoding': 'gzip', 'vary': 'Accept-Encoding'}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the status of the uploaded checkpoint \n",
    "byoc.get_checkpoints_status(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
