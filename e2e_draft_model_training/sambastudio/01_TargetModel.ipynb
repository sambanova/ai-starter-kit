{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. The Target Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target model is available in HuggingFace: [TsinghuaC3I/Llama-3.1-8B-UltraMedical](https://huggingface.co/TsinghuaC3I/Llama-3.1-8B-UltraMedical).\n",
    "\n",
    "`Llama-3.1-8B-UltraMedical` is an open-access large language model (LLM) specialized in biomedicine. Developed by the Tsinghua C3I Lab, this model aims to enhance medical examination access, literature comprehension, and clinical knowledge.\n",
    "\n",
    "Building on the foundation of Meta's `Llama-3.1-8B`, `Llama-3.1-8B-UltraMedical` is trained on the UltraMedical collection with supervised fine-tuning (SFT), iterative preference learning (like DPO and KTO). The UltraMedical collection is a large-scale, high-quality dataset of biomedical instructions, comprising 410,000 synthetic and manually curated samples, along with more than 100,000 preference data.\n",
    "\n",
    "In this notebook we will detail the following points:\n",
    "1. Download the target model from Hugging Face in your local directory.\n",
    "2. Configure the target model checkpoint.\n",
    "3. Upload the target model on SambaStudio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- [1. Setup](#1-setup)\n",
    "    - [1.1 Imports](#11-imports)\n",
    "    - [1.2 Instantiate the SambaStudio client for BYOC](#12-instantiate-the-sambastudio-client-for-byoc)\n",
    "- [2. Target model: `Llama-3.1-8B-UltraMedical`](#2-target-model-llama-31-8b-ultramedical)\n",
    "    - [2.1 Download the target model from HuggingFace](#21-download-the-target-model-from-huggingface)\n",
    "    - [2.2 Configure checkpoint](#22-configure-checkpoint)\n",
    "    - [2.3 Set and check chat template (optional)](#23-set-and-check-chat-template-optional)\n",
    "    - [2.4 Set padding token (required for training)](#24-set-padding-token-required-for-training)\n",
    "    - [2.5 Get model params and Sambastudio suitable Apps](#25-get-model-params-and-sambastudio-suitable-apps)\n",
    "- [3. Upload the checkpoint to SambaStudio](#3-upload-the-checkpoint-to-sambastudio)\n",
    "    - [3.1 Using the checkpoint dictionary](#31-using-the-checkpoint-dictionary)\n",
    "    - [3.2 Using the `target_checkpoint_config.yaml`](#32-using-the-target_checkpoint_configyaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "\n",
    "# Get absolute paths for kit_dir and repo_dir\n",
    "current_dir = os.getcwd()\n",
    "kit_dir =  os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "\n",
    "# Adding directories to the Python module search path\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# Bring Your Own Checkpoint (BYOC)\n",
    "from utils.byoc.src.snsdk_byoc_wrapper import BYOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the BYOC (Bring Your Own Checkpoint) SambaStudio client\n",
    "byoc = BYOC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the target model config\n",
    "config_target_yaml = '01_config_target.yaml'\n",
    "\n",
    "# Open and load the YAML file into a dictionary\n",
    "with open(config_target_yaml, 'r') as file:\n",
    "    config_target = yaml.safe_load(file)['model']\n",
    "\n",
    "pprint(config_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target model: `Llama-3.1-8B-UltraMedical`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Download the target model from HuggingFace\n",
    "You can use your own fine-tuned models or you can download and use [Huggingface model checkpoints](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).\n",
    "\n",
    "In our example we will use an available model in HuggingFace: [TsinghuaC3I/Llama-3.1-8B-UltraMedical](https://huggingface.co/TsinghuaC3I/Llama-3.1-8B-UltraMedical).\n",
    "\n",
    "Llama-3.1-8B-UltraMedical is an open-access large language model (LLM) specialized in biomedicine. Developed by the Tsinghua C3I Lab, this model aims to enhance medical examination access, literature comprehension, and clinical knowledge.\n",
    "\n",
    "Building on the foundation of Meta's Llama-3.1-8B, Llama-3.1-8B-UltraMedical is trained on the UltraMedical collection with supervised fine-tuning (SFT), iterative preference learning (like DPO and KTO). The UltraMedical collection is a large-scale, high-quality dataset of biomedical instructions, comprising 410,000 synthetic and manually curated samples, along with more than 100,000 preference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model name\n",
    "hf_model = config_target['hf_name']\n",
    "# Our local target directory\n",
    "target_dir = os.path.join(kit_dir, 'data', 'models') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target dir if it does not exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Download checkpoint to your target directory\n",
    "repo_files = HfApi().list_repo_files(hf_model)\n",
    "for file_name in repo_files:\n",
    "    hf_hub_download(repo_id=hf_model, filename=file_name, cache_dir=target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the snapshot folder inside your target directory\n",
    "for root, dirs, files in os.walk(target_dir):\n",
    "    if \"snapshots\" in root and hf_model.replace(\"/\", \"--\") in root:\n",
    "        checkpoint_folder = os.path.join(root, dirs[0])\n",
    "        break\n",
    "\n",
    "print(\"Checkpoint folder: \", checkpoint_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Configure checkpoint\n",
    "\n",
    "Some parameters should be provided as a checkpoint dictionary, in order to upload a previously created checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the checkpoint dictionary\n",
    "checkpoint = {\n",
    "    'model_name': config_target['model_name'],\n",
    "    'publisher': config_target['publisher'],\n",
    "    'description': config_target['description'],\n",
    "    'param_count': config_target['param_count'],\n",
    "    'checkpoint_path': checkpoint_folder\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set and check chat template (optional) \n",
    "\n",
    "If you want to use chat templates (roles structures), you need to include or update the existing chat template.\n",
    "\n",
    "This should be formatted as a Jinja2 String template as in the following `llama` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinjia chat template\n",
    "jinja_chat_template = \"\"\" \n",
    "{% for message in messages %}\n",
    "    {% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\\n' + message['content'] | trim + '<|eot_id|>'+'\\n' %}\n",
    "    {% if loop.index0 == 0 %}{% set content = bos_token + content %}\n",
    "    {% endif %}\n",
    "    {{content}}\n",
    "{% endfor %}\n",
    "{{'<|start_header_id|>assistant<|end_header_id|>'+'\\n'}}\n",
    "\"\"\"\n",
    "# Delete escape characters\n",
    "jinja_chat_template = re.sub(r\"(?<!')\\n(?!')\", \"\", jinja_chat_template).strip().replace('  ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and update your tokenizer config from the checkpoint path\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'tokenizer_config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['chat_template'] = jinja_chat_template\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render template when using a roles / chat structure\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"This is a system prompt.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a user prompt.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"This is a response from the assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is an user follow up\"}\n",
    "    ]\n",
    "byoc.check_chat_templates(test_messages, checkpoint_paths=checkpoint['checkpoint_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Set padding token (required for training)\n",
    "\n",
    "If `pad_token_id` is not set in your checkpoint configuration yet and you want to do a further fine-tuning over your checkpoint, you need to set `pad_token_id` in your checkpoint `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pad_token_id to checkpoint config\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['pad_token_id'] = None\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Get model params and Sambastudio suitable apps\n",
    "\n",
    "Extra parameters are required to upload your checkpoint, including model architecture, sequence length, and vocabulary size.\n",
    "\n",
    "These parameters can be extracted from your checkpoint configuration, and included in your checkpoint dictionary parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the checkpoint parameters\n",
    "checkpoint_config_params = byoc.find_config_params(checkpoint_paths=checkpoint['checkpoint_path'])[0]\n",
    "# Update your checkpoint dictionary\n",
    "checkpoint.update(checkpoint_config_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to upload a model checkpoint, you need to set a SambaStudio App.\n",
    "\n",
    "You can search for suitable apps using the checkpoint parameters, and then select the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for suitable apps in SambaStudio\n",
    "suitable_apps = byoc.get_suitable_apps(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the three suitable apps found, we will use the last one as it is more generic\n",
    "checkpoint[\"app_id\"] = suitable_apps[-1][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see here all the parameters required to upload the checkpoint\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload the checkpoint to SambaStudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using the checkpoint dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the `upload_checkpoint method` from client with your checkpoint parameters (this can take a while)\n",
    "model_id=byoc.upload_checkpoint(\n",
    "    model_name=checkpoint['model_name'],\n",
    "    checkpoint_path=checkpoint['checkpoint_path'],\n",
    "    description=checkpoint['description'],\n",
    "    publisher=checkpoint['publisher'],\n",
    "    param_count=checkpoint['param_count'],\n",
    "    model_arch=checkpoint['model_arch'],\n",
    "    seq_length=checkpoint['seq_length'],\n",
    "    vocab_size=checkpoint['vocab_size'],\n",
    "    app_id=checkpoint['app_id'],\n",
    "    retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the uploaded checkpoint \n",
    "byoc.get_checkpoints_status(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
