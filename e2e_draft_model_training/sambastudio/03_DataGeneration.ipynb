{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 03. Training Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from langchain_sambanova import ChatSambaStudio\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir =  os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from utils.fine_tuning.src.snsdk_wrapper import SnsdkWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SambaNova SDK SambaStudio client\n",
    "sambastudio_client = SnsdkWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the target model config\n",
    "config_target_yaml = '01_config_target.yaml'\n",
    "\n",
    "# Open and load the YAML file into a dictionary\n",
    "with open(config_target_yaml, 'r') as file:\n",
    "    config_target = yaml.safe_load(file)\n",
    "pprint('Target model:')\n",
    "pprint(config_target)\n",
    "\n",
    "# Load the training data preparation config\n",
    "config_training_data_generation_yaml = '03_config_data_generation.yaml'\n",
    "\n",
    "# Open and load the YAML file into a dictionary\n",
    "with open(config_training_data_generation_yaml, 'r') as file:\n",
    "    config_data_generation = yaml.safe_load(file)\n",
    "pprint('Dataset creation:')\n",
    "pprint(config_data_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Project configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = {\n",
    "    'project_name': config_data_generation['project']['project_name'],\n",
    "    'project_description': config_data_generation['project']['project_description'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the create project method from client with project parameters\n",
    "sambastudio_client.create_project(\n",
    "    project_name = project['project_name'],\n",
    "    project_description = project['project_description']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set endpoint config \n",
    "endpoint = {\n",
    "  'endpoint_name': config_target['model']['model_name'].lower(),\n",
    "  'endpoint_description': f'Endpoint for {config_target[\"model\"][\"model_name\"]}',\n",
    "  'endpoint_instances': 1,\n",
    "  'hyperparams': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the create endpoint method from client with endpoint parameters\n",
    "sambastudio_client.create_endpoint(\n",
    "    project_name=project['project_name'],\n",
    "    endpoint_name=endpoint['endpoint_name'],\n",
    "    endpoint_description=endpoint['endpoint_description'],\n",
    "    model_name=config_target['model']['model_name'],\n",
    "    model_version=1,\n",
    "    instances=endpoint['endpoint_instances'],\n",
    "    hyperparams=endpoint['hyperparams'],\n",
    "    rdu_arch=config_data_generation['sambastudio']['rdu_arch'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get endpoint details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get endpoint details, including api key and envs\n",
    "endpoint_env = sambastudio_client.get_endpoint_details(\n",
    "    project_name=project['project_name'],\n",
    "    endpoint_name=endpoint['endpoint_name']\n",
    "    )['langchain_wrapper_env']\n",
    "\n",
    "pprint(endpoint_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inference on a single question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate langchain chat models to test inference \n",
    "llm = ChatSambaStudio(\n",
    "    sambastudio_url=endpoint_env.get(\"SAMBASTUDIO_URL\"),\n",
    "    sambastudio_api_key=endpoint_env.get(\"SAMBASTUDIO_API_KEY\"),\n",
    "    temperature = 0.01,\n",
    "    max_tokens = 1024,\n",
    "    top_p = 0.1,\n",
    "    do_sample = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are an expert and experienced from the healthcare and biomedical domain with extensive medical knowledge and practical experience. Your name is OpenBioLLM, and you were developed by Saama AI Labs. who's willing to help answer the user's query with explanation. In your explanation, leverage your deep medical expertise such as relevant anatomical structures, physiological processes, diagnostic criteria, treatment guidelines, or other pertinent medical concepts. Use precise medical terminology while still aiming to make the explanation clear and accessible to a general audience.\"),\n",
    "    (\"human\", \"What are the morphological characteristics of a particular organism that determine its correct genus classification in the Taxonomy system? Identify the key features that indicate the proper classification for a given species and explain how they are used in Taxonomy\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the input JSONL file\n",
    "input_filename = config_data_generation['files']['input_filename']\n",
    "# Path to the output JSONL file\n",
    "output_filename = config_data_generation['files']['output_filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lines = []\n",
    "with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for record in tqdm(data):\n",
    "    # Extract the system prompt and the human instruction\n",
    "    system_msg: str = record.get('system_prompt', '')\n",
    "    human_msg: str = record.get('instruction', '')\n",
    "\n",
    "    # Construct the messages list as required\n",
    "    messages = [\n",
    "        (\"system\", system_msg),\n",
    "        (\"human\", human_msg)\n",
    "    ]\n",
    "\n",
    "    # Invoke the LLM with the messages list and get the content of the response\n",
    "    # Here we call the llm.invoke() and access its 'content' attribute.\n",
    "    response = llm.invoke(messages)\n",
    "    completion = response.content\n",
    "\n",
    "    # Create a new dictionary (triple) with the keys system, prompt, and completion.\n",
    "    new_entry = {\n",
    "        \"system\": system_msg,\n",
    "        \"prompt\": human_msg,\n",
    "        \"completion\": completion\n",
    "    }\n",
    "    output_lines.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the transformed entries into the output JSONL file\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for entry in output_lines:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"Successfully processed {len(output_lines)} entries and saved to {output_filename}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
