{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. The Draft Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our draft model is also available in HuggingFace: [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), as well as SambaStudio.\n",
    "\n",
    "The `Llama 3.2` collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The `Llama 3.2` instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n",
    "\n",
    "In this notebook we will detail the following points:\n",
    "1. Download the draft model from Hugging Face in your local directory.\n",
    "2. Configure the draft model checkpoint.\n",
    "3. Upload the draft model on SambaStudio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- [1. Setup](#1-setup)\n",
    "    - [1.1 Imports](#11-imports)\n",
    "    - [1.2 Instantiate the SambaStudio client for BYOC](#12-instantiate-the-sambastudio-client-for-byoc)\n",
    "- [2. Target model: `Llama-3.2-1B-Instruct`](#2-target-model-llama-1b-instruct)\n",
    "    - [2.1 Download the target model from HuggingFace](#21-download-the-target-model-from-huggingface)\n",
    "    - [2.2 Configure checkpoint](#22-configure-checkpoint)\n",
    "    - [2.3 Set and check chat template (optional)](#23-set-and-check-chat-template-optional)\n",
    "    - [2.4 Set padding token (required for training)](#24-set-padding-token-required-for-training)\n",
    "    - [2.5 Get model params and Sambastudio suitable Apps](#25-get-model-params-and-sambastudio-suitable-apps)\n",
    "- [3. Upload the checkpoint to SambaStudio](#3-upload-the-checkpoint-to-sambastudio)\n",
    "    - [3.1 Using the checkpoint dictionary](#31-using-the-checkpoint-dictionary)\n",
    "    - [3.2 Using the `draft_checkpoint_config.yaml`](#32-using-the-target_checkpoint_configyaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescar/Documents/ai-starter-kit/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "import time\n",
    "import yaml\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "\n",
    "# Get absolute paths for kit_dir and repo_dir\n",
    "current_dir = os.getcwd()\n",
    "kit_dir =  os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "\n",
    "# Adding directories to the Python module search path\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "# Bring Your Own Checkpoint (BYOC)\n",
    "from utils.byoc.src.snsdk_byoc_wrapper import BYOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:30:13,020 [INFO] Using variables from .snapi config to set up Snsdk.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the BYOC (Bring Your Own Checkpoint) SambaStudio client\n",
    "byoc = BYOC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'The Meta Llama 3.1 collection of multilingual large language '\n",
      "                'models (LLMs) is a collection of pretrained and instruction '\n",
      "                'tuned generative models in 8B, 70B and 405B sizes (text '\n",
      "                'in/text out).',\n",
      " 'hf_name': 'meta-llama/Llama-3.2-1B-Instruct',\n",
      " 'model_name': 'Llama-3.2-1B-Instruct',\n",
      " 'param_count': 8,\n",
      " 'publisher': 'meta-llama'}\n"
     ]
    }
   ],
   "source": [
    "# Load the draft model config\n",
    "config_draft_yaml = '02_config_draft.yaml'\n",
    "\n",
    "# Open and load the YAML file into a dictionary\n",
    "with open(config_draft_yaml, 'r') as file:\n",
    "    config_draft = yaml.safe_load(file)['model']\n",
    "\n",
    "pprint(config_draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target model: `Llama-3.2-1B-Instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Download the target model from HuggingFace\n",
    "You can use your own fine-tuned models or you can download and use [Huggingface model checkpoints](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).\n",
    "\n",
    "In our example we will use an available model in HuggingFace: [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), as well as SambaStudio.\n",
    "\n",
    "The `Llama 3.2` collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The `Llama 3.2` instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model name\n",
    "# xxx\n",
    "# hf_model = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "hf_model = config_draft['hf_name']\n",
    "# Our local target directory\n",
    "target_dir = os.path.join(kit_dir, 'data', 'models') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target dir if it does not exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Download checkpoint to your target directory\n",
    "repo_files = HfApi().list_repo_files(hf_model)\n",
    "for file_name in repo_files:\n",
    "    hf_hub_download(repo_id=hf_model, filename=file_name, cache_dir=target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint folder:  /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6\n"
     ]
    }
   ],
   "source": [
    "# Find the snapshot folder inside your target directory\n",
    "for root, dirs, files in os.walk(target_dir):\n",
    "    if \"snapshots\" in root and hf_model.replace(\"/\", \"--\") in root:\n",
    "        checkpoint_folder = os.path.join(root, dirs[0])\n",
    "        break\n",
    "\n",
    "print(\"Checkpoint folder: \", checkpoint_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Configure checkpoint\n",
    "\n",
    "Some parameters should be provided as a checkpoint dictionary, in order to upload a previously created checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the checkpoint dictionary\n",
    "checkpoint = {\n",
    "    'model_name': config_draft['model_name'],\n",
    "    'publisher': config_draft['publisher'],\n",
    "    'description': config_draft['description'],\n",
    "    'param_count': config_draft['param_count'],\n",
    "    'checkpoint_path': checkpoint_folder\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set and check chat template (optional) \n",
    "\n",
    "If you want to use chat templates (roles structures), you need to include or update the existing chat template.\n",
    "\n",
    "This should be formatted as a Jinja2 String template as in the following `llama` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinjia chat template\n",
    "jinja_chat_template = \"\"\" \n",
    "{% for message in messages %}\n",
    "    {% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\\n' + message['content'] | trim + '<|eot_id|>'+'\\n' %}\n",
    "    {% if loop.index0 == 0 %}{% set content = bos_token + content %}\n",
    "    {% endif %}\n",
    "    {{content}}\n",
    "{% endfor %}\n",
    "{{'<|start_header_id|>assistant<|end_header_id|>'+'\\n'}}\n",
    "\"\"\"\n",
    "# Delete escape characters\n",
    "jinja_chat_template = re.sub(r\"(?<!')\\n(?!')\", \"\", jinja_chat_template).strip().replace('  ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and update your tokenizer config from the checkpoint path\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'tokenizer_config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['chat_template'] = jinja_chat_template\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:30:17,932 [INFO] Raw chat template for checkpoint in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6:\n",
      "{% for message in messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\n",
      "' + message['content'] | trim + '<|eot_id|>'+'\n",
      "' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{content}}{% endfor %}{{'<|start_header_id|>assistant<|end_header_id|>'+'\n",
      "'}}\n",
      "\n",
      "2025-04-30 13:30:17,937 [INFO] Rendered template with input test messages:\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "This is a system prompt.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "This is a user prompt.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "This is a response from the assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "This is an user follow up<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Render template when using a roles / chat structure\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"This is a system prompt.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is a user prompt.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"This is a response from the assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is an user follow up\"}\n",
    "    ]\n",
    "byoc.check_chat_templates(test_messages, checkpoint_paths=checkpoint['checkpoint_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Set padding token (required for training)\n",
    "\n",
    "If `pad_token_id` is not set in your checkpoint configuration yet and you want to do a further fine-tuning over your checkpoint, you need to set `pad_token_id` in your checkpoint `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pad_token_id to checkpoint config\n",
    "with open(os.path.join(checkpoint['checkpoint_path'], 'config.json'), 'r+') as file:\n",
    "    data = json.load(file)\n",
    "    data['pad_token_id'] = None\n",
    "    file.seek(0)\n",
    "    file.truncate()\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Get model params and Sambastudio suitable apps\n",
    "\n",
    "Extra parameters are required to upload your checkpoint, including model architecture, sequence length, and vocabulary size.\n",
    "\n",
    "These parameters can be extracted from your checkpoint configuration, and included in your checkpoint dictionary parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:30:17,946 [INFO] Params for checkpoint in /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6:\n",
      "[{'model_arch': 'llama', 'seq_length': 131072, 'vocab_size': 128256}]\n"
     ]
    }
   ],
   "source": [
    "# Extract all the checkpoint parameters\n",
    "checkpoint_config_params = byoc.find_config_params(checkpoint_paths=checkpoint['checkpoint_path'])[0]\n",
    "# Update your checkpoint dictionary\n",
    "checkpoint.update(checkpoint_config_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to upload a model checkpoint, you need to set a SambaStudio App.\n",
    "\n",
    "You can search for suitable apps using the checkpoint parameters, and then select the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:30:18,888 [INFO] Checkpoint Llama-3.2-1B-Instruct suitable apps:\n",
      "[{'id': 'eb0aaad1-694f-41b6-958a-b974737635c4', 'name': 'Samba1 Llama3.1 Experts'}]\n"
     ]
    }
   ],
   "source": [
    "# Look for suitable apps in SambaStudio\n",
    "suitable_apps = byoc.get_suitable_apps(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have one suitable app\n",
    "checkpoint[\"app_id\"] = suitable_apps[0][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Llama-3.2-1B-Instruct', 'publisher': 'meta-llama', 'description': 'The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).', 'param_count': 8, 'checkpoint_path': '/Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6', 'model_arch': 'llama', 'seq_length': 131072, 'vocab_size': 128256, 'app_id': 'eb0aaad1-694f-41b6-958a-b974737635c4'}\n"
     ]
    }
   ],
   "source": [
    "# We can see here all the parameters required to upload the checkpoint\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload the checkpoint to SambaStudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using the checkpoint dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:30:19,119 [INFO] Model with name 'Llama-3.2-1B-Instruct' not found\n",
      "2025-04-30 13:30:19,306 [INFO] App with name 'eb0aaad1-694f-41b6-958a-b974737635c4' found with id eb0aaad1-694f-41b6-958a-b974737635c4\n",
      "2025-04-30 13:30:19,307 [INFO] running snapi upload command:\n",
      " snapi import model create --model-name Llama-3.2-1B-Instruct --app eb0aaad1-694f-41b6-958a-b974737635c4 --source-type LOCAL --source-path /Users/francescar/Documents/ai-starter-kit/e2e_draft_model_training/data/models/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 --model-arch llama --parameter-count 8b --sequence-length 131072 --vocab-size 128256 -ni --publisher meta-llama --description The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).\n",
      "This could take a while\n",
      "2025-04-30 13:38:27,378 [INFO] Model with name 'Llama-3.2-1B-Instruct' found with id b825ab89-2171-4175-9de2-e38c9ec6275e\n",
      "2025-04-30 13:38:27,380 [INFO] Model checkpoint with name 'Llama-3.2-1B-Instruct' created it with id b825ab89-2171-4175-9de2-e38c9ec6275e\n"
     ]
    }
   ],
   "source": [
    "# Call the `upload_checkpoint method` from client with your checkpoint parameters (this can take a while)\n",
    "model_id=byoc.upload_checkpoint(\n",
    "    model_name=checkpoint['model_name'],\n",
    "    checkpoint_path=checkpoint['checkpoint_path'],\n",
    "    description=checkpoint['description'],\n",
    "    publisher=checkpoint['publisher'],\n",
    "    param_count=checkpoint['param_count'],\n",
    "    model_arch=checkpoint['model_arch'],\n",
    "    seq_length=checkpoint['seq_length'],\n",
    "    vocab_size=checkpoint['vocab_size'],\n",
    "    app_id=checkpoint['app_id'],\n",
    "    retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 13:38:27,583 [INFO] model b825ab89-2171-4175-9de2-e38c9ec6275e status: \n",
      " {'model_id': 'b825ab89-2171-4175-9de2-e38c9ec6275e', 'status': '', 'progress': 0, 'stage': 'Copy', 'status_code': 200, 'headers': {'Date': 'Wed, 30 Apr 2025 12:38:27 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'access-control-allow-headers': 'Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, ResponseType, Access-Control-Allow-Origin', 'access-control-allow-methods': 'GET, POST, PATCH, DELETE', 'access-control-allow-origin': 'http://sn-studio-internal-1.cloud.snova.ai', 'content-security-policy': \"default-src 'self'\", 'permissions-policy': 'none', 'referrer-policy': 'no-referrer', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-content-type-options': 'nosniff', 'x-correlation-id': '56c92eb5-3bd0-439e-8bf3-8d1cf8319427', 'x-frame-options': 'DENY', 'x-envoy-upstream-service-time': '18', 'content-encoding': 'gzip', 'vary': 'Accept-Encoding'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model_id': 'b825ab89-2171-4175-9de2-e38c9ec6275e',\n",
       "  'status': '',\n",
       "  'progress': 0,\n",
       "  'stage': 'Copy',\n",
       "  'status_code': 200,\n",
       "  'headers': {'Date': 'Wed, 30 Apr 2025 12:38:27 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'access-control-allow-headers': 'Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, ResponseType, Access-Control-Allow-Origin', 'access-control-allow-methods': 'GET, POST, PATCH, DELETE', 'access-control-allow-origin': 'http://sn-studio-internal-1.cloud.snova.ai', 'content-security-policy': \"default-src 'self'\", 'permissions-policy': 'none', 'referrer-policy': 'no-referrer', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-content-type-options': 'nosniff', 'x-correlation-id': '56c92eb5-3bd0-439e-8bf3-8d1cf8319427', 'x-frame-options': 'DENY', 'x-envoy-upstream-service-time': '18', 'content-encoding': 'gzip', 'vary': 'Accept-Encoding'}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the status of the uploaded checkpoint \n",
    "byoc.get_checkpoints_status(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
