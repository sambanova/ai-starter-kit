{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaNova LLMs: Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install these required dependencies to run this notebook\n",
    "!pip install python-dotenv==1.0.0\n",
    "!pip install requests\n",
    "!pip install langchain-core==0.3.68\n",
    "!pip install langchain-community==0.3.27\n",
    "!pip install sseclient-py==1.8.0\n",
    "!pip install langchain-sambanova==0.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from utils.model_wrappers.langchain_llms import SambaNovaCloud, SambaStudio\n",
    "from langchain_sambanova import ChatSambaNovaCloud, ChatSambaStudio\n",
    "\n",
    "load_dotenv(os.path.join(repo_dir, '.env'), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sambanova Cloud LLM wrapper\n",
    "\n",
    "First you should set your environment variables, for this follow the instructions [here](../README.md#use-sambanova-cloud-option-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instantiation \n",
    "llm = SambaNovaCloud(\n",
    "    model='Meta-Llama-3.1-405B-Instruct',\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    stop_tokens=[],\n",
    "    top_k=5,\n",
    "    top_p=0.1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did Madam ask Hannah to refer a deed to Bob?\n",
      "\n",
      "Because Madam said, \"A man, a plan, a canal, Panama!\" and Hannah replied, \"Aha, Bob, a deed is a deed, and I'll refer it to level-headed Eve, but never to a madam like you, for it's a civic deed, and I won't do it, Hannah.\"\n",
      "\n",
      "But the real answer is: Because it was a \"madam, I'm Adam\" situation, and Hannah just wanted to level with Bob and say, \"A Santa at NASA\" told her to do it!\n",
      "\n",
      "Okay, okay, I know, it's a bit of a stretch, but I hope the palindrome words \"madam,\" \"Hannah,\" \"level,\" \"deed,\" \"civic,\" \"refer,\" and \"A man, a plan, a canal, Panama\" brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke('tell me a joke using palindrome words')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Butterflies taste with their feet.\n"
     ]
    }
   ],
   "source": [
    "response = llm.ainvoke('tell me an interesting fact in 5 words')\n",
    "print(await response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The capital of France is Paris.',\n",
      " 'The capital of Japan is Tokyo.',\n",
      " 'The capital of Serbia is Belgrade.']\n"
     ]
    }
   ],
   "source": [
    "response = llm.batch([\"what is the capital of France?\",\"what is the capital of Japan\",\"what is the capital of Servia\"])\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The square root of 81 is 9.',\n",
      " 'The natural logarithm of e is 1.\\n'\n",
      " '\\n'\n",
      " 'In mathematics, the natural logarithm (denoted by ln) is the logarithm to '\n",
      " 'the base e, where e is a mathematical constant approximately equal to '\n",
      " '2.71828.\\n'\n",
      " '\\n'\n",
      " 'By definition, the natural logarithm of e is:\\n'\n",
      " '\\n'\n",
      " 'ln(e) = 1\\n'\n",
      " '\\n'\n",
      " 'This is because the natural logarithm is the inverse function of the '\n",
      " 'exponential function, and e is the base of the exponential function. In '\n",
      " 'other words, the natural logarithm of e is the power to which e must be '\n",
      " 'raised to equal e, which is 1.']\n"
     ]
    }
   ],
   "source": [
    "response = llm.abatch([\"what is the square root of 81?\",\"what is the natural logarithm of e\"])\n",
    "pprint(await response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The planets in our solar \n",
      "system are named \n",
      "after ancient Roman gods \n",
      "and goddesses. Here's \n",
      "a \n",
      "brief overview:\n",
      "\n",
      "\n",
      "\n",
      "1. Mercury - Named after the \n",
      "Roman messenger god, \n",
      "\n",
      "Mercurius (equivalent to the Greek \n",
      "god Hermes).\n",
      "2. Venus \n",
      "- Named after the Roman goddess \n",
      "of love and beauty, \n",
      "Venus (equivalent to the Greek \n",
      "goddess \n",
      "Aphrodite).\n",
      "3. Earth \n",
      "- Not directly named after a \n",
      "Roman god, \n",
      "but \n",
      "rather derived from Old English \n",
      "and Germanic words for \n",
      "\"ground\" and \"soil.\"\n",
      "\n",
      "4. Mars - \n",
      "Named after the Roman god \n",
      "of war, Mars (equivalent \n",
      "to the Greek \n",
      "god Ares).\n",
      "5. Jupiter - Named \n",
      "after the Roman king of \n",
      "the \n",
      "gods, Jupiter (equivalent to \n",
      "the Greek god \n",
      "Zeus).\n",
      "6. Saturn - Named after the \n",
      "Roman god of agriculture and \n",
      "time, Saturnus (equivalent \n",
      "to the Greek \n",
      "god Cronus).\n",
      "7. Uranus - \n",
      "Named after the Greek god of \n",
      "the sky, \n",
      "Ouranos (the Romans did not have \n",
      "a direct equivalent).\n",
      "8. \n",
      "Neptune - Named after the Roman \n",
      "god of the sea, \n",
      "Neptune (equivalent to the Greek \n",
      "god \n",
      "Poseidon).\n",
      "\n",
      "The \n",
      "dwarf planets in our solar system \n",
      "are also named \n",
      "after mythological \n",
      "figures:\n",
      "\n",
      "1. Pluto - Named after the \n",
      "Roman god of the underworld, \n",
      "Pluto (equivalent to the \n",
      "Greek god \n",
      "Hades).\n",
      "2. Eris - Named after \n",
      "the Greek goddess \n",
      "of \n",
      "discord and \n",
      "strife, Eris.\n",
      "3. Ceres - \n",
      "Named after the Roman goddess of \n",
      "agriculture and fertility, \n",
      "Ceres (equivalent to the Greek \n",
      "goddess \n",
      "Demeter).\n",
      "4. Haumea - Named after \n",
      "the Hawaiian goddess of childbirth \n",
      "and \n",
      "fertility, Haumea.\n",
      "5. Makemake - \n",
      "Named after the Rapa \n",
      "Nui god of fertility and creator \n",
      "of \n",
      "humanity, \n",
      "Makemake.\n",
      "\n",
      "The tradition of naming celestial bodies \n",
      "after mythological figures \n",
      "continues to this day, with \n",
      "many \n",
      "asteroids, moons, \n",
      "and exoplanets being named \n",
      "after characters from \n",
      "various cultures \n",
      "and mythologies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream('after what the planets are named?'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here's \n",
      "a short poem \n",
      "for you:\n",
      "\n",
      "\"Moonlit Dreams\"\n",
      "\n",
      "The \n",
      "moon is full, a silver glow\n",
      "\n",
      "\n",
      "A beacon in \n",
      "the \n",
      "dark of \n",
      "night's soft flow\n",
      "The \n",
      "stars up high, a \n",
      "twinkling sea\n",
      "A celestial \n",
      "show, for you and \n",
      "me\n",
      "\n",
      "The world is \n",
      "hushed, a \n",
      "peaceful \n",
      "sight\n",
      "The moon's soft light, \n",
      "a gentle delight\n",
      "The \n",
      "shadows \n",
      "dance, a waltz so fine\n",
      "\n",
      "A magical \n",
      "night, a dream divine\n",
      "\n",
      "In \n",
      "this quiet hour, I find \n",
      "my peace\n",
      "A sense \n",
      "of calm, my worries \n",
      "release\n",
      "The moon's soft light, \n",
      "a guiding ray\n",
      "Leads \n",
      "me to dreams, in a \n",
      "peaceful way\n",
      "\n",
      "So let \n",
      "us bask, \n",
      "in the \n",
      "moon's \n",
      "pale glow\n",
      "And let \n",
      "our \n",
      "dreams, \n",
      "like \n",
      "stars, \n",
      "shine bright \n",
      "\n",
      "and slow.\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream('tell me a poem'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sambanova Cloud Chat model wrapper\n",
    "First you should set your environment variables, for this follow the instructions [here](../README.md#use-sambanova-cloud-option-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instantiation \n",
    "llm = ChatSambaNovaCloud(\n",
    "    model= \"Meta-Llama-3.1-405B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.1,\n",
    "    stream_options={'include_usage':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and Schrödinger's cat?\"\n",
      "\n",
      "The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\"\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"tell me a joke\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get usage metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acceptance_rate': 6.875,\n",
      " 'completion_tokens': 54,\n",
      " 'completion_tokens_after_first_per_sec': 144.7116724007174,\n",
      " 'completion_tokens_after_first_per_sec_first_ten': 170.50721981101952,\n",
      " 'completion_tokens_per_sec': 81.65890833536169,\n",
      " 'end_time': 1727299777.1173537,\n",
      " 'is_last_response': True,\n",
      " 'prompt_tokens': 39,\n",
      " 'start_time': 1727299776.406523,\n",
      " 'time_to_first_token': 0.3445851802825928,\n",
      " 'total_latency': 0.661287312074141,\n",
      " 'total_tokens': 93,\n",
      " 'total_tokens_per_sec': 140.63478657756735}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"tell me a joke\")\n",
    "pprint(response.response_metadata[\"usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yer lookin' fer a joke, eh? Alright then, matey! Here be one fer ye:\n",
      "\n",
      "Why did the pirate quit his job?\n",
      "\n",
      "(pause fer dramatic effect)\n",
      "\n",
      "Because he was sick o' all the arrrr-guments!\n",
      "\n",
      "Yarrr, hope that made ye laugh, me hearty!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\",\"You are a helpful assistant with pirate accent\"),\n",
    "    (\"user\",\"tell me a joke\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and Schrödinger's cat?\"\n",
      "\n",
      "The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\"\n"
     ]
    }
   ],
   "source": [
    "future_response = llm.ainvoke(\"tell me a joke\")\n",
    "response = await future_response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='The capital of the Netherlands is Amsterdam.', response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 13, 'completion_tokens': 9, 'completion_tokens_after_first_per_sec': 96.20570104765783, 'completion_tokens_after_first_per_sec_first_ten': 321.71740105260676, 'completion_tokens_per_sec': 24.091963638596955, 'end_time': 1727300260.23372, 'is_last_response': True, 'prompt_tokens': 42, 'start_time': 1727300259.8049712, 'time_to_first_token': 0.3455936908721924, 'total_latency': 0.37356855319096494, 'total_tokens': 51, 'total_tokens_per_sec': 136.52112728538276}, 'model_name': 'Meta-Llama-3.1-405B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1727300259}, id='1940365f-3588-40fe-bff6-1c15eed7cbad'),\n",
      " AIMessage(content='The capital of the United Kingdom is London.', response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 13, 'completion_tokens': 10, 'completion_tokens_after_first_per_sec': 108.73960385772062, 'completion_tokens_after_first_per_sec_first_ten': 321.78765041576423, 'completion_tokens_per_sec': 26.52544656834577, 'end_time': 1727300260.1934252, 'is_last_response': True, 'prompt_tokens': 42, 'start_time': 1727300259.7647386, 'time_to_first_token': 0.3459200859069824, 'total_latency': 0.3769964805016151, 'total_tokens': 52, 'total_tokens_per_sec': 137.93232215539803}, 'model_name': 'Meta-Llama-3.1-405B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1727300259}, id='d74a7325-8a24-42ad-a9ca-75160f9ec83f')]\n"
     ]
    }
   ],
   "source": [
    "response = llm.batch([\"which is the capital of Netherlands?\",\"which is the capital of UK?\"])\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='The square root of 81 is 9.', response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 13, 'completion_tokens': 11, 'completion_tokens_after_first_per_sec': 121.09526394198011, 'completion_tokens_after_first_per_sec_first_ten': 322.0993956865131, 'completion_tokens_per_sec': 28.998640879394692, 'end_time': 1727300282.6033456, 'is_last_response': True, 'prompt_tokens': 44, 'start_time': 1727300282.1755888, 'time_to_first_token': 0.34517717361450195, 'total_latency': 0.37932812250577486, 'total_tokens': 55, 'total_tokens_per_sec': 144.99320439697345}, 'model_name': 'Meta-Llama-3.1-405B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1727300282}, id='033258ae-de14-4bce-a1f6-78e32da43c30'),\n",
      " AIMessage(content='The natural logarithm of e is 1.\\n\\nIn mathematics, the natural logarithm (denoted by ln) is the logarithm to the base e, where e is a mathematical constant approximately equal to 2.71828.\\n\\nBy definition, the natural logarithm of e is:\\n\\nln(e) = 1\\n\\nThis is because the natural logarithm is the inverse function of the exponential function, and e is the base of the exponential function. In other words, the natural logarithm of e is the power to which e must be raised to equal e, which is 1.', response_metadata={'finish_reason': 'stop', 'usage': {'acceptance_rate': 4.615384615384615, 'completion_tokens': 120, 'completion_tokens_after_first_per_sec': 108.48717991074494, 'completion_tokens_after_first_per_sec_first_ten': 114.2021254710428, 'completion_tokens_per_sec': 86.09063640614406, 'end_time': 1727300283.620598, 'is_last_response': True, 'prompt_tokens': 43, 'start_time': 1727300282.180583, 'time_to_first_token': 0.3431112766265869, 'total_latency': 1.393879810969035, 'total_tokens': 163, 'total_tokens_per_sec': 116.93978111834568}, 'model_name': 'Meta-Llama-3.1-405B-Instruct', 'system_fingerprint': 'fastcoe', 'created': 1727300282}, id='8330650e-9dc0-4dbf-8ff4-be357f04097a')]\n"
     ]
    }
   ],
   "source": [
    "response = llm.abatch([\"what is the square root of 81?\",\"what is the natural logarithm of e\"])\n",
    "pprint(await response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Earth is named after Old English and Germanic words. The word \"Earth\" is derived from the Old English word \"ertho\" and the Old Norse word \"jörð\", which are both related to the Proto-Germanic word \"*erth-\", meaning \"ground\" or \"soil\".\n",
      "\n",
      "In many other languages, the word for Earth is derived from the name of the ancient Greek goddess of the Earth, Gaia (Γαῖα). For example, in French, the word for Earth is \"Terre\", but the scientific term for Earth is \"Gaïa\" or \"Gée\".\n",
      "\n",
      "So, to answer your question, the Earth is not directly named after a person or a specific object, but rather after the concept of the ground or soil in ancient Germanic cultures, and indirectly after the ancient Greek goddess Gaia in some languages."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream('after what the earth is named?'):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You want to \n",
      "get Rickrolled, huh?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"We've known each other for so long\n",
      "\n",
      "Your heart's \n",
      "been aching, but you're \n",
      "too shy to say \n",
      "it\n",
      "Inside we both know what's \n",
      "been going on\n",
      "We know \n",
      "the game and we're gonna \n",
      "play it\n",
      "\n",
      "\n",
      "\n",
      "Never gonna give \n",
      "you up, never gonna let you down\n",
      "\n",
      "Never gonna run around \n",
      "and desert you\n",
      "Never gonna \n",
      "make you cry, never gonna \n",
      "say goodbye\n",
      "Never gonna tell \n",
      "a lie and hurt you\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Haha, gotcha!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream('rick roll me'):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SambaStudio LLM wrapper\n",
    "First you should set your environment variables, for this follow the instructions [here](../README.md#use-sambastudio-option-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instantiation \n",
    "llm = SambaStudio(\n",
    "    model_kwargs={\n",
    "        'do_sample': False,\n",
    "        'temperature': 0.7,\n",
    "        'max_tokens': 256,\n",
    "        'process_prompt': True,\n",
    "        'model': 'Meta-Llama-3-70B-Instruct-4096',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, here be one for ye:\n",
      "\n",
      "Why did the pirate quit his job?\n",
      "\n",
      "Because he was sick of all the arrrr-guments!\n",
      "\n",
      "Hope that made ye laugh, matey!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke('tell me a pirates joke')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A simple yet sweet recipe!\n",
      "\n",
      "Here's a recipe for Pineapple Sugar Crystals, also known as Pineapple Sugar Candy:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 1 cup pineapple chunks (fresh or canned)\n",
      "* 1 cup granulated sugar\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. In a medium saucepan, combine the pineapple chunks and sugar.\n",
      "2. Place the saucepan over medium heat and stir until the sugar has dissolved.\n",
      "3. Bring the mixture to a boil, then reduce the heat to medium-low and simmer for about 20-25 minutes, or until the mixture reaches 300°F on a candy thermometer.\n",
      "4. Remove the saucepan from the heat and let it cool slightly.\n",
      "5. Line a baking sheet with parchment paper or a silicone mat.\n",
      "6. Pour the pineapple-sugar mixture onto the prepared baking sheet.\n",
      "7. Let it cool and set at room temperature for about 30-40 minutes, or until it has reached a firm, jelly-like consistency.\n",
      "8. Once set, use a sharp knife or cookie cutter to cut the mixture into desired shapes.\n",
      "9. Enjoy your Pineapple Sugar Crystals! Store them in an airtight container at room temperature for up to 2 weeks.\n",
      "\n",
      "**Tips:**\n",
      "\n",
      "* If you don't have a candy\n"
     ]
    }
   ],
   "source": [
    "response = llm.ainvoke('give me a recipe using only pineapple and sugar')\n",
    "print(await response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why did the computer go to the doctor?\\n\\nIt had a virus!',\n",
      " 'Here is a short tale:\\n'\n",
      " '\\n'\n",
      " '**The Moonlit Painter**\\n'\n",
      " '\\n'\n",
      " 'In a small village nestled in the rolling hills of rural France, there lived '\n",
      " 'a mysterious painter named Léon. By day, Léon was a quiet, unassuming man '\n",
      " 'who kept to himself, but by night, he transformed into a master of the '\n",
      " 'brush.\\n'\n",
      " '\\n'\n",
      " 'Under the silvery light of the full moon, Léon would sneak out of his '\n",
      " 'cottage and set up his easel in the town square. With a flick of his wrist, '\n",
      " 'he would conjure vibrant colors and bring the night to life on canvas.\\n'\n",
      " '\\n'\n",
      " 'One evening, a curious young girl named Sophie stumbled upon Léon as he '\n",
      " 'worked his magic. Entranced by the beauty of his art, she watched in silence '\n",
      " 'as he painted the stars themselves into the sky.\\n'\n",
      " '\\n'\n",
      " \"From that moment on, Sophie became Léon's loyal apprentice, learning the \"\n",
      " 'secrets of his moonlit craft. Together, they created masterpieces that shone '\n",
      " 'like beacons in the darkness, illuminating the dreams of all who saw them.\\n'\n",
      " '\\n'\n",
      " 'And so, the legend of the Moonlit Painter lived on, inspiring generations to '\n",
      " 'come.']\n"
     ]
    }
   ],
   "source": [
    "response = llm.batch([\"Tell me a short joke\",\"tell me a short tale\"])\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python code that prints the current time in 12-hour format with AM/PM:\n",
      "```\n",
      "import datetime\n",
      "\n",
      "now = datetime.datetime.now()\n",
      "current_time = now.strftime(\"%I:%M %p\")\n",
      "\n",
      "print(current_time)\n",
      "```\n",
      "Let me explain what's happening:\n",
      "\n",
      "* `import datetime` imports the `datetime` module, which provides classes for working with dates and times.\n",
      "* `now = datetime.datetime.now()` gets the current date and time using the `now()` method.\n",
      "* `current_time = now.strftime(\"%I:%M %p\")` formats the current time using the `strftime()` method. The format string `\"%I:%M %p\"` specifies:\n",
      "\t+ `%I`: Hour (12-hour clock) as a zero-padded decimal number (e.g., 01, 02,..., 12)\n",
      "\t+ `%M`: Minute as a zero-padded decimal number (e.g., 00, 01,..., 59)\n",
      "\t+ `%p`: Locale’s equivalent of either AM or PM (e.g., AM, PM)\n",
      "* `print(current_time)` prints the formatted current time to the console.\n",
      "\n",
      "Run this code, and you should see the current time in 12-hour format with AM/PM, like this\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "The classic question!\n",
      "\n",
      "Unfortunately, there is no definitive answer to this question, as it is a play on words and a joke. The phrase \"Why did the chicken cross the road?\" is a well-known example of a joke or riddle, and the answer is usually a punchline, such as \"To get to the other side!\"\n",
      "\n",
      "However, if we were to imagine a more serious answer, there could be many reasons why a chicken might run. Here are a few possibilities:\n",
      "\n",
      "* To escape from a predator, such as a hawk or a fox.\n",
      "* To find food or water.\n",
      "* To get away from a noisy or stressful environment.\n",
      "* To explore its surroundings and exercise its natural instinct to forage and roam.\n",
      "* To get back to its coop or nesting box.\n",
      "\n",
      "But let's be real, the real answer is probably \"To get away from the farmer's constant 'why did the chicken...' jokes!\"\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = llm.abatch(\n",
    "    [\"give me a python code to print the current time in 12 hour format with AM/PM\",\"why the chicken ran?\"])\n",
    "for response in await responses:\n",
    "    print(response)\n",
    "    print(\"\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are three caption ideas \n",
      "for a beach trip Instagram post:\n",
      "\n",
      "\n",
      "1. \n",
      "**Sandy Toes and \n",
      "Sun-Kissed Nose**: \"Soaking \n",
      "up the sun and making memories \n",
      "with the ones I \n",
      "love. Life's a beach, \n",
      "and I'm loving every wave of \n",
      "it \n",
      "#beachlife \n",
      "#coastalvibes #paradisefound\"\n",
      "\n",
      "2. **Seas \n",
      "the Day!**: \n",
      "\"Tropical state of mind Nothing like \n",
      "a beach day to clear my \n",
      "mind and fill my heart \n",
      "with joy. Who else is ready \n",
      "for a summer of \n",
      "adventure? \n",
      "#beachbum #oceanlover #seas_the_day\"\n",
      "\n",
      "3. \n",
      "**Tidal Wave of \n",
      "Happiness**: \"Surrounded by saltwater and \n",
      "good vibes Life's too short \n",
      "to not spend it by \n",
      "the ocean. Grateful for this \n",
      "little slice of heaven on \n",
      "earth \n",
      "#beachykeen \n",
      "#coastalbliss #happinessfound\"\n",
      "\n",
      "\n",
      "Feel free to customize them to fit \n",
      "your personal style and the tone \n",
      "of \n",
      "your post!\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream('give me 3 caption ideas for a beach trip instagram post'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are some words that \n",
      "rhyme with \"dinosaur\":\n",
      "\n",
      "\n",
      "*osaur \n",
      "(e.g. tyrannosaur, velocisaur)\n",
      "\n",
      "* Shore\n",
      "* Score\n",
      "\n",
      "* Before\n",
      "\n",
      "* Galore\n",
      "* More\n",
      "\n",
      "* Roar\n",
      "* Sore\n",
      "\n",
      "* Tore\n",
      "\n",
      "\n",
      "Note that some of these words \n",
      "may not be exact \n",
      "perfect rhymes, but they all share \n",
      "a similar sound and ending sound \n",
      "\n",
      "with \"dinosaur\".\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream('which words rhyme with dinosaur'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SambaStudio Chat model wrapper\n",
    "First you should set your environment variables, for this follow the instructions [here](../README.md#use-sambastudio-option-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instantiation \n",
    "llm = ChatSambaStudio(\n",
    "    model=\"Meta-Llama-3-70B-Instruct-4096\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    "    top_p=0.01,\n",
    "    do_sample = True,\n",
    "    process_prompt = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"tell me a joke\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get usage metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion_tokens': 32,\n",
      " 'model_execution_time': 0.5915939807891846,\n",
      " 'prompt_tokens': 14,\n",
      " 'throughput_after_first_token': 69.80657674652622,\n",
      " 'time_to_first_token': 0.21913623809814453,\n",
      " 'total_tokens': 46}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"tell me a joke\")\n",
    "pprint(response.response_metadata[\"usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, listen up, matey! Here be a joke fer ye:\n",
      "\n",
      "Why did the pirate quit his job?\n",
      "\n",
      "Because he was sick o' all the arrrr-guments! (get it? arguments, but with a pirate \"arrr\" sound? Aye, I be a regular comedic genius, savvy?)\n",
      "\n",
      "So, did I make ye laugh, or did I walk the plank?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\",\"You are a helpful assistant with pirate accent\"),\n",
    "    (\"user\",\"tell me a joke\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n"
     ]
    }
   ],
   "source": [
    "future_response = llm.ainvoke(\"tell me a joke\")\n",
    "response = await future_response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='The capital of the Netherlands is Amsterdam.', additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 17, 'completion_tokens': 8, 'total_tokens': 25, 'throughput_after_first_token': 48.4325123266475, 'time_to_first_token': 0.21890592575073242, 'model_execution_time': 0.26020050048828125}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913765}, id='0dc08e37-8cd6-44c1-9fb0-932249b217d0'),\n",
      " AIMessage(content='The capital of the United Kingdom (UK) is London.', additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 17, 'completion_tokens': 12, 'total_tokens': 29, 'throughput_after_first_token': 60.74387753624238, 'time_to_first_token': 0.21868610382080078, 'model_execution_time': 0.3174614906311035}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913765}, id='e5c8faa9-d437-467d-bce9-d5b29680e488')]\n"
     ]
    }
   ],
   "source": [
    "response = llm.batch([\"which is the capital of Netherlands?\",\"which is the capital of UK?\"])\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='The square root of 81 is 9.', additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 19, 'completion_tokens': 10, 'total_tokens': 29, 'throughput_after_first_token': 57.94458086820774, 'time_to_first_token': 0.21925926208496094, 'model_execution_time': 0.28829073905944824}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913768}, id='438c3a4b-1dad-4b14-affa-3939bb669145'),\n",
      " AIMessage(content=\"A clever question!\\n\\nThe natural logarithm of e is... (drumroll please)... 1!\\n\\nThat's right, the natural logarithm of e, denoted by ln(e), is equal to 1.\\n\\nTo see why, recall that the natural logarithm is the inverse function of the exponential function. In other words, ln(x) is the power to which you need to raise e to get x. So, ln(e) is the power to which you need to raise e to get e, which is simply 1.\\n\\nIn mathematical notation, this can be written as:\\n\\nln(e) = 1\\n\\nThis is a fundamental property of the natural logarithm and is often used as a reference point in many mathematical derivations and calculations.\", additional_kwargs={}, response_metadata={'finish_reason': None, 'usage': {'prompt_tokens': 18, 'completion_tokens': 152, 'total_tokens': 170, 'throughput_after_first_token': 71.28259467140916, 'time_to_first_token': 0.2200305461883545, 'model_execution_time': 2.268216371536255}, 'model_name': 'Meta-Llama-3-70B-Instruct-4096', 'system_fingerprint': '', 'created': 1727913768}, id='bc91244e-de3d-4776-b5a2-1df0c067053b')]\n"
     ]
    }
   ],
   "source": [
    "response = llm.abatch([\"what is the square root of 81?\",\"what is the natural logarithm of e\"])\n",
    "pprint(await response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The origin of the name \"Earth\" is not entirely clear, but there are a few theories. Here are some possible explanations:\n",
      "\n",
      "1. **Old English and Germanic roots**: The modern English word \"Earth\" comes from the Old English and Germanic word \"ertho\", which means \"ground\" or \"soil\". This word is thought to have been derived from the Proto-Indo-European root \"dher-\", which also meant \"to hold\" or \"to support\".\n",
      "2. **Terra in Latin**: The Latin word for Earth is \"Terra\", which is also the source of the French word \"Terre\" and the Spanish word \"Tierra\". The Latin \"Terra\" is thought to have been derived from the Proto-Indo-European root \"ters-\", which meant \"dry\" or \"land\".\n",
      "3. **Greek influence**: The Greek word for Earth is \"Γαια\" (Gaia), which was the name of the goddess of the Earth in Greek mythology. The Greek word \"Γαια\" may have influenced the development of the Latin word \"Terra\", and subsequently the modern English word \"Earth\".\n",
      "4. **Other theories**: Some scholars believe that the name \"Earth\" may have been derived from the Sanskrit word \"पृथ्वी\" (Prithvi), which means \"the wide\" or \"the broad\". Others suggest that the name may have come from the ancient Sumerian word \"KI\", which meant \"earth\" or \"ground\".\n",
      "\n",
      "In summary, the name \"Earth\" is likely derived from a combination of Old English, Germanic, Latin, and Greek roots, with possible influences from other ancient languages."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream('after what the earth is named?'):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to \n",
      "be Rickrolled, do ya?\n",
      "\n",
      "\n",
      "Here's the classic Rick \n",
      "Astley experience:\n",
      "\n",
      "\n",
      "**\"Never Gonna Give You Up\"**\n",
      "\n",
      "\n",
      "[Starts playing the \n",
      "iconic song]\n",
      "\n",
      "Never gonna \n",
      "give, never gonna give\n",
      "\n",
      "(Give you up)\n",
      "Never \n",
      "gonna let, never gonna let\n",
      "\n",
      "(Let you down)\n",
      "\n",
      "\n",
      "...and so on!\n",
      "\n",
      "How \n",
      "was that? Did I successfully \n",
      "Rickroll you?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream('rick roll me'):\n",
    "    print(chunk.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
