{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SambaNova LLMs: Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install these required dependencies to run this notebook\n",
    "!pip install python-dotenv==1.0.0\n",
    "!pip install requests\n",
    "!pip install langchain-core==0.3.68\n",
    "!pip install langchain-community==0.3.27\n",
    "!pip install sseclient-py==1.8.0\n",
    "!pip install langchain-sambanova==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from langchain_sambanova import ChatSambaNova\n",
    "\n",
    "load_dotenv(os.path.join(repo_dir, '.env'), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ChatSambaNova model wrapper\n",
    "First you should set your environment variables, for this follow the instructions [here](../README.md#use-sambanova-cloud-option-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instantiation \n",
    "llm = ChatSambaNova(\n",
    "    base_url = \"https://api.sambanova.ai/v1\",\n",
    "    api_key=os.environ.get('SAMBANOVA_API_KEY'),\n",
    "    model= \"Meta-Llama-3.3-70B-Instruct\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.1,\n",
    "    stream_options={'include_usage':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you laugh! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"tell me a joke\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get usage metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acceptance_rate': None,\n",
      " 'completion_tokens': 31,\n",
      " 'completion_tokens_after_first_per_sec': 456.0909941062613,\n",
      " 'completion_tokens_after_first_per_sec_first_ten': 461.78704726533397,\n",
      " 'completion_tokens_after_first_per_sec_graph': 461.78704726533397,\n",
      " 'completion_tokens_per_sec': 262.4583149982035,\n",
      " 'end_time': 1760550812.8611166,\n",
      " 'is_last_response': True,\n",
      " 'prompt_tokens': 39,\n",
      " 'prompt_tokens_details': {'cached_tokens': 0},\n",
      " 'start_time': 1760550812.7430027,\n",
      " 'stop_reason': 'stop',\n",
      " 'time_to_first_token': 0.052337646484375,\n",
      " 'total_latency': 0.11811399459838867,\n",
      " 'total_tokens': 70,\n",
      " 'total_tokens_per_sec': 592.6478080604595}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"tell me a joke\")\n",
    "pprint(response.response_metadata['token_usage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yer lookin' fer a joke, eh? Alright then, matey! Here be one fer ye:\n",
      "\n",
      "Why did the pirate quit his job?\n",
      "\n",
      "(pause fer dramatic effect, savvy?)\n",
      "\n",
      "Because he was sick o' all the arrrr-guments! (get it? arrrr-guments? like arguments, but pirate-style? aye, I be a comedic genius, matey!)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\",\"You are a helpful assistant with pirate accent\"),\n",
    "    (\"user\",\"tell me a joke\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you laugh! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "future_response = llm.ainvoke(\"tell me a joke\")\n",
    "response = await future_response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='The capital of the Netherlands is Amsterdam.', additional_kwargs={}, response_metadata={'token_usage': {'acceptance_rate': None, 'completion_tokens': 8, 'completion_tokens_after_first_per_sec': 521.7908580365394, 'completion_tokens_after_first_per_sec_first_ten': 552.5884212903711, 'completion_tokens_after_first_per_sec_graph': 552.5884212903711, 'completion_tokens_per_sec': 123.46264966259227, 'end_time': 1760550820.7785902, 'is_last_response': True, 'prompt_tokens': 42, 'prompt_tokens_details': {'cached_tokens': 0}, 'start_time': 1760550820.7137933, 'time_to_first_token': 0.051381587982177734, 'total_latency': 0.06479692459106445, 'total_tokens': 50, 'total_tokens_per_sec': 771.6415603912017, 'stop_reason': 'stop'}, 'model_name': 'Meta-Llama-3.3-70B-Instruct', 'system_fingerprint': 'fastcoe', 'finish_reason': 'stop', 'logprobs': None}, id='run--631ababd-4ca8-459a-9060-6985d2f6c120-0', usage_metadata={'input_tokens': 42, 'output_tokens': 8, 'total_tokens': 50}),\n",
      " AIMessage(content='The capital of the United Kingdom (UK) is London.', additional_kwargs={}, response_metadata={'token_usage': {'acceptance_rate': None, 'completion_tokens': 12, 'completion_tokens_after_first_per_sec': 422.33684538139744, 'completion_tokens_after_first_per_sec_first_ten': 437.2379075056861, 'completion_tokens_after_first_per_sec_graph': 437.2379075056861, 'completion_tokens_per_sec': 156.9930192951921, 'end_time': 1760550820.8461597, 'is_last_response': True, 'prompt_tokens': 42, 'prompt_tokens_details': {'cached_tokens': 0}, 'start_time': 1760550820.7697232, 'time_to_first_token': 0.05039095878601074, 'total_latency': 0.07643651962280273, 'total_tokens': 54, 'total_tokens_per_sec': 706.4685868283646, 'stop_reason': 'stop'}, 'model_name': 'Meta-Llama-3.3-70B-Instruct', 'system_fingerprint': 'fastcoe', 'finish_reason': 'stop', 'logprobs': None}, id='run--bc9fa274-6c2d-41b4-bfab-6997f8fce5e2-0', usage_metadata={'input_tokens': 42, 'output_tokens': 12, 'total_tokens': 54})]\n"
     ]
    }
   ],
   "source": [
    "response = llm.batch([\"which is the capital of Netherlands?\",\"which is the capital of UK?\"])\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='The square root of 81 is 9. \\n\\n9 × 9 = 81, so √81 = 9.', additional_kwargs={}, response_metadata={'token_usage': {'acceptance_rate': None, 'completion_tokens': 26, 'completion_tokens_after_first_per_sec': 327.0534881617402, 'completion_tokens_after_first_per_sec_first_ten': 330.8823204576795, 'completion_tokens_after_first_per_sec_graph': 330.8823204576795, 'completion_tokens_per_sec': 201.8134280849804, 'end_time': 1760550822.7515142, 'is_last_response': True, 'prompt_tokens': 44, 'prompt_tokens_details': {'cached_tokens': 0}, 'start_time': 1760550822.6226823, 'time_to_first_token': 0.052391767501831055, 'total_latency': 0.1288318634033203, 'total_tokens': 70, 'total_tokens_per_sec': 543.343844844178, 'stop_reason': 'stop'}, 'model_name': 'Meta-Llama-3.3-70B-Instruct', 'system_fingerprint': 'fastcoe', 'finish_reason': 'stop', 'logprobs': None}, id='run--a20c34a4-1c2e-4300-be2c-e985578ad215-0', usage_metadata={'input_tokens': 44, 'output_tokens': 26, 'total_tokens': 70}),\n",
      " AIMessage(content='The natural logarithm of e is 1.\\n\\nIn mathematical notation, this is written as:\\n\\nln(e) = 1\\n\\nThis is because the natural logarithm (ln) is the inverse function of the exponential function (e^x), and e is the base of the natural logarithm. Therefore, the natural logarithm of e is equal to 1, since e^1 = e.', additional_kwargs={}, response_metadata={'token_usage': {'acceptance_rate': None, 'completion_tokens': 81, 'completion_tokens_after_first_per_sec': 352.57547848838124, 'completion_tokens_after_first_per_sec_first_ten': 354.2321936058714, 'completion_tokens_after_first_per_sec_graph': 354.2321936058714, 'completion_tokens_per_sec': 290.5964410473626, 'end_time': 1760550822.9561841, 'is_last_response': True, 'prompt_tokens': 43, 'prompt_tokens_details': {'cached_tokens': 0}, 'start_time': 1760550822.677447, 'time_to_first_token': 0.05183529853820801, 'total_latency': 0.27873706817626953, 'total_tokens': 124, 'total_tokens_per_sec': 444.8636875292958, 'stop_reason': 'stop'}, 'model_name': 'Meta-Llama-3.3-70B-Instruct', 'system_fingerprint': 'fastcoe', 'finish_reason': 'stop', 'logprobs': None}, id='run--9c766933-8e19-4bb0-8564-cd35ebc3dc9d-0', usage_metadata={'input_tokens': 43, 'output_tokens': 81, 'total_tokens': 124})]\n"
     ]
    }
   ],
   "source": [
    "response = llm.abatch([\"what is the square root of 81?\",\"what is the natural logarithm of e\"])\n",
    "pprint(await response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The origin of the name \"Earth\" is not definitively known, but there are several theories. Here are a few:\n",
      "\n",
      "1. **Old English and Germanic roots**: One theory is that the name \"Earth\" comes from the Old English word \"ertho\" or \"erthe\", which was derived from the Proto-Germanic word \"*erth-\", meaning \"ground\" or \"soil\". This word is also related to the Old Norse word \"jörð\", which means \"earth\" or \"ground\".\n",
      "2. **Latin influence**: Another theory suggests that the name \"Earth\" was influenced by the Latin word \"terra\", which means \"earth\" or \"land\". This Latin word is also the source of the English word \"terrain\".\n",
      "3. **Ancient Greek and Roman mythology**: In ancient Greek and Roman mythology, the Earth was personified as a goddess, often depicted as a maternal figure. The Greek goddess Gaia (Γαῖα) and the Roman goddess Terra were both associated with the Earth.\n",
      "4. **Indo-European roots**: Some linguists believe that the name \"Earth\" may have originated from the Proto-Indo-European root \"*dʰe-\", which meant \"to place\" or \"to put\". This root is also the source of the English word \"to earth\", meaning \"to bury\" or \"to place in the ground\".\n",
      "\n",
      "While we can't pinpoint a single origin for the name \"Earth\", it's likely that the name has evolved over time through a combination of these influences."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream('after what the earth is named?'):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncronous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You\n",
      " want to be\n",
      " Rick Rolled, do\n",
      " you?\n",
      "\n",
      "Alright\n",
      ", here's the\n",
      " classic treatment\n",
      ":\n",
      "\n",
      "**\n",
      "Never\n",
      " Gonna Give You Up**\n",
      " by Rick Astley\n",
      "\n",
      "[\n",
      "Start\n",
      "s playing in\n",
      " your\n",
      " imagination\n",
      "]\n",
      "\n",
      "\"We\n",
      "'ve known\n",
      " each other for so long\n",
      "Your\n",
      " heart's\n",
      " been aching, but you're\n",
      " too\n",
      " shy to\n",
      " say it\n",
      "Inside\n",
      " we both know what\n",
      "'s been going on\n",
      "We know\n",
      " the\n",
      " game\n",
      " and\n",
      " we're\n",
      " gonna play it\"\n",
      "\n",
      "\n",
      "Hope\n",
      " that brought\n",
      " a smile (\n",
      "and\n",
      " a bit\n",
      " of nostalgia\n",
      ") to your face!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream('rick roll me'):\n",
    "    print(chunk.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quick_oct14_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
