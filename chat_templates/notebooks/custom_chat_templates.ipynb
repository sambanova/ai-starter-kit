{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffac235",
   "metadata": {},
   "source": [
    "# Custom Chat Templates with SambaNova API\n",
    "\n",
    "This notebook walks through the complete workflow of how modern chat models format, send, and interpret conversations. From Jinja chat templates to Completions API invocation and tool-call parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89886c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f0a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgep/Documents/ask_public_own/chat_templete_kit/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from sambanova import SambaNova\n",
    "from transformers import AutoTokenizer\n",
    "from jinja2 import Template, Environment, TemplateSyntaxError\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir =  os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(repo_dir, '.env'), override=True)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e0dbb",
   "metadata": {},
   "source": [
    "## Get Chat Template\n",
    "\n",
    "Load the tokenizer’s built-in chat template from a Hugging Face model or define a custom Jinja template manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05008b6d",
   "metadata": {},
   "source": [
    "### From HugginFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3833421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name: str, hf_token: str | None = None, cache_dir: str = \"../data\"):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token, cache_dir=cache_dir)\n",
    "    print(f\"Tokenizer loaded: {model_name}\")\n",
    "    return tok\n",
    "\n",
    "def get_chat_template(tokenizer):\n",
    "    tpl = getattr(tokenizer, \"chat_template\", None)\n",
    "    if not tpl:\n",
    "        raise ValueError(\"Model has no chat_template\")\n",
    "    print(\"chat template retrieved\")\n",
    "    return tpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5577f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgep/Documents/ask_public_own/chat_templete_kit/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1025: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: meta-llama/Llama-3.1-8B-Instruct\n",
      "chat template retrieved\n",
      "\n",
      "--- Chat template (truncated) ---\n",
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hf_model = \"meta-llama/Llama-3.1-8B-Instruct\"  # example model\n",
    "hf_tokenizer = load_tokenizer(hf_model, os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "hf_chat_template = get_chat_template(hf_tokenizer)\n",
    "\n",
    "print(\"\\n--- Chat template (truncated) ---\")\n",
    "print(hf_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe35f4",
   "metadata": {},
   "source": [
    "### Create Custom Jinja template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4532d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_jinja(template_str: str):\n",
    "    try:\n",
    "        Environment().parse(template_str)\n",
    "        print(\"Jinja template valid\")\n",
    "        return True\n",
    "    except TemplateSyntaxError as e:\n",
    "        raise ValueError(f\"Invalid Jinja syntax at line {e.lineno}: {e.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384333c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jinja template valid\n",
      "\n",
      "--- Custom chat template ---\n",
      "\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}\n",
      "{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}\n",
      "\n",
      "{# Collect system messages #}\n",
      "{%- for message in messages %}\n",
      "    {%- if message['role'] == 'system' %}\n",
      "        {%- if ns.is_first_sp %}\n",
      "            {% set ns.system_prompt = ns.system_prompt + message['content'] %}\n",
      "            {% set ns.is_first_sp = false %}\n",
      "        {%- else %}\n",
      "            {% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{{ bos_token }}{{ ns.system_prompt }}{{- \"\n",
      "Today Date: \" + date_string + \"\n",
      "\n",
      "\" }}\n",
      "\n",
      "{# Iterate through user/assistant/tool messages #}\n",
      "{%- for message in messages %}\n",
      "    {%- if message['role'] == 'user' %}\n",
      "        {% set ns.is_tool = false -%}\n",
      "        {{ '<｜User｜>' + message['content'] }}\n",
      "    {%- endif %}\n",
      "\n",
      "    {%- if message['role'] == 'assistant' and message['content'] is none %}\n",
      "        {% set ns.is_tool = false -%}\n",
      "        {%- for tool in message['tool_calls'] %}\n",
      "            {%- if not ns.is_first %}\n",
      "                {{ '<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n```json\\n' + tool['function']['arguments'] + '\\n```<｜tool▁call▁end｜>' }}\n",
      "                {% set ns.is_first = true -%}\n",
      "            {%- else %}\n",
      "                {{ '\\n<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n```json\\n' + tool['function']['arguments'] + '\\n```<｜tool▁call▁end｜>' }}\n",
      "                {{ '<｜tool▁calls▁end｜><｜end▁of▁sentence｜>' }}\n",
      "            {%- endif %}\n",
      "        {%- endfor %}\n",
      "    {%- endif %}\n",
      "\n",
      "    {%- if message['role'] == 'assistant' and message['content'] is not none %}\n",
      "        {%- if ns.is_tool %}\n",
      "            {{ '<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>' }}\n",
      "            {% set ns.is_tool = false -%}\n",
      "        {%- else %}\n",
      "            {{ '<｜Assistant｜>' + message['content'] + '<｜end▁of▁sentence｜>' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "\n",
      "    {%- if message['role'] == 'tool' %}\n",
      "        {% set ns.is_tool = true -%}\n",
      "        {%- if ns.is_output_first %}\n",
      "            {{ '<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>' }}\n",
      "            {% set ns.is_output_first = false %}\n",
      "        {%- else %}\n",
      "            {{ '\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "{% if ns.is_tool %}{{ '<｜tool▁outputs▁end｜>' }}{% endif %}\n",
      "{% if add_generation_prompt and not ns.is_tool %}{{ '<｜Assistant｜>' }}{% endif %}\n",
      "\n",
      "{# ---- Append tool metadata if tools exist ---- #}\n",
      "{% if tools and tools | length > 0 %}\n",
      "<｜system▁tools｜>\n",
      "Below are available tools for this task. Use them when relevant, wrapping each call in <｜tool▁call▁begin｜> ... <｜tool▁call▁end｜> XML markers, ensure using exact tool and parameter names.\n",
      "\n",
      "{%- for t in tools %}\n",
      "    {{- t | tojson(indent=4) }}\n",
      "    {{- \"\n",
      "\n",
      "\" }}\n",
      "{%- endfor %}\n",
      "\n",
      "Example tool usage output:\n",
      "<｜tool▁calls▁begin｜>\n",
      "<｜tool▁call▁begin｜>tool_1_name<｜tool▁sep｜>{\"tool_1_name_param_1_name\": \"tool_1_name_param_0_value\"}<｜tool▁call▁end｜>\n",
      "<｜tool▁call▁begin｜>tool_name_2<｜tool▁sep｜>{\"tool_2_name_param_1_name\": \"tool_2_name_param_1_value\", \"tool_2_name_param_2_name\"\": \"tool_2_name_param_2_value\"}<｜tool▁call▁end｜>\n",
      "<｜tool▁calls▁end｜>\n",
      "{% endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# custom deepseek v3 chat template with tool input and expected XML output format\n",
    "custom_chat_template = \"\"\"\n",
    "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}\n",
    "{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}\n",
    "\n",
    "{# Collect system messages #}\n",
    "{%- for message in messages %}\n",
    "    {%- if message['role'] == 'system' %}\n",
    "        {%- if ns.is_first_sp %}\n",
    "            {% set ns.system_prompt = ns.system_prompt + message['content'] %}\n",
    "            {% set ns.is_first_sp = false %}\n",
    "        {%- else %}\n",
    "            {% set ns.system_prompt = ns.system_prompt + '\\\\n\\\\n' + message['content'] %}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "\n",
    "{%- if not date_string is defined %}\n",
    "    {%- set date_string = \"26 Jul 2024\" %}\n",
    "{%- endif %}\n",
    "\n",
    "{{ bos_token }}{{ ns.system_prompt }}{{- \"\\nToday Date: \" + date_string + \"\\n\\n\" }}\n",
    "\n",
    "{# Iterate through user/assistant/tool messages #}\n",
    "{%- for message in messages %}\n",
    "    {%- if message['role'] == 'user' %}\n",
    "        {% set ns.is_tool = false -%}\n",
    "        {{ '<｜User｜>' + message['content'] }}\n",
    "    {%- endif %}\n",
    "\n",
    "    {%- if message['role'] == 'assistant' and message['content'] is none %}\n",
    "        {% set ns.is_tool = false -%}\n",
    "        {%- for tool in message['tool_calls'] %}\n",
    "            {%- if not ns.is_first %}\n",
    "                {{ '<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n```json\\\\n' + tool['function']['arguments'] + '\\\\n```<｜tool▁call▁end｜>' }}\n",
    "                {% set ns.is_first = true -%}\n",
    "            {%- else %}\n",
    "                {{ '\\\\n<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n```json\\\\n' + tool['function']['arguments'] + '\\\\n```<｜tool▁call▁end｜>' }}\n",
    "                {{ '<｜tool▁calls▁end｜><｜end▁of▁sentence｜>' }}\n",
    "            {%- endif %}\n",
    "        {%- endfor %}\n",
    "    {%- endif %}\n",
    "\n",
    "    {%- if message['role'] == 'assistant' and message['content'] is not none %}\n",
    "        {%- if ns.is_tool %}\n",
    "            {{ '<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>' }}\n",
    "            {% set ns.is_tool = false -%}\n",
    "        {%- else %}\n",
    "            {{ '<｜Assistant｜>' + message['content'] + '<｜end▁of▁sentence｜>' }}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "\n",
    "    {%- if message['role'] == 'tool' %}\n",
    "        {% set ns.is_tool = true -%}\n",
    "        {%- if ns.is_output_first %}\n",
    "            {{ '<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>' }}\n",
    "            {% set ns.is_output_first = false %}\n",
    "        {%- else %}\n",
    "            {{ '\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>' }}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "\n",
    "{% if ns.is_tool %}{{ '<｜tool▁outputs▁end｜>' }}{% endif %}\n",
    "{% if add_generation_prompt and not ns.is_tool %}{{ '<｜Assistant｜>' }}{% endif %}\n",
    "\n",
    "{# ---- Append tool metadata if tools exist ---- #}\n",
    "{% if tools and tools | length > 0 %}\n",
    "<｜system▁tools｜>\n",
    "Below are available tools for this task. Use them when relevant, wrapping each call in <｜tool▁call▁begin｜> ... <｜tool▁call▁end｜> XML markers, ensure using exact tool and parameter names.\n",
    "\n",
    "{%- for t in tools %}\n",
    "    {{- t | tojson(indent=4) }}\n",
    "    {{- \"\\n\\n\" }}\n",
    "{%- endfor %}\n",
    "\n",
    "Example tool usage output:\n",
    "<｜tool▁calls▁begin｜>\n",
    "<｜tool▁call▁begin｜>tool_1_name<｜tool▁sep｜>{\"tool_1_name_param_1_name\": \"tool_1_name_param_0_value\"}<｜tool▁call▁end｜>\n",
    "<｜tool▁call▁begin｜>tool_name_2<｜tool▁sep｜>{\"tool_2_name_param_1_name\": \"tool_2_name_param_1_value\", \"tool_2_name_param_2_name\"\": \"tool_2_name_param_2_value\"}<｜tool▁call▁end｜>\n",
    "<｜tool▁calls▁end｜>\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "validate_jinja(custom_chat_template)\n",
    "print(\"\\n--- Custom chat template ---\")\n",
    "print(custom_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596d371",
   "metadata": {},
   "source": [
    "## Apply chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc08aa0",
   "metadata": {},
   "source": [
    "When a Jinja chat template is rendered, variables like `messages`, `tools`, or `add_generation_prompt` are passed in a dictionary called the *context*.  \n",
    "It can also include tokens or special variables from the tokenizer (like `bos_token`, `eos_token`, etc.).  \n",
    "The template then uses this context to produce the final text string sent to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4faabdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_tokenizer_context(tokenizer):\n",
    "    \"\"\"\n",
    "    Collect all simple (JSON-serializable) attributes from the tokenizer\n",
    "    that may be referenced by the Jinja chat template.\n",
    "    \"\"\"\n",
    "    context = {}\n",
    "    for key, value in vars(tokenizer).items():\n",
    "        # Skip private/internal and complex objects\n",
    "        if key.startswith(\"_\"):\n",
    "            continue\n",
    "        if isinstance(value, (str, int, float, list, dict, tuple, type(None))):\n",
    "            context[key] = value\n",
    "    return context\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "    template_str: str, \n",
    "    messages: list, \n",
    "    tools: list | None = None,\n",
    "    add_generation_prompt: bool = True, \n",
    "    tokenizer=None,\n",
    "    extra_context: dict | None = None\n",
    "    ) -> str:\n",
    "    \"\"\"Render a Jinja2 chat template with messages and optional tools.\"\"\"\n",
    "    context = {\n",
    "        \"messages\": messages,\n",
    "        \"add_generation_prompt\": add_generation_prompt,\n",
    "    }\n",
    "    if tools is not None:\n",
    "        context[\"tools\"] = tools\n",
    "    if tokenizer is not None:\n",
    "        context.update(extract_tokenizer_context(tokenizer))\n",
    "    if extra_context:\n",
    "        context.update(extra_context)\n",
    "\n",
    "    try:\n",
    "        template = Template(template_str)\n",
    "        rendered = template.render(**context)\n",
    "        return rendered.strip()\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error rendering custom Jinja template: {e}\"\n",
    "        raise ValueError(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b08000de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example messages and tools\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What’s the weather in Paris?\"}\n",
    "]\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Retrieve weather data for a given city\",\n",
    "            \"parameters\": {\"city\": \"Paris\"}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c92de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Rendered prompt ---\n",
      "\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables.\n",
      "\n",
      "{\n",
      "    \"function\": {\n",
      "        \"description\": \"Retrieve weather data for a given city\",\n",
      "        \"name\": \"get_weather\",\n",
      "        \"parameters\": {\n",
      "            \"city\": \"Paris\"\n",
      "        }\n",
      "    },\n",
      "    \"type\": \"function\"\n",
      "}\n",
      "\n",
      "What’s the weather in Paris?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "#apply hf_chat_template \n",
    "rendered_prompt_hf_template = apply_chat_template(\n",
    "    hf_chat_template,\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    add_generation_prompt=True,\n",
    "    tokenizer=hf_tokenizer\n",
    ")\n",
    "\n",
    "print(\"\\n--- Rendered prompt ---\\n\")\n",
    "print(rendered_prompt_hf_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a5150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Rendered prompt ---\n",
      "\n",
      "<｜begin▁of▁sentence｜>You are a helpful assistant.\n",
      "Today Date: 10 Nov 2025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        <｜User｜>What’s the weather in Paris?\n",
      "\n",
      "\n",
      "<｜Assistant｜>\n",
      "\n",
      "\n",
      "\n",
      "<｜system▁tools｜>\n",
      "Below are available tools for this task. Use them when relevant, wrapping each call in <｜tool▁call▁begin｜> ... <｜tool▁call▁end｜> XML markers, ensure using exact tool and parameter names.{\n",
      "    \"function\": {\n",
      "        \"description\": \"Retrieve weather data for a given city\",\n",
      "        \"name\": \"get_weather\",\n",
      "        \"parameters\": {\n",
      "            \"city\": \"Paris\"\n",
      "        }\n",
      "    },\n",
      "    \"type\": \"function\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example tool usage output:\n",
      "<｜tool▁calls▁begin｜>\n",
      "<｜tool▁call▁begin｜>tool_1_name<｜tool▁sep｜>{\"tool_1_name_param_1_name\": \"tool_1_name_param_0_value\"}<｜tool▁call▁end｜>\n",
      "<｜tool▁call▁begin｜>tool_name_2<｜tool▁sep｜>{\"tool_2_name_param_1_name\": \"tool_2_name_param_1_value\", \"tool_2_name_param_2_name\"\": \"tool_2_name_param_2_value\"}<｜tool▁call▁end｜>\n",
      "<｜tool▁calls▁end｜>\n"
     ]
    }
   ],
   "source": [
    "#apply custom_chat_template (custom DeepSeekV3))\n",
    "rendered_prompt_custom_template = apply_chat_template(\n",
    "    custom_chat_template,\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    add_generation_prompt=True,\n",
    "    extra_context={\n",
    "        \"bos_token\": \"<｜begin▁of▁sentence｜>\",\n",
    "        \"date_string\": f'{datetime.now().strftime('%d %b %Y')}'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n--- Rendered prompt ---\\n\")\n",
    "print(rendered_prompt_custom_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea26a7b",
   "metadata": {},
   "source": [
    "## Invoke completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da52d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the rendered prompt to SambaNova Completions API using the SambaNova SDK.\n",
    "base_url = \"https://api.sambanova.ai/v1\" \n",
    "api_key = os.getenv(\"SAMBANOVA_API_KEY\")\n",
    "client = SambaNova(api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e37105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{\"name\": \"get_weather\", \"parameters\": {\"city\": \"Paris\"}}\n"
     ]
    }
   ],
   "source": [
    "# call llama model with hf_template generated prompt\n",
    "response = client.completions.create(\n",
    "    model=\"Meta-Llama-3.1-8B-Instruct\",\n",
    "    prompt=rendered_prompt_hf_template,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.0,\n",
    "    stream=False\n",
    ")\n",
    "raw_output_hf_template = response.choices[0].text\n",
    "print(raw_output_hf_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a05d2ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>get_weather<｜tool▁sep｜>{\"city\": \"Paris\"}<｜tool▁call▁end｜><｜tool▁calls▁end｜>\n"
     ]
    }
   ],
   "source": [
    "# call deepseek model with custom_template generated prompt\n",
    "response = client.completions.create(\n",
    "    model=\"DeepSeek-V3.1\",\n",
    "    prompt=rendered_prompt_custom_template,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.0,\n",
    "    stream=False\n",
    ")\n",
    "raw_output_custom_template = response.choices[0].text\n",
    "print(raw_output_custom_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd9c43",
   "metadata": {},
   "source": [
    "\n",
    "## Parse Model Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84c83ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate random ID for  invoked tool  \n",
    "def generate_random_id(length=18):\n",
    "    return \"call_\" + str(uuid.uuid4()).replace('-', '')[:length]\n",
    "\n",
    "# Basic Pydantic model for verifying function calling format \n",
    "class ToolCallModel(BaseModel):\n",
    "    \"\"\"Schema validator for a tool-call output.\"\"\"\n",
    "    name: str\n",
    "    arguments: dict\n",
    "    \n",
    "# Use a the Pydantic model to verify the format\n",
    "def instantiate_function_calling_model(model_name: str, parameters: dict):\n",
    "    \"\"\"\n",
    "    Validate and format a function-call into OpenAI tool-call schema.\n",
    "    \"\"\"\n",
    "    ToolCallModel(name=model_name, arguments=parameters)\n",
    "    return {\n",
    "        \"id\": generate_random_id(),\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\"name\": model_name, \"arguments\": json.dumps(parameters)},\n",
    "    }\n",
    "\n",
    "# parse to messages chat structure\n",
    "def parse_to_message(response: str, tool_calls: str):\n",
    "    if tool_calls:\n",
    "        return {\"role\": \"assistant\", \"content\": None, \"tool_calls\": tool_calls}\n",
    "    return {\"role\": \"assistant\", \"content\": response.strip(), \"tool_calls\": []}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb639dda",
   "metadata": {},
   "source": [
    "### Parse JSON tool calls (default llama-3.1-8B hf chat template )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41f845bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama3.1 8B's chat template instructs the model to use JSON tool call format.\n",
    "\n",
    "def extract_llama_json_strings(response: str):\n",
    "    \"\"\"Extract every top-level {...} JSON object from text.\"\"\"\n",
    "    fc_strings, brace_count, start = [], 0, None\n",
    "    for i, ch in enumerate(response):\n",
    "        if ch == \"{\":\n",
    "            if brace_count == 0:\n",
    "                start = i\n",
    "            brace_count += 1\n",
    "        elif ch == \"}\" and start is not None:\n",
    "            brace_count -= 1\n",
    "            if brace_count == 0:\n",
    "                fc_strings.append(response[start : i + 1])\n",
    "                start = None\n",
    "    return fc_strings\n",
    "\n",
    "def llama3_parser(response: str):\n",
    "    \"\"\"Parse JSON-style tool calls (Llama3).\"\"\"\n",
    "    calls = []\n",
    "    for js in extract_llama_json_strings(response):\n",
    "        try:\n",
    "            obj = json.loads(js)\n",
    "            name, params = obj[\"name\"], obj[\"parameters\"]\n",
    "            calls.append(instantiate_function_calling_model(name, params))\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Invalid tool call in block {js[:120]}...: {e}\"\n",
    "            raise ValueError(error_msg)\n",
    "    return calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b504ef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': None,\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"city\": \"Paris\"}',\n",
      "                              'name': 'get_weather'},\n",
      "                 'id': 'call_25aae1eb0404424699',\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "llama_response = raw_output_hf_template\n",
    "llama_tools = llama3_parser(llama_response)\n",
    "\n",
    "pprint(parse_to_message(llama_response, llama_tools))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3772eee",
   "metadata": {},
   "source": [
    "### Parse XML tool calls (custom deepseek chat template )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef29ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek V3.1 uses a XML style format for tool calls with special tokens as markers\n",
    "def extract_deepseek_v3_xml_strings(response: str):\n",
    "    \"\"\"Extract tool-call pairs from DeepSeek XML markers.\"\"\"\n",
    "    fc_strings = []\n",
    "    pattern = r\"<｜tool▁call▁begin｜>(.*?)<｜tool▁sep｜>(.*?)<｜tool▁call▁end｜>\"\n",
    "    for match in re.findall(pattern, response, re.DOTALL):\n",
    "        name, args = match\n",
    "        try:\n",
    "            args = json.loads(args)\n",
    "            fc_strings.append({\"name\": name.strip(), \"parameters\": args})\n",
    "        except Exception as e:\n",
    "            error_msg = (\n",
    "                f\"Invalid JSON in DeepSeek tool-call arguments for '{name.strip()}': {e.msg}\"\n",
    "            )\n",
    "            raise ValueError(error_msg)\n",
    "    return fc_strings\n",
    "\n",
    "def deepseek_v3_parser(response: str):\n",
    "    \"\"\"Parse DeepSeek XML-style tool calls into OpenAI format.\"\"\"\n",
    "    calls = []\n",
    "    for fc in extract_deepseek_v3_xml_strings(response):\n",
    "        calls.append(instantiate_function_calling_model(fc[\"name\"], fc[\"parameters\"]))\n",
    "    return calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b848a7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': None,\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"city\": \"Paris\"}',\n",
      "                              'name': 'get_weather'},\n",
      "                 'id': 'call_d99d4bae428b478dad',\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "deepseek_response = raw_output_custom_template\n",
    "deepseek_tools = deepseek_v3_parser(deepseek_response)\n",
    "\n",
    "pprint(parse_to_message(deepseek_response, deepseek_tools))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d88a5b",
   "metadata": {},
   "source": [
    "## End to End Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7da34c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Llama 3.1 Full workflow ===\n",
      "\n",
      "--- Rendered Prompt ---\n",
      "\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables.\n",
      "\n",
      "{\n",
      "    \"function\": {\n",
      "        \"description\": \"Retrieve weather data for a given city\",\n",
      "        \"name\": \"get_weather\",\n",
      "        \"...\n",
      "\n",
      "--- Raw Model Output ---\n",
      "\n",
      "{\"name\": \"get_weather\", \"parameters\": {\"city\": \"Paris\"}}\n",
      "\n",
      "--- Parsed Assistant Message ---\n",
      "\n",
      "{'content': None,\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"city\": \"Paris\"}',\n",
      "                              'name': 'get_weather'},\n",
      "                 'id': 'call_adadcf6579804b279a',\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example full roundtrip (Llama JSON-based chat template)\n",
    "\n",
    "print(\"=== Llama 3.1 Full workflow ===\")\n",
    "\n",
    "# Step 1: Apply chat template\n",
    "rendered_llama_prompt = apply_chat_template(\n",
    "    hf_chat_template,\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    add_generation_prompt=True,\n",
    "    tokenizer=hf_tokenizer\n",
    ")\n",
    "print(\"\\n--- Rendered Prompt ---\\n\")\n",
    "print(rendered_llama_prompt[:600] + \"...\" if len(rendered_llama_prompt) > 600 else rendered_llama_prompt)\n",
    "\n",
    "# Step 2: Invoke model\n",
    "response = client.completions.create(\n",
    "    model=\"Meta-Llama-3.1-8B-Instruct\",\n",
    "    prompt=rendered_llama_prompt,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.0,\n",
    ")\n",
    "raw_text = response.choices[0].text\n",
    "print(\"\\n--- Raw Model Output ---\\n\")\n",
    "print(raw_text.strip())\n",
    "\n",
    "# Step 3: Parse output\n",
    "parsed_calls = llama3_parser(raw_text)\n",
    "final_message = parse_to_message(raw_text, parsed_calls)\n",
    "\n",
    "print(\"\\n--- Parsed Assistant Message ---\\n\")\n",
    "pprint(final_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd128a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DeepSeek V3 Full workflow ===\n",
      "\n",
      "--- Rendered Prompt ---\n",
      "\n",
      "<｜begin▁of▁sentence｜>You are a helpful assistant.\n",
      "Today Date: 10 Nov 2025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        <｜User｜>What’s the weather in Paris?\n",
      "\n",
      "\n",
      "<｜Assistant｜>\n",
      "\n",
      "\n",
      "\n",
      "<｜system▁tools｜>\n",
      "Below are available tools for this task. Use them when relevant, wrapping each call in <｜tool▁call▁begin｜> ... <｜tool▁call▁end｜> XML markers, ensure using exact tool and parameter names.{\n",
      "    \"function\": {\n",
      "        \"description\": \"Retrieve weather data for a given city\",\n",
      "        \"name\": \"get_weather\",\n",
      "        \"parameters\": {\n",
      "            \"city\": \"Paris\"\n",
      "        }\n",
      "    },\n",
      "    \"type\": \"function\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example tool usage output:\n",
      "<｜tool▁calls▁be...\n",
      "\n",
      "--- Raw Model Output ---\n",
      "\n",
      "<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>get_weather<｜tool▁sep｜>{\"city\": \"Paris\"}<｜tool▁call▁end｜><｜tool▁calls▁end｜>\n",
      "\n",
      "--- Parsed Assistant Message ---\n",
      "\n",
      "{'content': None,\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"city\": \"Paris\"}',\n",
      "                              'name': 'get_weather'},\n",
      "                 'id': 'call_cb2f89aca42f4cde9f',\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example full roundtrip (custom Deepseek XML-based chat template)\n",
    "\n",
    "print(\"=== DeepSeek V3 Full workflow ===\")\n",
    "\n",
    "# Step 1: Apply custom chat template\n",
    "rendered_deepseek_prompt = apply_chat_template(\n",
    "    custom_chat_template,\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    add_generation_prompt=True,\n",
    "    extra_context={\n",
    "        \"bos_token\": \"<｜begin▁of▁sentence｜>\",\n",
    "        \"date_string\": f'{datetime.now().strftime(\"%d %b %Y\")}'\n",
    "    }\n",
    ")\n",
    "print(\"\\n--- Rendered Prompt ---\\n\")\n",
    "print(rendered_deepseek_prompt[:600] + \"...\" if len(rendered_deepseek_prompt) > 600 else rendered_deepseek_prompt)\n",
    "\n",
    "# Step 2: Invoke model\n",
    "response = client.completions.create(\n",
    "    model=\"DeepSeek-V3.1\",\n",
    "    prompt=rendered_deepseek_prompt,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.0,\n",
    ")\n",
    "raw_text = response.choices[0].text\n",
    "print(\"\\n--- Raw Model Output ---\\n\")\n",
    "print(raw_text.strip())\n",
    "\n",
    "# Step 3: Parse output\n",
    "parsed_calls = deepseek_v3_parser(raw_text)\n",
    "final_message = parse_to_message(raw_text, parsed_calls)\n",
    "\n",
    "print(\"\\n--- Parsed Assistant Message ---\\n\")\n",
    "pprint(final_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee62a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
