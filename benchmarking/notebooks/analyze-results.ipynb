{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56950450",
   "metadata": {},
   "source": [
    "# Analyze responses\n",
    "The following is an example of the analysis that can be done on individual responses that are saved when running `token_benchmark_ray.py` with the flag `--results-dir` which enables the saving of all responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfe98a-e81b-4089-9506-97a652993b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd0e93",
   "metadata": {},
   "source": [
    "## Read the input json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7abe9-ed9e-466c-b034-577489aaf98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to the individual responses json file\n",
    "individual_responses_path = f'../data/results/llmperf/<<file_name_for_individual_responses>>.json'\n",
    "df_user = pd.read_json(individual_responses_path)\n",
    "df_user = df_user[df_user['error_code'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb61de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non-batching endpoints, batching_exposed will be False\n",
    "batching_exposed = True\n",
    "if df_user[\"batch_size_used\"].isnull().all():\n",
    "    batching_exposed = False\n",
    "by_batch_size_suffix = ' by batch size' if batching_exposed else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5328791",
   "metadata": {},
   "source": [
    "## Server vs client metrics\n",
    "Following charts show a comparison between server-side and client-side metrics across different performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_client_vs_server_barplots(df_user: pd.DataFrame, x_col: str, y_cols: List[str], legend_labels: List[str], title: str, ylabel: str, xlabel: str, batching_exposed: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Plots bar plots for client vs server metrics from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df_user (pd.DataFrame): The DataFrame containing the data to plot.\n",
    "        x_col (str): The column name to be used as the x-axis.\n",
    "        y_cols (List[str]): A list of column names to be used as the y-axis.\n",
    "        legend_labels (List[str]): Human-readable labels for each grouping in y_cols.\n",
    "        title (str): The title of the plot.\n",
    "        ylabel (str): The label for the y-axis.\n",
    "        xlabel (str): The label for the x-axis.\n",
    "        batching_exposed (bool): boolean identifying if batching was exposed.\n",
    "\n",
    "    Returns:\n",
    "        fig (go.Figure): The plotly figure container\n",
    "    \"\"\"    \n",
    "    value_vars = y_cols\n",
    "    title_text = title\n",
    "    yaxis_title = ylabel\n",
    "    xaxis_title = xlabel if batching_exposed else \"\"\n",
    "\n",
    "    df_melted = df_user.melt(\n",
    "        id_vars=[x_col], \n",
    "        value_vars=value_vars, \n",
    "        var_name='Metric', \n",
    "        value_name='Value',\n",
    "    )\n",
    "    xgroups = [str(x) for x in sorted(pd.unique(df_melted[x_col]))]\n",
    "    df_melted[x_col] = [str(x) for x in df_melted[x_col]]\n",
    "\n",
    "    valsl = {}\n",
    "    valsr = {}\n",
    "    for i in xgroups:\n",
    "        maskl = (df_melted[\"Metric\"] == value_vars[0]) & (df_melted[x_col] == i)\n",
    "        valsl[i] = np.percentile(df_melted[\"Value\"][maskl], [5, 50, 95])\n",
    "        maskr = (df_melted[\"Metric\"] == value_vars[1]) & (df_melted[x_col] == i)\n",
    "        valsr[i] = np.percentile(df_melted[\"Value\"][maskr], [5, 50, 95])\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = xgroups,\n",
    "            y = [0 for _ in xgroups],\n",
    "            base = [valsl[i][1] for i in xgroups],\n",
    "            customdata=[legend_labels[0] for _ in xgroups],\n",
    "            marker={\"color\":\"#325c8c\",\"line\":{\"color\":\"#325c8c\", \"width\":2}},\n",
    "            offsetgroup=0,\n",
    "            legendgroup=legend_labels[0],\n",
    "            name=legend_labels[0],\n",
    "            showlegend=False,\n",
    "            hovertemplate=\"<extra></extra><b>%{customdata}</b> median: %{base:.2f}\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = xgroups,\n",
    "            y = [valsl[i][2] - valsl[i][0] for i in xgroups],\n",
    "            base = [valsl[i][0] for i in xgroups],\n",
    "            customdata = [valsl[i][2] for i in xgroups],\n",
    "            marker={\"color\":\"#325c8c\"},\n",
    "            opacity=0.5,\n",
    "            offsetgroup=0,\n",
    "            legendgroup=legend_labels[0],\n",
    "            name=legend_labels[0],\n",
    "            hovertemplate=\"<extra></extra>5–95 pctile range: %{base:.2f}–%{customdata:.2f}\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = xgroups,\n",
    "            y= [0 for _ in xgroups],\n",
    "            base = [valsr[i][1] for i in xgroups],\n",
    "            customdata=[legend_labels[1] for _ in xgroups],\n",
    "            marker={\"color\":\"#ee7625\",\"line\":{\"color\":\"#ee7625\", \"width\":2}},\n",
    "            offsetgroup=1,\n",
    "            legendgroup=legend_labels[1],\n",
    "            name=legend_labels[1],\n",
    "            showlegend=False,\n",
    "            hovertemplate=\"<extra></extra><b>%{customdata}</b> median: %{base:.2f}\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = xgroups,\n",
    "            y = [valsr[i][2] - valsr[i][0] for i in xgroups],\n",
    "            base = [valsr[i][0] for i in xgroups],\n",
    "            customdata = [valsr[i][2] for i in xgroups],\n",
    "            marker={\"color\":\"#ee7625\"},\n",
    "            opacity=0.5,\n",
    "            offsetgroup=1,\n",
    "            legendgroup=legend_labels[1],\n",
    "            name=legend_labels[1],\n",
    "            hovertemplate=\"<extra></extra>5–95 pctile range: %{base:.2f}–%{customdata:.2f}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        xaxis_title=xaxis_title,\n",
    "        yaxis_title=yaxis_title,\n",
    "        barmode=\"group\",\n",
    "        template=\"plotly_dark\",\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(\n",
    "        hoverformat=\"foo\",\n",
    "        showticklabels=batching_exposed\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add96f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"batch_size_used\"\n",
    "xaxis_title=\"Batch size\"\n",
    "value_vars = ['server_output_token_per_s_per_request', 'client_output_token_per_s_per_request']\n",
    "legend_labels = [\"Server\", \"Client\"]\n",
    "yaxis_title = \"Tokens per second, per request\"\n",
    "title_text = \"Distribution of output throughput\" + by_batch_size_suffix\n",
    "plot_client_vs_server_barplots(df_user, x_col, value_vars, legend_labels, title_text, yaxis_title, xaxis_title, batching_exposed).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"batch_size_used\"\n",
    "xaxis_title=\"Batch size\"\n",
    "value_vars = ['server_ttft_s', 'client_ttft_s']\n",
    "legend_labels = [\"Server\", \"Client\"]\n",
    "yaxis_title = \"TTFT (s), per request\"\n",
    "title_text = \"Distribution of Time to First Token (TTFT)\" + by_batch_size_suffix\n",
    "plot_client_vs_server_barplots(df_user, x_col, value_vars, legend_labels, title_text, yaxis_title, xaxis_title, batching_exposed).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e042e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"batch_size_used\"\n",
    "xaxis_title=\"Batch size\"\n",
    "value_vars = ['server_end_to_end_latency_s', 'client_end_to_end_latency_s']\n",
    "legend_labels = [\"Server\", \"Client\"]\n",
    "yaxis_title = \"Latency (s), per request\"\n",
    "title_text = \"Distribution of end-to-end latency\" + by_batch_size_suffix\n",
    "plot_client_vs_server_barplots(df_user, x_col, value_vars, legend_labels, title_text, yaxis_title, xaxis_title, batching_exposed).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f1208",
   "metadata": {},
   "source": [
    "## Summarize results\n",
    "Group results by batch and get sum of number of tokens, mean throughput, mean TTFT, and batch frequency. Also, calculate the total number of output tokens per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_thorughput = df_user.groupby('batch_size_used', dropna=False)['server_output_token_per_s_per_request'].mean().reset_index()\n",
    "df_summary_output_tokens = df_user.groupby('batch_size_used', dropna=False)['server_number_output_tokens'].sum().reset_index()\n",
    "df_summary_ttft = df_user.groupby('batch_size_used', dropna=False)['server_ttft_s'].mean().reset_index()\n",
    "df_summary_count = df_user.groupby('batch_size_used', dropna=False).size().reset_index(name='Counts')\n",
    "\n",
    "\n",
    "df_summary = pd.merge(df_summary_thorughput, df_summary_output_tokens, on='batch_size_used', how='inner')\n",
    "df_summary = pd.merge(df_summary, df_summary_ttft, on='batch_size_used', how='inner')\n",
    "df_summary = pd.merge(df_summary, df_summary_count, on='batch_size_used', how='inner')\n",
    "if batching_exposed:\n",
    "    df_summary['server_total_output_tokens_per_s'] = df_summary['server_output_token_per_s_per_request'] * df_summary['batch_size_used']\n",
    "    subset_style = [True,False,True,False,True]\n",
    "else:\n",
    "    subset_style = [True,False,True,False]\n",
    "\n",
    "df_summary.rename(columns={\n",
    "    \"batch_size_used\": \"Batch size\",\n",
    "    \"server_output_token_per_s_per_request\": \"Avg. server output tokens per sec per request\",\n",
    "    \"server_number_output_tokens\": \"Total output tokens\",\n",
    "    \"server_ttft_s\": \"Avg. server TTFT (s)\",\n",
    "    \"Counts\": \"Total number of completed requests\",\n",
    "    \"server_total_output_tokens_per_s\": \"Avg. server total output tokens per second\"\n",
    "}, \n",
    "inplace=True)\n",
    "\n",
    "df_summary.set_index(\"Batch size\", inplace=True)\n",
    "df_summary = df_summary.reset_index().fillna(\"\").set_index('Batch size')\n",
    "df_summary.T.style \\\n",
    "    .format(\"{:.4f}\", subset=(subset_style,[True]*len(df_summary))) \\\n",
    "    .format(\"{:.4f}\", subset=(subset_style,[True]*len(df_summary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ccb9bd",
   "metadata": {},
   "source": [
    "Show stats from the corresponding summary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read summary json file \n",
    "\n",
    "json_filename = individual_responses_path.split('/')[-1]\n",
    "summary_filename = json_filename.replace('individual_responses.json', 'summary.json')\n",
    "summary_path = os.path.join('/'.join(individual_responses_path.split('/')[:-1]), summary_filename)\n",
    "\n",
    "with open(summary_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "df_summary = pd.DataFrame(json_data, index=[0])\n",
    "df_summary = df_summary.rename(columns=lambda x: x.replace('results_', '')\n",
    "                   .replace('_quantiles', '')\n",
    "                   .replace('_per_request', '')).copy()\n",
    "\n",
    "# Show summary\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(df_summary.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb81a80",
   "metadata": {},
   "source": [
    "## Time taken\n",
    "- Approximate computation to compare the time spent waiting due to TTFT vs response generation time\n",
    "- Compute the time that calls are cumulatively waiting for time-to-first-token vs time to generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "if batching_exposed:\n",
    "    # calculate number of batches executed in each batch size\n",
    "    df_summary['num_batches_executed'] = np.ceil(df_summary['Total number of completed requests'] / df_summary.index.get_level_values('Batch size'))\n",
    "\n",
    "    # calculate average time taken per request in each batch size\n",
    "    df_summary['output_tokens_per_request'] = df_summary['Total output tokens']/df_summary['Total number of completed requests']\n",
    "    df_summary['time_taken_per_request'] = df_summary['output_tokens_per_request']/df_summary['Avg. server output tokens per sec per request']\n",
    "\n",
    "    # calculate total ttft and generation times across all batch sizes\n",
    "    total_wait_time_ttft = (df_summary['num_batches_executed']*df_summary['Avg. server TTFT (s)']).sum()\n",
    "    total_generation_time = (df_summary['num_batches_executed']*df_summary['time_taken_per_request']).sum()\n",
    "\n",
    "    print(f'Total wait time due to ttft (mins) = {total_wait_time_ttft/60:,.4f}')\n",
    "    print(f'Total generation time due (mins) = {total_generation_time/60:,.4f}')\n",
    "    print(f'Total time (mins) = {(total_wait_time_ttft + total_generation_time)/60:,.4f}')\n",
    "else:\n",
    "    total_wait_time_ttft = df_user['server_ttft_s'].sum()\n",
    "    total_generation_time = df_user['server_end_to_end_latency_s'].sum() - df_user['server_ttft_s'].sum()\n",
    "    total_time = df_user['server_end_to_end_latency_s'].sum()\n",
    "    \n",
    "    print(f'Total wait time due to ttft (mins) = {total_wait_time_ttft/60:,.4f}')\n",
    "    print(f'Total generation time due (mins) = {total_generation_time/60:,.4f}')\n",
    "    print(f'Total time (mins) = {(total_time)/60:,.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7a38e",
   "metadata": {},
   "source": [
    "## Requests Gantt Chart\n",
    "- Blue bar is the total time to get back full response\n",
    "- Orange line is the time call is waiting to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_requests_gantt_chart(df_user: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plots a Gantt chart of response timings across all requests\n",
    "\n",
    "    # if i> 100:\n",
    "    #     break\n",
    "    Args:\n",
    "        df_user (pd.DataFrame): The DataFrame containing the data to plot.\n",
    "\n",
    "    Returns:\n",
    "        fig (go.Figure): The plotly figure container\n",
    "    \"\"\"    \n",
    "    requests = df_user.index+1\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=requests,\n",
    "            x=1000*df_user[\"client_ttft_s\"],\n",
    "            base=[str(x) for x in df_user[\"start_time\"]],\n",
    "            name=\"TTFT\",\n",
    "            orientation=\"h\",\n",
    "            marker_color=\"#ee7625\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=requests,\n",
    "            x=1000*df_user[\"client_end_to_end_latency_s\"],\n",
    "            base=[str(x) for x in df_user[\"start_time\"]],\n",
    "            name=\"End-to-end latency\",\n",
    "            orientation=\"h\",\n",
    "            marker_color=\"#325c8c\",\n",
    "        )\n",
    "    )\n",
    "    for i in range(0, len(df_user.index), 2):\n",
    "        fig.add_hrect(y0=i+0.5, y1=i+1.5, line_width=0, fillcolor=\"grey\", opacity=0.1)\n",
    "    fig.update_xaxes(\n",
    "        type=\"date\",\n",
    "        tickformat=\"%H:%M:%S\",\n",
    "        hoverformat=\"%H:%M:%S.%2f\",)\n",
    "    fig.update_layout(\n",
    "        title_text=\"LLM requests across time\",\n",
    "        xaxis_title=\"Time stamp\",\n",
    "        yaxis_title=\"Request index\",\n",
    "        template=\"plotly_dark\",\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_requests_gantt_chart(df_user).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4176e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_venv",
   "language": "python",
   "name": "benchmark_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
