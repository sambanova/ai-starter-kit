{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56950450",
   "metadata": {},
   "source": [
    "# Analyze responses\n",
    "The following is an example of the analysis that can be done on individual responses that are saved when running `token_benchmark_ray.py` with the flag `--results-dir` which enables the saving of all responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacfe98a-e81b-4089-9506-97a652993b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd0e93",
   "metadata": {},
   "source": [
    "## Read the input json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f7abe9-ed9e-466c-b034-577489aaf98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to the individual responses json file\n",
    "df_user = pd.read_json(f'../data/results/llmperf/COE-llama-2-7B-chat-hf_1024_1024_32_stream_individual_responses.json')\n",
    "df_user = df_user[(df_user[\"error_code\"] != \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb61de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non-batching endpoints, batch_size_used will be 1\n",
    "if df_user[\"batch_size_used\"].isnull().all():\n",
    "    df_user[\"batch_size_used\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5328791",
   "metadata": {},
   "source": [
    "## Server vs client metrics\n",
    "Following charts show a comparison between server-side and client-side metrics across different performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e143d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_client_vs_server_barplots(df_user: pd.DataFrame, x_col: str, y_cols: List[str], legend_labels: List[str], title: str, ylabel: str, xlabel: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots bar plots for client vs server metrics from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df_user (pd.DataFrame): The DataFrame containing the data to plot.\n",
    "        x_col (str): The column name to be used as the x-axis.\n",
    "        y_cols (List[str]): A list of column names to be used as the y-axis.\n",
    "        legend_labels (List[str]): Human-readable labels for each grouping in y_cols.\n",
    "        title (str): The title of the plot.\n",
    "        ylabel (str): The label for the y-axis.\n",
    "        xlabel (str): The label for the x-axis.\n",
    "\n",
    "    Returns:\n",
    "        fig (go.Figure): The plotly figure container\n",
    "    \"\"\"    \n",
    "    value_vars = y_cols\n",
    "    title_text = title\n",
    "    yaxis_title = ylabel\n",
    "    xaxis_title = xlabel\n",
    "\n",
    "    df_melted = df_user.melt(\n",
    "        id_vars=[x_col], \n",
    "        value_vars=value_vars, \n",
    "        var_name='Metric', \n",
    "        value_name='Value',\n",
    "    )\n",
    "    xgroups = [str(x) for x in sorted(pd.unique(df_melted[x_col]))]\n",
    "    df_melted[x_col] = [str(x) for x in df_melted[x_col]]\n",
    "\n",
    "    valsl = {}\n",
    "    valsr = {}\n",
    "    for i in xgroups:\n",
    "        maskl = (df_melted[\"Metric\"] == value_vars[0]) & (df_melted[x_col] == i)\n",
    "        valsl[i] = np.percentile(df_melted[\"Value\"][maskl], [5, 50, 95])\n",
    "        maskr = (df_melted[\"Metric\"] == value_vars[1]) & (df_melted[x_col] == i)\n",
    "        valsr[i] = np.percentile(df_melted[\"Value\"][maskr], [5, 50, 95])\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = xgroups,\n",
    "            y = [0 for _ in xgroups],\n",
    "            base = [valsl[i][1] for i in xgroups],\n",
    "            customdata=[legend_labels[0] for _ in xgroups],\n",
    "            marker={\"color\":\"#325c8c\",\"line\":{\"color\":\"#325c8c\", \"width\":2}},\n",
    "            offsetgroup=0,\n",
    "            legendgroup=legend_labels[0],\n",
    "            name=legend_labels[0],\n",
    "            showlegend=False,\n",
    "            hovertemplate=\"<extra></extra><b>%{customdata}</b> median: %{base:.2f}\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = xgroups,\n",
    "            y = [valsl[i][2] - valsl[i][0] for i in xgroups],\n",
    "            base = [valsl[i][0] for i in xgroups],\n",
    "            customdata = [valsl[i][2] for i in xgroups],\n",
    "            marker={\"color\":\"#325c8c\"},\n",
    "            opacity=0.5,\n",
    "            offsetgroup=0,\n",
    "            legendgroup=legend_labels[0],\n",
    "            name=legend_labels[0],\n",
    "            hovertemplate=\"<extra></extra>5–95 pctile range: %{base:.2f}–%{customdata:.2f}\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = xgroups,\n",
    "            y= [0 for _ in xgroups],\n",
    "            base = [valsr[i][1] for i in xgroups],\n",
    "            customdata=[legend_labels[1] for _ in xgroups],\n",
    "            marker={\"color\":\"#ee7625\",\"line\":{\"color\":\"#ee7625\", \"width\":2}},\n",
    "            offsetgroup=1,\n",
    "            legendgroup=legend_labels[1],\n",
    "            name=legend_labels[1],\n",
    "            showlegend=False,\n",
    "            hovertemplate=\"<extra></extra><b>%{customdata}</b> median: %{base:.2f}\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = xgroups,\n",
    "            y = [valsr[i][2] - valsr[i][0] for i in xgroups],\n",
    "            base = [valsr[i][0] for i in xgroups],\n",
    "            customdata = [valsr[i][2] for i in xgroups],\n",
    "            marker={\"color\":\"#ee7625\"},\n",
    "            opacity=0.5,\n",
    "            offsetgroup=1,\n",
    "            legendgroup=legend_labels[1],\n",
    "            name=legend_labels[1],\n",
    "            hovertemplate=\"<extra></extra>5–95 pctile range: %{base:.2f}–%{customdata:.2f}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        xaxis_title=xaxis_title,\n",
    "        yaxis_title=yaxis_title,\n",
    "        barmode=\"group\",\n",
    "        template=\"plotly_dark\",\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(hoverformat=\"foo\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add96f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"batch_size_used\"\n",
    "xaxis_title=\"Batch size\"\n",
    "value_vars = ['server_output_token_per_s_per_request', 'client_output_token_per_s_per_request']\n",
    "legend_labels = [\"Server\", \"Client\"]\n",
    "yaxis_title = \"Tokens per second, per request\"\n",
    "title_text = \"Distribution of throughput by batch size\"\n",
    "plot_client_vs_server_barplots(df_user, x_col, value_vars, legend_labels, title_text, yaxis_title, xaxis_title).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"batch_size_used\"\n",
    "xaxis_title=\"Batch size\"\n",
    "value_vars = ['server_ttft_s', 'client_ttft_s']\n",
    "legend_labels = [\"Server\", \"Client\"]\n",
    "yaxis_title = \"TTFT (s), per request\"\n",
    "title_text = \"Distribution of Time to First Token (TTFT) by batch size\"\n",
    "plot_client_vs_server_barplots(df_user, x_col, value_vars, legend_labels, title_text, yaxis_title, xaxis_title).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e042e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"batch_size_used\"\n",
    "xaxis_title=\"Batch size\"\n",
    "value_vars = ['server_end_to_end_latency_s', 'client_end_to_end_latency_s']\n",
    "legend_labels = [\"Server\", \"Client\"]\n",
    "yaxis_title = \"Latency (s), per request\"\n",
    "title_text = \"Distribution of end-to-end latency by batch size\"\n",
    "plot_client_vs_server_barplots(df_user, x_col, value_vars, legend_labels, title_text, yaxis_title, xaxis_title).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f1208",
   "metadata": {},
   "source": [
    "## Create a summary dataframe\n",
    "Group results by batch and get sum of number of tokens, mean throughput, mean TTFT, and batch frequency. Finally, calculate the total number of output tokens per batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_thorughput = df_user.groupby('batch_size_used')['server_output_token_per_s_per_request'].mean().reset_index()\n",
    "df_summary_output_tokens = df_user.groupby('batch_size_used')['server_number_output_tokens'].sum().reset_index()\n",
    "df_summary_ttft = df_user.groupby('batch_size_used')['server_ttft_s'].mean().reset_index()\n",
    "df_summary_count = df_user.groupby('batch_size_used').size().reset_index(name='Counts')\n",
    "\n",
    "\n",
    "df_summary = pd.merge(df_summary_thorughput, df_summary_output_tokens, on='batch_size_used', how='inner')\n",
    "df_summary = pd.merge(df_summary, df_summary_ttft, on='batch_size_used', how='inner')\n",
    "df_summary = pd.merge(df_summary, df_summary_count, on='batch_size_used', how='inner')\n",
    "df_summary['server_combined_output_tokens_per_s'] = df_summary['server_output_token_per_s_per_request']*df_summary['batch_size_used']\n",
    "\n",
    "df_summary.rename(columns={\n",
    "    \"batch_size_used\": \"Batch size\",\n",
    "    \"server_output_token_per_s_per_request\": \"Avg. server tokens per sec per request\",\n",
    "    \"server_number_output_tokens\": \"Total output tokens\",\n",
    "    \"server_ttft_s\": \"Avg. server TTFT (s)\",\n",
    "    \"Counts\": \"Total number of requests\",\n",
    "    \"server_combined_output_tokens_per_s\": \"Avg. server total tokens per second\"\n",
    "}, \n",
    "inplace=True)\n",
    "\n",
    "df_summary.set_index(\"Batch size\", inplace=True)\n",
    "df_summary.T.style \\\n",
    "    .format(\"{:.2f}\", subset=([True,False,True,False,True],[True]*len(df_summary))) \\\n",
    "    .format(\"{:.0f}\", subset=([False,True,False,True,False],[True]*len(df_summary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb81a80",
   "metadata": {},
   "source": [
    "## Time taken\n",
    "- Compute the time that calls are cumulatively waiting for time-to-first-token vs time to generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wait_time_ttft = (df_summary[\"Total number of requests\"]/df_summary.index*df_summary[\"Avg. server TTFT (s)\"]).sum()\n",
    "total_generation_time = (df_summary[\"Total output tokens\"]/df_summary[\"Avg. server tokens per sec per request\"]).sum()\n",
    "print(f'Total wait time due to TTFT (mins) = {total_wait_time_ttft/60:,.4f}')\n",
    "print(f'Total generation time due (mins) = {total_generation_time/60:,.4f}')\n",
    "print(f'Total time (mins) = {(total_wait_time_ttft + total_generation_time)/60:,.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7a38e",
   "metadata": {},
   "source": [
    "## Requests Gantt Chart\n",
    "- Blue bar is the total time to get back full response\n",
    "- Orange line is the time call is waiting to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a2adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_requests_gantt_chart(df_user: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plots a Gantt chart of response timings across all requests\n",
    "\n",
    "    Args:\n",
    "        df_user (pd.DataFrame): The DataFrame containing the data to plot.\n",
    "\n",
    "    Returns:\n",
    "        fig (go.Figure): The plotly figure container\n",
    "    \"\"\"    \n",
    "    requests = df_user.index+1\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=requests,\n",
    "            x=1000*df_user[\"client_ttft_s\"],\n",
    "            base=[str(x) for x in df_user[\"start_time\"]],\n",
    "            name=\"TTFT\",\n",
    "            orientation=\"h\",\n",
    "            marker_color=\"#ee7625\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=requests,\n",
    "            x=1000*df_user[\"client_end_to_end_latency_s\"],\n",
    "            base=[str(x) for x in df_user[\"start_time\"]],\n",
    "            name=\"End-to-end latency\",\n",
    "            orientation=\"h\",\n",
    "            marker_color=\"#325c8c\",\n",
    "        )\n",
    "    )\n",
    "    for i in range(0, len(df_user.index), 2):\n",
    "        fig.add_hrect(y0=i+0.5, y1=i+1.5, line_width=0, fillcolor=\"grey\", opacity=0.1)\n",
    "    fig.update_xaxes(\n",
    "        type=\"date\",\n",
    "        tickformat=\"%H:%M:%S\",\n",
    "        hoverformat=\"%H:%M:%S.%2f\",)\n",
    "    fig.update_layout(\n",
    "        title_text=\"LLM requests across time\",\n",
    "        xaxis_title=\"Time stamp\",\n",
    "        yaxis_title=\"Request index\",\n",
    "        template=\"plotly_dark\",\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_requests_gantt_chart(df_user).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
