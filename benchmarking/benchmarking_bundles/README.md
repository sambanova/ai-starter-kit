# Benchmarking bundles

This directory contains scripts to run **end-to-end benchmarking for model bundles**, supporting both **synthetic (concurrency-based)** and **real workload (QPS-based)** inference tests, with optional **row-level concurrency**. Results include automatic **batching** and **switching time** estimations.

---

## Required steps

The required steps for Bundle benchmarking are the following:


### 1. Configuration file

Modify the following file:

- `<PATH TO AISK REPO HERE>/benchmarking/benchmarking_scripts/config.yaml`

Example:

```yaml
model_configs_path: '<PATH TO AISK REPO HERE>/benchmarking/benchmarking_bundles/model_configs_example.csv'
llm_api: 'sncloud'  # only option currently
output_files_dir: '<PATH TO AISK REPO HERE>/benchmarking/data/bundle_tests/output_files'
consolidated_results_dir: '<PATH TO AISK REPO HERE>/benchmarking/data/bundle_tests/consolidated_results'
timeout: 3600
time_delay: 0
# Row-level concurrency
concurrency_enabled: False
max_workers: 4
# Prompt behavior
use_multiple_prompts: False
```

#### Key notes:
- timeout: it's the maximum time in seconds that each model config row will take to process.
- concurrency_enabled: If true, each model config row in the CSV is benchmarked concurrently using a thread pool.
- max_workers: Maximum number of parallel benchmark jobs.
- time_delay: Optional sleep between runs (per row).
- use_multiple_prompts: If true, multiple prompts in located in `<PATH TO AISK REPO>/benchmarking/prompts/user-prompt_template-text_instruct.yaml` will be used randomly.

### 2. Model configuration file

Modify:

`<PATH TO AISK REPO HERE>/benchmarking/benchmarking_bundles/model_configs_example.csv`

Header:

`model_name,input_tokens,output_tokens,num_requests,concurrent_requests,qps,qps_distribution,multimodal_img_size`

Each row defines one benchmark job.

#### Configuration parameters

The configuration table in `model_configs_example.csv` details each individual model of the composition that we would like to test.

- `model_name`  
  Name of the model in the bundle.

- `input_tokens`  
  Number of input tokens in the generated prompt.

- `output_tokens`  
  Maximum number of output tokens generated by the model.

- `num_requests`  
  Total number of requests sent.

  ⚠️ The run may timeout before all requests are sent.  
  Configure the `timeout` parameter in `benchmarking_bundles/config.yaml` accordingly.

- `concurrent_requests`  
  Enables **synthetic workload benchmarking**.

  - Used when testing batching-enabled models
  - Typical values: `1`, `4`, `8`, `16`
  - If this value is set, `qps` is ignored

- `qps`  
  Enables **real workload benchmarking** (queries per second).

  - Recommended values `< 10`
  - Ignored if `concurrent_requests` is set

- `qps_distribution`  
  Wait-time distribution between requests.

  Supported values:
  - `constant` (default)
  - `uniform`
  - `exponential`
  
  Ignored if concurrent_requests is set
- `multimodal_img_size`  
  Used only for multimodal models to include an image in the benchmark requests.

  Supported values:
  - `small` → 500×500 px
  - `medium` → 1000×1000 px
  - `large` → 2000×2000 px

  For non-multimodal models, leave it empty.

> **Important:**  
> The sum of `input_tokens` and `output_tokens` must not exceed the maximum sequence length supported by the model.
>  
> Example: for a model with a 4096-token context window, you may set  
> `input_tokens = 4000` and `output_tokens = 64`. However, consider  that models might have internal prompting tokens, so include them when calculating the full context tokens.

### 3. Execution modes

#### 3.1 Sequential execution

If `concurrency_enabled: false` in `config.yaml`, models are benchmarked **row by row** in the order defined in `model_configs_example.csv`.

This mode is useful for:
- Running on limited resources
- Exposing switching-time effects
- Debugging

#### 3.2 Concurrent execution

If `concurrency_enabled: true`, each row in `model_configs_example.csv` is benchmarked **in parallel** using a `ThreadPoolExecutor`.

Relevant configuration fields:
- `concurrency_enabled`: Enables row-level parallelism
- `max_workers`: Maximum number of benchmark jobs running concurrently

This mode is useful when:
- A more real approach needs to be tested
- Faster turnaround or switching is needed for large bundles

### 4. Results

#### 4.1 Results per model config

Individual model config results are stored under: `output_files_dir` in `config.yaml`.

For every model configuration (row in `model_configs_example.csv`), two result files are generated:

1. **Individual responses** (`*individual_responses.json`)  
   - One entry per request
   - Contains per-request metrics such as:
     - Time to First Token (TTFT)
     - End-to-end latency
     - Output throughput

2. **Summary files** (`*summary.json`)  
   - Aggregated statistics computed from individual responses:
     - Minimum
     - Maximum
     - Mean
     - Median (p50)
     - Standard deviation

If you'd like to know more details about these output files, please check the kit's CLI version in the main [README](../README.md).

#### 4.2 Consolidated results

Consolidated results are written to: `consolidated_results_dir` in `config.yaml`.

Each row in the Excel file corresponds to **one row in `model_configs_example.csv`**.

##### Terminology

- **Server metrics**  
  Performance metrics returned by the inference server API.  
  Metric names are prefixed with `server_`.

- **Client metrics**  
  Performance metrics computed on the client side or machine sending the API requests.  
  Metric names are prefixed with `client_`.

- **Suffixes**
  - `min`, `max`, `p50`, `mean`
  - `s` indicates that the value is in seconds

##### Main consolidated metrics

- **TTFT (Time to First Token)**  
  TTFT typically shows higher variance due to request queueing, especially when concurrency is enabled. Client-side TTFT could be influenced by the networking time.

- **End-to-end latency**  
  Includes queueing time and TTFT. Client-side latency could be influenced by the networking time.

- **Output tokens per second**  
  Output throughput per request. For batching-enabled endpoints, throughput per request may decrease as batch size increases, but the overall throughput will increase.

- **Acceptance rate**  
  Acceptance rate for speculative decoding pairs.

##### Batching analysis

The benchmarking pipeline automatically estimates **batching behavior** by analyzing request timing patterns.

- Requests with identical `server_ttft` are grouped together
- Batch size is inferred as the next power of two of the group size

Reported batching fields

- `request_batching_frequencies`  
  Frequency of observed batch sizes within a run.

- `representative_batch_size`  
  Median-dominant batch size that accounts for more than 50% of requests.

__Note__: this is just an estimation based on close server TTFTs and may not be accurate to the actual batch size value. So, using a dedicated environment is recommended.

##### Switching time

Switching time represents the overhead incurred when loading a new model or configuration into HBM memory during inference.

A switch can be triggered by changes in:
- Model name
- Sequence length
- Batch size

##### How switching time is estimated:

For each benchmark run (identified by a UUID):

1. Identify the largest estimated batch size.
2. Compute the maximum and minimum server-side TTFT within that batch.
3. Switching time is calculated as the **variation in server-side Time To First Token (TTFT) in seconds**:
`switching_time = max(server_ttft_s) - min(server_ttft_s)`

__Note__: Only requests at the **highest batching level** are considered.

##### Where to find it

- Column: `switching_time`
- Derived from the first requests in `individual_responses.json`

##### Switching time important notes

- It's recommended that the user include a warm up set of models, either in the same `model_configs_example.csv` or in a separate csv, so HBM could be set with the model configurations that will be tested. Right after the warm up models are procesed, proceed run the configs that would include the switching time. 
- Include multiple sequence lengths and batch sizes according to the node environment configuration.
- Run multiple requests per model config row to obtain stable switching time estimates
