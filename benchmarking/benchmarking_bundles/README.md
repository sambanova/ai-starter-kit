# Benchmarking bundles

This directory contains scripts to run **end-to-end benchmarking for model bundles**, supporting both **synthetic (concurrency-based)** and **real workload (QPS-based)** inference tests, with optional **row-level concurrency**. Results include automatic **batching** and **switching time** estimations.

---

## Required steps

The required steps for Bundle benchmarking are the following:


### 1. Configuration file

Modify the following file:

- `_PATH TO AISK REPO HERE/benchmarking/benchmarking_scripts/config.yaml_`

Example:

```yaml
model_configs_path: '<PATH TO AISK REPO HERE>/benchmarking/benchmarking_scripts/model_configs_example.csv'
llm_api: 'sambastudio'  # or sncloud
output_files_dir: '<PATH TO AISK REPO HERE>/benchmarking/data/benchmarking_tracking_tests/logs/output_files'
consolidated_results_dir: '<PATH TO AISK REPO HERE>/benchmarking/data/benchmarking_tracking_tests/consolidated_results'
timeout: 3600
time_delay: 0

# Row-level concurrency
concurrency_enabled: true
max_workers: 4

# Prompt behavior
use_multiple_prompts: false
```

#### Key notes:
- concurrency_enabled: If true, each row in the CSV is benchmarked concurrently using a thread pool.
- max_workers: Maximum number of parallel benchmark jobs.
- time_delay: Optional sleep between runs (per row).

### 2. Model configuration file

Modify:

`_PATH TO AISK REPO HERE/benchmarking/benchmarking_scripts/model_configs_example.csv_`

Header:

`model_name,input_tokens,output_tokens,num_requests,concurrent_requests,qps,qps_distribution,multimodal_img_size`

Each row defines one benchmark job.

#### Configuration parameters

The configuration table in `model_configs_example.csv` details each individual model of the composition that we would like to test.

- `model_name`  
  Name of the model in the bundle.

- `input_tokens`  
  Number of input tokens in the generated prompt.

- `output_tokens`  
  Maximum number of output tokens generated by the model.

- `num_requests`  
  Total number of requests sent.

  ⚠️ The run may timeout before all requests are sent.  
  Configure the `timeout` parameter in `benchmarking_scripts/config.yaml` accordingly.

- `concurrent_requests`  
  Enables **synthetic workload benchmarking**.

  - Used when testing batching-enabled models
  - Typical values: `1`, `4`, `8`, `16`
  - If this value is set, `qps` is ignored

- `qps`  
  Enables **real workload benchmarking** (queries per second).

  - Recommended values `< 10`
  - Ignored if `concurrent_requests` is set

- `qps_distribution`  
  Wait-time distribution between requests.

  Supported values:
  - `constant` (default)
  - `uniform`
  - `exponential`

- `multimodal_img_size`  
  Used only for multimodal models to include an image in the benchmark requests.

  Supported values:
  - `small` → 500×500 px
  - `medium` → 1000×1000 px
  - `large` → 2000×2000 px

  For non-multimodal models, set this field to `N/A` or leave it empty.

> **Important:**  
> The sum of `input_tokens` and `output_tokens` must not exceed the maximum sequence length supported by the model.
>  
> Example: for a model with a 4096-token context window, you may set  
> `input_tokens = 4000` and `output_tokens = 64`.

### 3. Execution modes

#### 3.1 Sequential execution

If `concurrency_enabled: false` in `config.yaml`, models are benchmarked **row by row** in the order defined in `model_configs_example.csv`.

This mode is useful for:
- Debugging
- Isolating switching-time effects
- Running on limited resources

#### 3.2 Concurrent execution

If `concurrency_enabled: true`, each row in `model_configs_example.csv` is benchmarked **in parallel** using a `ThreadPoolExecutor`.

Relevant configuration fields:
- `concurrency_enabled`: Enables row-level parallelism
- `max_workers`: Maximum number of benchmark jobs running concurrently

This mode is useful when:
- Models are independent
- Faster turnaround is needed for large bundles

### 4. Results

#### 4.1 Individual results

Individual results are stored under:

For every model configuration (row in `model_configs_example.csv`), two result files are generated:

1. **Individual responses** (`*individual_responses.json`)  
   - One entry per request
   - Contains per-request metrics such as:
     - Time to First Token (TTFT)
     - End-to-end latency
     - Output throughput
     - Switching-related indicators

2. **Summary files** (`*summary.json`)  
   - Aggregated statistics computed from individual responses:
     - Minimum
     - Maximum
     - Mean
     - Median (p50)
     - Standard deviation

#### 4.2 Consolidated results

Consolidated results are written to:

Each row in the Excel file corresponds to **one row in `model_configs_example.csv`**.

##### Terminology

- **Server metrics**  
  Performance metrics returned by the inference server API.  
  Metric names are prefixed with `server_`.

- **Client metrics**  
  Performance metrics computed on the client side.  
  Metric names are prefixed with `client_`.

- **Suffixes**
  - `min`, `max`, `p50`, `mean`
  - `s` indicates that the value is in seconds

##### Main consolidated metrics

- **TTFT (Time to First Token)**  
  Client-side TTFT typically shows higher variance due to request queueing, especially when concurrency is enabled.

- **End-to-end latency**  
  Client-side latency includes queue wait time, while server-side latency does not.

- **Output tokens per second**  
  Output throughput per request.  
  For batching-enabled endpoints, throughput per request may decrease as batch size increases.

- **Acceptance rate**  
  Acceptance rate for speculative decoding pairs.

##### Batching analysis

The benchmarking pipeline automatically infers **effective batching behavior** by analyzing request timing patterns.

- Requests with identical `server_ttft` are grouped together
- Batch size is inferred as the next power of two of the group size

Reported batching fields

- `request_batching_frequencies`  
  Frequency of observed batch sizes within a run.

- `representative_batch_size`  
  Median-dominant batch size that accounts for more than 50% of requests.

##### Switching time

Switching time represents the overhead incurred when loading a new model or configuration into HBM memory during inference.

A switch can be triggered by changes in:
- Model name
- Sequence length
- Batch size
- Multimodal configuration

##### How switching time is estimated:

For each benchmark run (identified by a UUID):

1. Identify the largest observed batch size
2. Compute the maximum and minimum server-side TTFT within that batch
3. Switching time is calculated as:


##### Where to find it

- Column: `switching_time`
- Derived from the first requests in `individual_responses.json`

##### Switching time important notes

- In `model_configs_example.csv`, Include a warm up set of models so HBM could be set with the models that will be tested. Right after the warm up models, proceed to include the models that would be tested. 
- Include multiple sequence lengths and batch sizes according to the node environment configuration.
- Run multiple benchmark iterations per sequence lenght and batch size to obtain stable switching time estimates
