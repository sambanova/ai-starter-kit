{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from typing import Optional\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('../../src')\n",
    "sys.path.append('../../prompts')\n",
    "sys.path.append('../../src/llmperf')\n",
    "\n",
    "from benchmarking.utils import read_perf_eval_json_files\n",
    "from synthetic_performance_eval_script import load_requests_with_switching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from bundled model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dir = '../data/results/tracking_tests'\n",
    "results_dir = '../../data/bundle_tests/aa_tests/20260113-110355.785523'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze metrics through models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read summary json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_perf_eval_json_files(results_dir, type='summary')\n",
    "df = df.set_index(['num_input_tokens','num_output_tokens','num_concurrent_requests'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_benchmarking_data(df, ycol, ylabel):\n",
    "    df = df.reset_index()\n",
    "    # Create a new column combining 'in' and 'out' into a single legend label\n",
    "    df['in_out'] = df.apply(lambda row: f\"({row['num_input_tokens']}, {row['num_output_tokens']})\", axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    # Plot main performance metric\n",
    "    sns.lineplot(data=df, x='num_concurrent_requests', y=ycol, hue='in_out', marker='o', linewidth=2)\n",
    "    \n",
    "    # Plot client_output_token_per_sec as a dashed line\n",
    "    if ycol=='output_token_per_s_p50' and 'client_output_token_per_s_p50' in df.columns:\n",
    "        sns.lineplot(data=df, x='num_concurrent_requests', y='client_output_token_per_s_p50', hue='in_out', \n",
    "                     marker='o', linestyle='dashed', alpha=0.6)\n",
    "    \n",
    "    # Set x-axis to log2 scale\n",
    "    plt.xscale(\"log\", base=2)\n",
    "    \n",
    "    # Customize x-axis ticks to show real numbers instead of 2^n notation\n",
    "    xticks = sorted(df['num_concurrent_requests'].unique())\n",
    "    plt.xticks(xticks, labels=[str(x) for x in xticks])\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Concurrency [log2 scale]\")\n",
    "    plt.ylabel(ylabel)\n",
    "    if ycol=='output_token_per_s_p50':\n",
    "        plt.title(\"Performance for Different (in, out) Token Combinations\\n(Solid = Server, Dashed = Client)\")\n",
    "    else:\n",
    "        plt.title(\"Performance for Different (in, out) Token Combinations\")\n",
    "    \n",
    "    plt.legend(title=\"(in, out)\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\")\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_benchmarking_data(df, 'server_output_token_per_s_p50', 'Output Tokens per Second per Request (median)')\n",
    "plot_benchmarking_data(df, 'server_ttft_s_p50', 'Server Time to First Token')\n",
    "plot_benchmarking_data(df, 'client_ttft_s_p50', 'Client Time to First Token')\n",
    "plot_benchmarking_data(df, 'client_total_output_throughput', 'E2E Token Throughput per Sec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze switching time\n",
    "\n",
    "__Note:__ It's recomended that you've warmed up your environment previously by running a first round of models with their available sequence sizes and batch sizes combinations, so the switching time is more probable to show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_requests = load_requests_with_switching(\n",
    "    output_files_dir=results_dir,\n",
    "    read_perf_eval_json_files_fn=read_perf_eval_json_files,\n",
    ")\n",
    "\n",
    "# get model name \n",
    "df_requests['model'] = df_requests['filename'].apply(lambda x: x.split('_')[2])\n",
    "\n",
    "cols = [\n",
    "    \"uuid\",\n",
    "    \"model\",\n",
    "    \"server_number_input_tokens\",\n",
    "    \"server_number_output_tokens\",\n",
    "    \"start_time\",\n",
    "    \"server_ttft_s\",\n",
    "    \"requests_batching_per_request\",\n",
    "    \"switching_time\",\n",
    "]\n",
    "\n",
    "df_switching_per_request = df_requests[cols].sort_values(\"start_time\")\n",
    "df_switching_per_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_start_time(df: pd.DataFrame, col: str = \"start_time\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robust parsing for start_time provided as string.\n",
    "    Supports:\n",
    "      - ISO timestamps\n",
    "      - Datetime-like strings\n",
    "      - Epoch seconds (as string)\n",
    "    \"\"\"\n",
    "    # Try datetime parsing first\n",
    "    parsed = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "\n",
    "    if parsed.isna().all():\n",
    "        # Fallback: epoch seconds as string\n",
    "        try:\n",
    "            parsed = pd.to_datetime(\n",
    "                df[col].astype(float),\n",
    "                unit=\"s\",\n",
    "                utc=True,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                f\"Unable to parse '{col}' as datetime or epoch seconds\"\n",
    "            ) from e\n",
    "\n",
    "    if parsed.isna().any():\n",
    "        raise ValueError(\n",
    "            f\"Some values in '{col}' could not be parsed. \"\n",
    "            \"Please check the input format.\"\n",
    "        )\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def plot_ttft_switching_scatter(\n",
    "    df_requests: pd.DataFrame,\n",
    "    save_html: bool = False,\n",
    "    output_path: Optional[str] = None,\n",
    "    title: str = \"TTFT vs Request Start Time (Switching Effects)\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactive Plotly scatter plot:\n",
    "      - x: request start time (relative, seconds)\n",
    "      - y: server TTFT\n",
    "      - color: model + sequence size\n",
    "      - symbol: batch size\n",
    "\n",
    "    Handles start_time provided as string.\n",
    "    \"\"\"\n",
    "\n",
    "    if df_requests.empty:\n",
    "        raise ValueError(\"df_requests is empty\")\n",
    "\n",
    "    df = df_requests.copy()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Parse start_time safely (string -> datetime)\n",
    "    # --------------------------------------------------\n",
    "    df[\"_start_dt\"] = _parse_start_time(df, \"start_time\")\n",
    "\n",
    "    t0 = df[\"_start_dt\"].min()\n",
    "    df[\"start_time_rel_s\"] = (df[\"_start_dt\"] - t0).dt.total_seconds()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Derived visualization fields\n",
    "    # --------------------------------------------------\n",
    "    df[\"sequence\"] = (\n",
    "        df[\"server_number_input_tokens\"].astype(str)\n",
    "        + \"→\"\n",
    "        + df[\"server_number_output_tokens\"].astype(str)\n",
    "    )\n",
    "\n",
    "    df[\"model_sequence\"] = df[\"model\"] + \" | \" + df[\"sequence\"]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Plotly scatter\n",
    "    # --------------------------------------------------\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"start_time_rel_s\",\n",
    "        y=\"server_ttft_s\",\n",
    "        color=\"model_sequence\",\n",
    "        symbol=\"requests_batching_per_request\",\n",
    "        title=title,\n",
    "        labels={\n",
    "            \"start_time_rel_s\": \"Request start time (relative, s)\",\n",
    "            \"server_ttft_s\": \"Server TTFT (s)\",\n",
    "            \"model_sequence\": \"Model | Sequence\",\n",
    "            \"requests_batching_per_request\": \"Batch size\",\n",
    "        },\n",
    "        hover_data={\n",
    "            \"uuid\": True,\n",
    "            \"model\": True,\n",
    "            \"sequence\": True,\n",
    "            \"requests_batching_per_request\": True,\n",
    "            \"switching_time\": True,\n",
    "            \"start_time\": True,  # original string preserved\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(size=8, opacity=0.75))\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        height=600,\n",
    "        legend_title_text=\"Model | Sequence\",\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Optional HTML export\n",
    "    # --------------------------------------------------\n",
    "    if save_html:\n",
    "        if not output_path:\n",
    "            raise ValueError(\"output_path must be provided when save_html=True\")\n",
    "\n",
    "        output_path = os.path.expanduser(output_path)\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        fig.write_html(\n",
    "            output_path,\n",
    "            full_html=True,\n",
    "            include_plotlyjs=\"cdn\",\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Plot saved to {output_path}\")\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_ttft_switching_scatter(\n",
    "    df_requests,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidation file path\n",
    "parent_dir = os.path.dirname(results_dir)\n",
    "run_name = os.path.basename(results_dir)\n",
    "consolidation_file = os.path.join(parent_dir, f'consolidated_results/{run_name}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance_dashboard(\n",
    "    consolidation_file: str,\n",
    "    save_html: bool = False,\n",
    "    output_path: Optional[str] = None,\n",
    "    title=\"Model Performance Dashboard (Tokens × Concurrency)\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a per-model performance dashboard with:\n",
    "      - Switching Time\n",
    "      - Server TTFT p50\n",
    "      - Output Tokens/s p50\n",
    "\n",
    "    X-axis is a composite of (num_input_tokens × num_concurrent_requests),\n",
    "    ordered independently per model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    consolidation_file : str\n",
    "        Path to the Excel file containing consolidated benchmarking results.\n",
    "    save_html : bool, optional\n",
    "        Whether to save the figure as an HTML file, by default False.\n",
    "    output_path : Optional[str], optional\n",
    "        Path where the HTML file will be saved.\n",
    "        Required if save_html=True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    plotly.graph_objects.Figure\n",
    "        The generated Plotly figure.\n",
    "    \"\"\"\n",
    "\n",
    "    # =========================\n",
    "    # Load data\n",
    "    # =========================\n",
    "    df = pd.read_excel(consolidation_file)\n",
    "\n",
    "    # =========================\n",
    "    # Prepare composite X axis\n",
    "    # =========================\n",
    "    df = df.sort_values(\n",
    "        by=[\"model\", \"num_input_tokens\", \"num_concurrent_requests\"]\n",
    "    )\n",
    "\n",
    "    df[\"x_label\"] = (\n",
    "        \"tokens=\" + df[\"num_input_tokens\"].astype(str)\n",
    "        + \" | conc=\" + df[\"num_concurrent_requests\"].astype(str)\n",
    "    )\n",
    "\n",
    "    models = df[\"model\"].unique()\n",
    "    rows = len(models)\n",
    "    cols = 3\n",
    "\n",
    "    # =========================\n",
    "    # Subplot titles\n",
    "    # =========================\n",
    "    subplot_titles = []\n",
    "    for model in models:\n",
    "        subplot_titles.extend(\n",
    "            [\n",
    "                f\"{model} - Switching Time\",\n",
    "                f\"{model} - Server TTFT p50\",\n",
    "                f\"{model} - Output Tokens/s p50\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=subplot_titles,\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # Add traces + per-model x-axis\n",
    "    # =========================\n",
    "    row_idx = 1\n",
    "\n",
    "    for model in models:\n",
    "        model_df = df[df[\"model\"] == model]\n",
    "\n",
    "        # Model-specific x-axis order\n",
    "        model_x_order = model_df[\"x_label\"].unique().tolist()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_df[\"x_label\"],\n",
    "                y=model_df[\"switching_time\"],\n",
    "                mode=\"markers+lines\",\n",
    "            ),\n",
    "            row=row_idx,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_df[\"x_label\"],\n",
    "                y=model_df[\"server_ttft_s_p50\"],\n",
    "                mode=\"markers+lines\",\n",
    "            ),\n",
    "            row=row_idx,\n",
    "            col=2,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=model_df[\"x_label\"],\n",
    "                y=model_df[\"server_output_token_per_s_p50\"],\n",
    "                mode=\"markers+lines\",\n",
    "            ),\n",
    "            row=row_idx,\n",
    "            col=3,\n",
    "        )\n",
    "\n",
    "        # Apply x-axis ordering only to this model's row\n",
    "        for col_idx in range(1, cols + 1):\n",
    "            fig.update_xaxes(\n",
    "                categoryorder=\"array\",\n",
    "                categoryarray=model_x_order,\n",
    "                tickangle=45,\n",
    "                row=row_idx,\n",
    "                col=col_idx,\n",
    "            )\n",
    "\n",
    "        row_idx += 1\n",
    "\n",
    "    # =========================\n",
    "    # Layout\n",
    "    # =========================\n",
    "    fig.update_layout(\n",
    "        height=350 * rows,\n",
    "        width=1600,\n",
    "        title=title,\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # Output handling\n",
    "    # =========================\n",
    "    if save_html:\n",
    "        if not output_path:\n",
    "            raise ValueError(\"output_path must be provided when save_html=True\")\n",
    "        fig.write_html(output_path)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_model_performance_dashboard(\n",
    "    consolidation_file=consolidation_file,\n",
    "    save_html=True,\n",
    "    output_path=\"model_performance_dashboard.html\",\n",
    "    title=\"Model Performance Dashboard (Tokens × Concurrency)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmarking_venv",
   "language": "python",
   "name": "benchmarking_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
