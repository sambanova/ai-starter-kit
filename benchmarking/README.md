
<a href="https://sambanova.ai/">
<picture>
 <source media="(prefers-color-scheme: dark)" srcset="../images/SambaNova-light-logo-1.png" height="100">
  <img alt="SambaNova logo" src="../images/SambaNova-dark-logo-1.png" height="100">
</picture>
</a>

Benchmarking
======================

<!-- TOC -->

- [Benchmarking](#benchmarking)
- [Overview](#overview)
- [Before you begin](#before-you-begin)
  - [Clone this repository](#clone-this-repository)
  - [Set up the inference endpoint, and environment variables](#set-up-the-inference-endpoint-and-environment-variables)
  - [Create the (virtual) environment](#create-the-virtual-environment)
- [Use the starter kit](#use-the-starter-kit)
  - [GUI Option](#gui-option)
    - [Deploy the starter kit GUI](#deploy-the-starter-kit-gui)
    - [Full Walkthrough](#full-walkthrough)
      - [Synthetic Performance Evaluation](#synthetic-performance-evaluation)
      - [Real Workload Performance Evaluation](#real-workload-performance-evaluation)
      - [Custom Performance Evaluation](#custom-performance-evaluation)
      - [Performance on Chat](#performance-on-chat)
  - [CLI Option](#cli-option)
    - [Synthetic Dataset](#synthetic-dataset)
    - [Real Workload Dataset](#real-workload-dataset)
    - [Custom Dataset](#custom-dataset)
- [Third-party tools and data sources](#third-party-tools-and-data-sources)

<!-- /TOC -->

# Overview

This AI Starter Kit evaluates the performance of different LLM models hosted by [SambaNova.ai](https://sambanova.ai/). It allows users to configure various LLMs with diverse parameters, enabling experiments to not only generate different outputs but also measurement metrics simultaneously. The Kit includes:
- Configurable SambaStudio and SambanNova Cloud connectors. The connectors generate answers from a deployed model. You can also use any of them to run OpenAI compatible endpoints from other providers.
- An app with four functionalities:
    - A synthetic performance evaluation process with configurable options that users will utilize to obtain and compare different metrics over synthetic data generated by the app.
    - A real workload performance evaluation process with configurable options that users will utilize to obtain and compare different metrics over real workloads of synthetic data generated by the app.
    - A custom performance evaluation process with configurable options that users will utilize to obtain and compare different metrics over their own customed prompts. 
    - A chat interface with configurable options that users will set to interact and get performance metrics
- A set of bash scripts that are the core of the performance evaluations and provide more flexibility to users

This sample is ready-to-use. We provide:
- Instructions for setup with SambaStudio or SambaNova Cloud
- Instructions for running the model as-is
- Instructions for customizing the model
   
# Before you begin

To perform this setup, you must be a SambaNova customer with a SambaStudio account or have a SambaNova Cloud API key (more details in the following sections). You also have to set up your environment before you can run or customize the starter kit. 

_These steps assume a Mac/Linux/Unix shell environment. If using Windows, you will need to adjust some commands for navigating folders, activating virtual environments, etc._

## Clone this repository

Clone the starter kit repo.
```bash
git clone https://github.com/sambanova/ai-starter-kit.git
```

## Set up the inference endpoint, and environment variables

The next step is to set up your environment variables to use one of the models available from SambaNova. If you're a current SambaNova customer, you can deploy your models with SambaStudio. If you are not a SambaNova customer, you can self-service provision API endpoints using SambaNova Cloud.

- If using **SambaNova Cloud** Please follow the instructions [here](../README.md#use-sambanova-cloud-option-1) for setting up your environment variables.

- If using **SambaStudio** Please follow the instructions [here](../README.md#use-sambastudio-option-2) for setting up endpoint and your environment variables. 
_Note: In case of OpenAI compatible SambaNova endpoints, dynamic batching is already supported. However, in case you're using a **SambaStudio Turbo** endpoint, please update the URL variable and include `api/v2` like the following to ensure the use of dynamic batching:_
``` bash
SAMBASTUDIO_URL="https://api-stage.sambanova.net/api/v2/predict/generic/12345678-9abc-def0-1234-56789abcdef0/456789ab-cdef-0123-4567-89abcdef0123"
```


**Note**: You can also use an OpenAI compatible endpoint from other providers. You will need to generate respective API keys on their platforms and map the URL and API Key to either SambaStudio or SambaNova Cloud environment variables.

## Create the (virtual) environment
1. (Recommended) Create a virtual environment and activate it (python version 3.11 recommended): 
    ```bash
    python<version> -m venv <virtual-environment-name>
    source <virtual-environment-name>/bin/activate
    ```

2. Install the required dependencies:
    ```bash
    cd benchmarking # If not already in the benchmarking folder
    pip install -r requirements.txt
    ```

# Use the starter kit

When using the benchmarking starter kit, you have two options for running the program:

- [*GUI Option*](#gui-option): This option contains plots and configurations from a web browser.
- [*CLI Option*](#cli-option): This option allows you to run the program from the command line and provides more flexibility.


## GUI Option

The GUI for this starter kit uses Streamlit, a Python framework for building web applications. This method is useful for analyzing outputs in a graphical manner since the results are shown via plots in the UI.

### Deploy the starter kit GUI

Ensure you are in the `benchmarking` folder and run the following command:

```shell
ulimit -n 4096
streamlit run streamlit/app.py --browser.gatherUsageStats false 
```

### Full Walkthrough

There are 4 options on the left side bar for running the benchmarking tool. Pick the walkthrough that best suits your needs.

<details id="synthetic-performance-evaluation">
<summary><strong>Synthetic Performance Evaluation</strong></summary>

This option allows you to evaluate the performance of the selected LLM on synthetic data generated by this benchmarking tool.

![Synthetic Performance Evaluation](./imgs/synthetic_performance_eval.png)

1. Enter a model name and choose the right API type

     _Note: Currently we have specific prompting support for Llama2, Llama3, Llama3.1, Llama3.2, Llama3.3, Llama4, Mistral, Deepseek, Qwen, QwQ, Solar, and Eeve. Other instruction models can work, but number of tokens may not be close to the ones specified._

  - If the model specified is a bundle, specify the desired expert in the Model Name text box with the prefix `Bundle`. 
    - For example, the Samba-1 Turbo Llama-3-8B expert in studio is titled `Meta-Llama-3-8B-Instruct` so the model name would be `Bundle/Meta-Llama-3-8B-Instruct`.
  - If the model comes from a SambaStudio endpoint using its OpenAI compatible URL or a standalone model, enter the full model name shown on the model card. E.g. `Meta-Llama-3.3-70B-Instruct`.
  - If the model is a SambaNova Cloud one, then use one of the models displayed in the website. Then, choose `SambaNova Cloud` in the API type dropdown option. E.g. `DeepSeek-R1`.

2. If the model selected is **multimodal**, then select the pre-set image size to include in the benchmarking requests. There are three categories: Small (500x500px), Medium (1000x1000px) and Large (2000x2000px). Otherwise, if model is not multimodal, then leave the value to N/A. 

    **Warning!** Multimodal models may activate their guardrails when running benchmarks. Changing the input or output number of tokens may help to solve the issue.

3. Set the configuration parameters

- **Number of input tokens**: The number of input tokens in the generated prompt. *Default*: 1000.
- **Number of output tokens**: The number of output tokens the LLM can generate. *Default*: 1000.
- **Number of total requests**: Number of requests sent. *Default*: 10. *Note*: the program can timeout before all requests are sent. Configure the **Timeout** parameter accordingly.
- **Number of concurrent requests**: The number of concurrent requests. *Default*: 1. For testing [batching-enabled models](https://docs.sambanova.ai/sambastudio/latest/dynamic-batching.html), this value should be greater than the largest batch_size one needs to test. The typical batch sizes that are supported are 1,4,8 and 16.
- **Timeout**: Number of seconds before program times out. *Default*: 600 seconds

4. Run the performance evaluation

- Click the `Run!` button. This will start the program and a progress bar will be shown in the UI.
- Depending on the parameter configurations, it should take between 1 min and 10 min. 
- If needed, you can stop the process at any time by clicking the `Stop` button. **Warning** Stopping a run with multiple concurrent requests and a high number of input/output tokens may leave some requests still running on the API. To allow the API to recover, it's recommended to wait one to two minutes before starting a new run.
- Once the process is complete, you can download the output files to analyze performance metrics per request or review summary statistics.

5. Analyze results

    _Note: Not all model endpoints currently support the calculation of server-side statistics. Depending on your choice of endpoint, you may see either client and server information, or you may see just the client-side information._

    **Plots**

    The plots compare (if available) the following:

    - **Server metrics**: These are performance metrics from the Server API. 
    - **Client metrics**: These are performance metrics computed on the client side / local machine.
    
    Additionally, if the endpoint supports dynamic batching, the plots will show per-batch metrics.

    The results are composed of five plots:

    - ```Distribution of TTFT by batch size```: This bar plot shows the median Time to First Token (TTFT) in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see higher values and higher variance in the client-side metrics compared to the server-side metrics. This difference is mainly due to the request waiting in the queue to be served (for concurrent requests), which is not included in server-side metrics. 

    - ```Distribution of end-to-end latency by batch size```: This bar plot shows the median end-to-end latency in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see higher values and higher variance in the client-side metrics compared to the server-side metrics. This difference is also mainly due to the request waiting in the queue to be served (for concurrent requests), which is not included in server-side metrics. 

    - ```Distribution of output throughput by batch size``` plot: This bar plot shows the median number of **output** tokens per second per request in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see good agreement between the client and server-side metrics. For endpoints that support dynamic batching, one should see a decreasing trend in metrics as the batch size increases.

    - ```Total output throughput per batch size```: This bar plot shows the median total tokens generated per second per batch in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see good agreement between the client and server-side metrics. This metric will calculate the same values as the previous metric for batch size = 1. However, for batch size > 1, it is estimated as the average of ```Output throughput by batch size * Batch size``` for each batch, to account for more generated tokens due to concurrent requests being served in batch mode.

    - ```LLM requests across time```: This gantt plot shows the duration of the TTFT and end-to-end latency per request in a timeline. One should expect latencies considerably greater than TTFTs, and multiple bars starting at the same time as number of concurrent requests specified. In addition, if the endpoint allows dynamic batching, one could see grouped bars according to the batch sizes supported.

6. Customize synthetic prompts.

    Synthetic prompts for performance evaluation can be found [here](./prompts/). You are free to add, modify, or remove prompts as needed. If adding new prompts, please follow the data structure used in the existing ones as a reference.

7. Synthetic Performance common examples
    1. Dynamic batching speed  

        Parameters:
        - Model name: Bundle/Meta-Llama-3.3-70B-Instruct
        - API type: SambaStudio
        - Number of input tokens: 250
        - Number of output tokens: 250
        - Number of total requests: 80
        - Number of concurrent requests: 20
        - Timeout: 600
          
        Results:

        ![dynamic_batching_speed-ttft](./imgs/dynamic_batching_speed-ttft.png)
        ![dynamic_batching_speed-latency](./imgs/dynamic_batching_speed-latency.png)
        ![dynamic_batching_speed-output_throughput](./imgs/dynamic_batching_speed-output_throughput.png)
        ![dynamic_batching_speed-total_output_throughput](./imgs/dynamic_batching_speed-total_output_throughput.png)
        ![dynamic_batching_speed-gantt](./imgs/dynamic_batching_speed-gantt.png)
      
    2. High input and ouput tokens

        Parameters:
        - Model name: Bundle/Meta-Llama-3.3-70B-Instruct
        - API type: SambaStudio
        - Number of input tokens: 2000
        - Number of output tokens: 2000
        - Number of total requests: 80
        - Number of concurrent requests: 25
        - Timeout: 600

        Results:

        ![high_input_output_tokens-ttft](./imgs/high_input_output_tokens-ttft.png)
        ![high_input_output_tokens-latency](./imgs/high_input_output_tokens-latency.png)
        ![high_input_output_tokens-output_throughput](./imgs/high_input_output_tokens-output_throughput.png)
        ![high_input_output_tokens-total_output_throughput](./imgs/high_input_output_tokens-total_output_throughput.png)
        ![high_input_output_tokens-gantt](./imgs/high_input_output_tokens-gantt.png)

    3. Running multiple concurrent requests

        Parameters:
        - Model name: Bundle/Meta-Llama-3.3-70B-Instruct
        - API type: SambaStudio
        - Number of input tokens: 250
        - Number of output tokens: 250
        - Number of total requests: 1000
        - Number of concurrent requests: 100
        - Timeout: 600

        Results:
        
        ![running_multiple_concurrent_requests-ttft](./imgs/running_multiple_concurrent_requests-ttft.png)
        ![running_multiple_concurrent_requests-latency](./imgs/running_multiple_concurrent_requests-latency.png)
        ![running_multiple_concurrent_requests-output_throughput](./imgs/running_multiple_concurrent_requests-output_throughput.png)
        ![running_multiple_concurrent_requests-total_output_throughput](./imgs/running_multiple_concurrent_requests-total_output_throughput.png)
        ![running_multiple_concurrent_requests-gantt](./imgs/running_multiple_concurrent_requests-gantt.png)

  </details>

<details id="real-workload-performance-evaluation">
<summary><strong>Real Workload Performance Evaluation</strong></summary>

This option allows you to evaluate the performance of the selected LLM on real workload synthetic data generated by this benchmarking tool. 

![Real Workload Performance Evaluation](./imgs/real_workload_performance_eval.png) 

1. Enter a model name and choose the right API type

     _Note: Currently we have specific prompting support for Llama2, Llama3, Llama3.1, Llama3.2, Llama3.3, Llama4, Mistral, Deepseek, Qwen, QwQ, Solar, and Eeve. Other instruction models can work, but number of tokens may not be close to the ones specified._

  - If the model specified is a bundle, specify the desired expert in the Model Name text box with the prefix `Bundle`. 
    - For example, the Samba-1 Turbo Llama-3-8B expert in studio is titled `Meta-Llama-3-8B-Instruct` so the model name would be `Bundle/Meta-Llama-3-8B-Instruct`.
  - If the model comes from a SambaStudio endpoint using its OpenAI compatible URL or a standalone model, enter the full model name shown on the model card. E.g. `Meta-Llama-3.3-70B-Instruct`.
  - If the model is a SambaNova Cloud one, then use one of the models displayed in the website. Then, choose `SambaNova Cloud` in the API type dropdown option. E.g. `DeepSeek-R1`.

2. If the model selected is **multimodal**, then select the pre-set image size to include in the benchmarking requests. There are three categories: Small (500x500px), Medium (1000x1000px) and Large (2000x2000px). Otherwise, if model is not multimodal, then leave the value to N/A.

    **Warning!** Multimodal models may activate their guardrails when running benchmarks. Changing the input or output number of tokens may help to solve the issue.

3. Set the configuration parameters

- **Number of input tokens**: The number of input tokens in the generated prompt. *Default*: 1000.
- **Number of output tokens**: The number of output tokens the LLM can generate. *Default*: 1000.
- **Number of total requests**: Number of requests sent. *Default*: 10. *Note*: the program can timeout before all requests are sent. Configure the **Timeout** parameter accordingly.
- **Queries per second**: the number of queries that will be sent to the endpoint per second. Values QPS<10 are recommended since user can hit rate limits. *Default*: 1.0
- **Queries per second distribution**: the type of wait time distribution in between requests. User can choose the values 'constant', 'uniform', 'exponential'. *Default*: constant.
- **Timeout**: Number of seconds before program times out. *Default*: 600 seconds

4. Run the performance evaluation

- Click the `Run!` button. This will start the program and a progress bar will be shown in the UI.
- Depending on the parameter configurations, it should take between 1 min and 10 min. 
- If needed, you can stop the process at any time by clicking the `Stop` button. **Warning** Stopping a run with multiple concurrent requests and a high number of input/output tokens may leave some requests still running on the API. To allow the API to recover, it's recommended to wait one to two minutes before starting a new run.

5. Analyze results

    _Note: Not all model endpoints currently support the calculation of server-side statistics. Depending on your choice of endpoint, you may see either client and server information, or you may see just the client-side information._

    **Plots**

    The plots compare (if available) the following:

    - **Server metrics**: These are performance metrics from the Server API. 
    - **Client metrics**: These are performance metrics computed on the client side / local machine.
    
    Additionally, if the endpoint supports dynamic batching, the plots will show per-batch metrics.

    The results are composed of five plots:

    - ```Distribution of TTFT by batch size```: This bar plot shows the median Time to First Token (TTFT) in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see higher values and higher variance in the client-side metrics compared to the server-side metrics. This difference is mainly due to the request waiting in the queue to be served (for concurrent requests), which is not included in server-side metrics. 

    - ```Distribution of end-to-end latency by batch size```: This bar plot shows the median end-to-end latency in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see higher values and higher variance in the client-side metrics compared to the server-side metrics. This difference is also mainly due to the request waiting in the queue to be served (for concurrent requests), which is not included in server-side metrics. 

    - ```Distribution of output throughput by batch size``` plot: This bar plot shows the median number of **output** tokens per second per request in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see good agreement between the client and server-side metrics. For endpoints that support dynamic batching, one should see a decreasing trend in metrics as the batch size increases.

    - ```Total output throughput per batch size```: This bar plot shows the median total tokens generated per second per batch in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see good agreement between the client and server-side metrics. This metric will calculate the same values as the previous metric for batch size = 1. However, for batch size > 1, it is estimated as the average of ```Output throughput by batch size * Batch size``` for each batch, to account for more generated tokens due to concurrent requests being served in batch mode.

    - ```LLM requests across time```: This gantt plot shows the duration of the TTFT and end-to-end latency per request in a timeline. One should expect latencies considerably greater than TTFTs, and multiple bars starting at the same time as number of concurrent requests specified. In addition, if the endpoint allows dynamic batching, one could see grouped bars according to the batch sizes supported.

6. Customize synthetic prompts:

    Synthetic prompts for performance evaluation can be found [here](./prompts/). You are free to add, modify, or remove prompts as needed. If adding new prompts, please follow the data structure used in the existing ones as a reference.

</details>

<details id="custom-performance-evaluation">
<summary><strong>Custom Performance Evaluation</strong></summary>

This option allows you to evaluate the performance of the selected LLM on your own custom dataset. The interface should look like this:

![Custom Performance Evaluation](./imgs/custom_performance_eval.png) 

1. Prep your dataset

- The dataset needs to be in `.jsonl` format - these means a file with one JSON object per line. You can take as example the file [in here.](./prompts/custom_prompt_example.jsonl)
- Each JSON object need to have a `prompt` key with the value being the prompt you want to pass to the LLM. If you want to benchmark a multimodal model, then also include the `image_path` key with an absolute path value. **Warning!** Multimodal models may activate their guardrails when running benchmarks. Changing prompt may help to solve the issue.

2. Upload the jsonl file using the option in the sidebar.

3. Enter a model name and choose the right API type

    _Note: Currently we have specific prompting support for Llama2, Llama3, Llama3.1, Llama3.2, Llama3.3, Llama4, Mistral, Deepseek, Qwen, QwQ, Solar, and Eeve. Other instruction models can work, but number of tokens may not be close to the ones specified._

  - If the model specified is a bundle, specify the desired expert in the Model Name text box with the prefix `Bundle`. 
    - For example, the Samba-1 Turbo Llama-3-8B expert in studio is titled `Meta-Llama-3-8B-Instruct` so the model name would be `Bundle/Meta-Llama-3-8B-Instruct`.
  - If the model comes from a SambaStudio endpoint using its OpenAI compatible URL or a standalone model, enter the full model name shown on the model card.
  - If the model is a SambaNova Cloud one, then use one of the models displayed in the website. Then, choose `SambaNova Cloud` in the API type dropdown option. E.g. `DeepSeek-R1`.

4. Set the configuration and tuning parameters

- **Number of concurrent requests**: The number of concurrent requests. *Default*: 1. For testing [batching-enabled models](https://docs.sambanova.ai/sambastudio/latest/dynamic-batching.html), this value should be greater than the largest batch_size one needs to test. The typical batch sizes that are supported are 1,4,8 and 16.
- **Timeout**: Number of seconds before program times out. *Default*: 600 seconds
- **Max Output Tokens**: Maximum number of tokens to generate. *Default*: 256
- **Save LLM Responses**: Whether to save the actual outputs of the LLM to an output file. The output file will contain the `response_texts` suffix.

5. Analyze results

    _Note: Not all model endpoints currently support the calculation of server-side statistics. Depending on your choice of endpoint, you may see either client and server information, or you may see just the client-side information._

    **Plots**

    The plots compare (if available) the following:

    - **Server metrics**: These are performance metrics from the Server API. 
    - **Client metrics**: These are performance metrics computed on the client side / local machine.
    
    Additionally, if the endpoint supports dynamic batching, the plots will show per-batch metrics.

    The results are composed of five plots:

    - ```Distribution of end-to-end latency by batch size```: This bar plot shows the median end-to-end latency in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see higher values and higher variance in the client-side metrics compared to the server-side metrics. This difference is also mainly due to the request waiting in the queue to be served (for concurrent requests), which is not included in server-side metrics. 

    - ```Distribution of output throughput by batch size``` plot: This bar plot shows the median number of **output** tokens per second per request in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see good agreement between the client and server-side metrics. For endpoints that support dynamic batching, one should see a decreasing trend in metrics as the batch size increases.

    - ```Total output throughput per batch size```: This bar plot shows the median total tokens generated per second per batch in a bold colored horizontal line, and a rectangular area representing the range between the 5th and 95th percentile. One should see good agreement between the client and server-side metrics. This metric will calculate the same values as the previous metric for batch size = 1. However, for batch size > 1, it is estimated as the average of ```Output throughput by batch size * Batch size``` for each batch, to account for more generated tokens due to concurrent requests being served in batch mode.

    - ```LLM requests across time```: This gantt plot shows the duration of the TTFT and end-to-end latency per request in a timeline. One should expect latencies considerably greater than TTFTs, and multiple bars starting at the same time as number of concurrent requests specified. In addition, if the endpoint allows dynamic batching, one could see grouped bars according to the batch sizes supported.
</details>

<details id="performance-on-chat">
<summary><strong>Performance on Chat</strong></summary>

This option allows you to measure performance during a multi-turn conversation with an LLM. The interface should look like this:

![perf_on_chat_image](./imgs/performance_on_chat.png)

1. Enter a model name and choose the right API type

  - If the model specified is a bundle, specify the desired expert in the Model Name text box with the prefix `Bundle`. 
    - For example, the Samba-1 Turbo Llama-3-8B expert in studio is titled `Meta-Llama-3-8B-Instruct` so the model name would be `Bundle/Meta-Llama-3-8B-Instruct`.
  - If the model comes from a SambaStudio endpoint using its OpenAI compatible URL or a standalone model, enter the full model name shown on the model card.
  - If the model is a SambaNova Cloud one, then use one of the models displayed in the website. Then, choose `SambaNova Cloud` in the API type dropdown option. E.g. `DeepSeek-R1`.

2. If the model to benchmark is multimodal, then you can upload an image using the option in the sidebar. (Limit size: 200MB) **Warning!** Multimodal models may activate their guardrails when running benchmarks. Changing prompt may help to solve the issue.

3. Set the configuration parameters

- **Max tokens to generate**: Maximum number of tokens to generate. *Default*: 256
<!-- - **Do sample**: 
- **Repetition penalty**:
- **Temperature**:
- **Top k**:
- **Top p**: -->

4. Start the chat session

After entering the model name and configuring the parameters, press `Run!` to activate the chat session.

5. Ask anything and see results

Users are able to ask anything and get a generated answer to their questions, as shown in the image below. In addition to the back and forth conversations between the user and the LLM, there is a expander option that users can click to see the following metrics per each LLM response:
- **Latency (s)**
- **Throughput (tokens/s)**
- **Time to first token (s)** 

![perf_on_chat_image](./imgs/performance_on_chat_results.png)
</details>



## CLI Option

This method can be ran from a terminal session. Users have this option if they want to experiment using values that are beyond the limits specified in the Streamlit app parameters. You have 3 options for running the program from terminal:

<details id="synthetic-dataset">
<summary><strong>Synthetic Dataset</summary></strong>

_Note: Currently we have specific prompting support for Llama2, Llama3, Llama3.1, Llama3.2, Llama3.3, Llama4, Mistral, Deepseek, Qwen, QwQ, Solar, and Eeve. Other instruction models can work, but number of tokens may not be close to the ones specified._

1. Open the file `run_synthetic_dataset.sh` and configure the following parameters:
  - **model-name**: Model name to be used. See section `1. Enter a model name and choose the right API type` in [Synthetic Performance Evaluation](#synthetic-performance-evaluation) for more information about model name.
  - **llm-api**: API type to be chosen.
  - **results-dir**: Path to the results directory. _Default_: "./data/results/llmperf"
  - **num-concurrent-requests**: Number of concurrent requests. _Default_: 1
  - **timeout**: Timeout in seconds. _Default_: 600
  - **num-input-tokens**: Number of input tokens to include in the request prompts. It's recommended to choose no more than 2000 tokens to avoid long wait times. _Default_: 1000.
  - **num-output-tokens**: Number of output tokens in the generation. It is strongly recommended to set this value to no more than 2000, as most LLMs cannot generate outputs beyond this limit. _Default_: 1000.
  - **multimodal-image-size**: Size of the pre-set image to be used with a **multimodal** model. There are three categories: small (500x500px), medium (1000x1000px) and large (2000x2000px). **Warning!** Multimodal models may activate their guardrails when running benchmarks. Changing the input or output number of tokens may help to solve the issue. If model is not multimodal, then leave the value to na. _Default:_ na.
  - **num-requests**: Number of requests sent. _Default_: 16. _Note_: the program can timeout before all requests are sent. Configure the **Timeout** parameter accordingly.
  - **use-multiple-prompts**: Whether to use multiple prompts selected randomly or a single default prompt. This option is only available on **text instruct** models. Go to [this link](./prompts/user-prompt_template-text_instruct.yaml) if customization is needed. _Default_: False
  - **save-llm-responses**: Whether to save the actual outputs of the LLM to an output file. The output file will contain the `response_texts` suffix. _Default_: False
  - **use-debugging-mode**: Whether to use the debugging mode or not. WARNING: Debug mode will provide more detailed response at the cost of increased latency. _Default_: False

   _Note_: You should leave the `--mode` parameter untouched - this indicates what dataset mode to use.

2. Run the script

- Run the following command in your terminal:
```shell
sh run_synthetic_dataset.sh
```
- The evaluation process will start and a progress bar will be shown until it's complete.

3. Analyze results

- Results will be saved at the location specified in `results-dir`.
- The name of the output files will depend on the input file name, mode name, and number of concurrent requests. You should see files that follow a similar format to the following:

```
synthetic_<MODEL_IDX>_<MODEL_NAME><MULTIMODAL_SUFFIX>_{NUM_INPUT_TOKENS}_{NUM_OUTPUT_TOKENS}_{NUM_CONCURRENT_REQUESTS}_{MODE}
```

- For each run, two files are generated with the following suffixes in the output file names: `_individual_responses` and `_summary`.
  
  - Individual responses file

    - This output file contains the number of input and output tokens, number of total tokens, Time To First Token (TTFT), End-To-End Latency (E2E Latency) and Throughput from Server (if available) and Client side, for each individual request sent to the LLM. Users can use this data for further analysis. We provide this notebook `notebooks/analyze-results.ipynb` with some charts that they can use to start.

![individual_responses_image](./imgs/synthetic_individual_ouput.png)

  - Summary file

    - This file includes various statistics such as percentiles, mean and standard deviation to describe the number of input and output tokens, number of total tokens, Time To First Token (TTFT), End-To-End Latency (E2E Latency) and Throughput from Client side. It also provides additional data points that bring more information about the overall run, like inputs used, number of errors, and number of completed requests per minute. 

![summary_output_image](./imgs/synthetic_summary_ouput.png)

- There's an additional notebook `notebooks/multiple-models-benchmark.ipynb` that will help users on running multiple benchmarks with different experts and gather performance results in one single table. A Bundle endpoint is meant to be used for this analysis. 

4. Customize synthetic prompts

Synthetic prompts for performance evaluation can be found [here](./prompts/). You are free to add, modify, or remove prompts as needed. If adding new prompts, please follow the data structure used in the existing ones as a reference.

</details>

<details id="real-workload-dataset">
<summary><strong>Real Workload Dataset</summary></strong>

_Note: Currently we have specific prompting support for Llama2, Llama3, Llama3.1, Llama3.2, Llama3.3, Llama4, Mistral, Deepseek, Qwen, QwQ, Solar, and Eeve. Other instruction models can work, but number of tokens may not be close to the ones specified._

1. Open the file `run_real_workload_dataset.sh` and configure the following parameters:
  - **model-name**: Model name to be used. See section `1. Enter a model name and choose the right API type` in [Real Workload Evaluation](#real-workload-performance-evaluation) for more information about model name.
  - **llm-api**: API type to be chosen. 
  - **results-dir**: Path to the results directory. _Default_: "./data/results/llmperf"
  - **qps**: the number of queries that will be sent to the endpoint per second. Values QPS<10 are recommended since user can hit rate limits._Default_: 1
  - **qps-distribution**: the type of wait time distribution in between requests. User can choose the values 'constant', 'uniform', 'exponential'. _Default_: constant
  - **timeout**: Timeout in seconds. _Default_: 600
  - **num-input-tokens**: Number of input tokens to include in the request prompts. It's recommended to choose no more than 2000 tokens to avoid long wait times. _Default_: 1000.
  - **num-output-tokens**: Number of output tokens in the generation. It's recommended to choose no more than 2000 tokens to avoid long wait times. _Default_: 1000.
  - **multimodal-image-size**: Size of the pre-set image to be used with a **multimodal** model. There are three categories: small (500x500px), medium (1000x1000px) and large (2000x2000px). **Warning!** Multimodal models may activate their guardrails when running benchmarks. Changing the input or output number of tokens may help to solve the issue. If model is not multimodal, then leave the value to na. _Default:_ na.
  - **num-requests**: Number of requests sent. _Default_: 16. _Note_: the program can timeout before all requests are sent. Configure the **Timeout** parameter accordingly.
  - **use-debugging-mode**: Whether to use the debugging mode or not. WARNING: Debug mode will provide more detailed response at the cost of increased latency. _Default_: False

   _Note_: You should leave the `--mode` parameter untouched - this indicates what dataset mode to use.

2. Run the script

- Run the following command in your terminal:
```shell
sh run_real_workload_dataset.sh
```
- The evaluation process will start and a progress bar will be shown until it's complete.

3. Analyze results

- Results will be saved at the location specified in `results-dir`.
- The name of the output files will depend on the input file name, mode name, and number of concurrent requests. You should see files that follow a similar format to the following:

```
realworkload_<MODEL_IDX>_<MODEL_NAME><MULTIMODAL_SUFFIX>_{NUM_INPUT_TOKENS}_{NUM_OUTPUT_TOKENS}_{QPS}_{QPS_DISTRIBUTION}_{MODE}
```

- For each run, two files are generated with the following suffixes in the output file names: `_individual_responses` and `_summary`.
  
  - Individual responses file

    - This output file contains the number of input and output tokens, number of total tokens, Time To First Token (TTFT), End-To-End Latency (E2E Latency) and Throughput from Server (if available) and Client side, for each individual request sent to the LLM. Users can use this data for further analysis. We provide this notebook `notebooks/analyze-results.ipynb` with some charts that they can use to start.

![individual_responses_image](./imgs/real_workload_individual_ouput.png)

  - Summary file

    - This file includes various statistics such as percentiles, mean and standard deviation to describe the number of input and output tokens, number of total tokens, Time To First Token (TTFT), End-To-End Latency (E2E Latency) and Throughput from Client side. It also provides additional data points that bring more information about the overall run, like inputs used, number of errors, and number of completed requests per minute. 

![summary_output_image](./imgs/real_workload_summary_ouput.png)

- There's an additional notebook `notebooks/multiple-models-benchmark.ipynb` that will help users on running multiple benchmarks with different experts and gather performance results in one single table. A Bundle endpoint is meant to be used for this analysis. 

4. Customize synthetic prompts

Synthetic prompts for performance evaluation can be found [here](./prompts/). You are free to add, modify, or remove prompts as needed. If adding new prompts, please follow the data structure used in the existing ones as a reference.

</details>

<details id="custom-dataset">
<summary><strong>Custom Dataset</summary></strong>

_Note: Currently we have specific prompting support for Llama2, Llama3, Llama3.1, Llama3.2, Llama3.3, Llama4, Mistral, Deepseek, Qwen, QwQ, Solar, and Eeve. Other instruction models can work, but number of tokens may not be close to the ones specified._

1. Open the file `run_custom_dataset.sh` and configure the following parameters:
  - **model-name**: Model name to be used. See section `1. Enter a model name and choose the right API type` in [Synthetic Performance Evaluation](#synthetic-performance-evaluation) for more information about model name.
  - **llm-api**: API type to be chosen.
  - **results-dir**: Path to the results directory. _Default_: "./data/results/llmperf"
  - **num-concurrent-requests**: Number of concurrent requests. _Default_: 1
  - **timeout**: Timeout in seconds. _Default_: 600
  - **input-file-path**: The location of the custom dataset that you want to evaluate with. You can take as example the file [in here.](./prompts/custom_prompt_example.jsonl)
  - **save-llm-responses**: Whether to save the actual outputs of the LLM to an output file. The output file will contain the `response_texts` suffix.
  - **use-debugging-mode**: Whether to use the debugging mode or not. WARNING: Debug mode will provide more detailed response at the cost of increased latency. _Default_: False

  _Note_: You should leave the `--mode` parameter untouched - this indicates what dataset mode to use. 

2. Run the script

- Run the following command in your terminal:
```shell
sh run_custom_dataset.sh
```
- The evaluation process will start and a progress bar will be shown until it's complete.

3. Analyze results

- Results will be saved at the location specified in `results-dir`.
- The name of the output files will depend on the input file name, mode name, and number of concurrent requests. You should see files that follow a similar format to the following:

```
custom_<MODEL_NAME>_{FILE_NAME}_{NUM_CONCURRENT_REQUESTS}_{MODE}
```

- For each run, two files are generated with the following suffixes in the output file names: `_individual_responses` and `_summary`.
  
  - Individual responses file

    - This output file contains the number of input and output tokens, number of total tokens, Time To First Token (TTFT), End-To-End Latency (E2E Latency) and Throughput from Server (if available) and Client side, for each individual request sent to the LLM. Users can use this data for further analysis. We provide this notebook `notebooks/analyze-results.ipynb` with some charts that they can use to start.

![individual_responses_image](./imgs/custom_individual_ouput.png)

  - Summary file

    - This file includes various statistics such as percentiles, mean and standard deviation to describe the number of input and output tokens, number of total tokens, Time To First Token (TTFT), End-To-End Latency (E2E Latency) and Throughput from Client side. It also provides additional data points that bring more information about the overall run, like inputs used, number of errors, and number of completed requests per minute. 

![summary_output_image](./imgs/custom_summary_ouput.png)
</details>

# Third-party tools and data sources 

All the packages/tools are listed in the `requirements.txt` file in the project directory.

# TroubleShooting

1. ModuleNotFoundError: No module named 'st_pages'

If you come across this error, please restart your vscode or venv in order for the module to be found.

<!-- JS scripts -->
<script>
function openDetails(id) {
  const details = document.getElementById(id);
  if (details && !details.open) {
    details.open = true;  
  }
}
</script>