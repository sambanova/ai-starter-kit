{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read benchmarking output files\n",
    "Responses with errors will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_files(folder_path):\n",
    "    data = []\n",
    "    \n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file ends with 'individual_responses.json'\n",
    "        if filename.endswith('individual_responses.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Open and load the JSON file\n",
    "            with open(file_path, 'r') as file:\n",
    "                try:\n",
    "                    json_data = json.load(file)\n",
    "                    # Reading responses but skipping the ones that have an error code\n",
    "                    json_data = [{**request_response, 'filename': filename} for request_response in json_data if request_response['error_code'] is None]\n",
    "                    data.append(json_data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths to the results directories of each provider\n",
    "provider_results_dir_paths = [\n",
    "    '../data/results/path/provider1',\n",
    "    '../data/results/path/provider2',\n",
    "    '../data/results/path/provider3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = []\n",
    "for provide_path in provider_results_dir_paths:\n",
    "    provider_identifier = provide_path.split('/')[-1]\n",
    "    provider_responses = {\n",
    "        'provider': provider_identifier,\n",
    "        'responses': read_json_files(provide_path)\n",
    "    }\n",
    "    all_responses.append(provider_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_stats = []\n",
    "for run in all_responses:\n",
    "    metrics = {\n",
    "        'provider': [],\n",
    "        'filename': [],\n",
    "        'model': [],\n",
    "        'input_tokens': [],\n",
    "        'output_tokens': [],\n",
    "        'concurrent_requests': [], \n",
    "        'server_number_input_tokens': [],\n",
    "        'server_number_output_tokens': [],\n",
    "        'server_ttft_s': [], \n",
    "        'server_output_token_per_s_per_request': [],\n",
    "        'server_end_to_end_latency_s': [],\n",
    "        'client_ttft_s': [], \n",
    "        'client_output_token_per_s_per_request': [], \n",
    "        'client_end_to_end_latency_s': []\n",
    "    }\n",
    "    \n",
    "    # Read responses\n",
    "    provider = run['provider']\n",
    "    for requests_from_file in run['responses']:\n",
    "        for request_metrics in requests_from_file:\n",
    "            metrics['provider'].append(provider)\n",
    "            metrics['filename'].append(request_metrics['filename'])\n",
    "            \n",
    "            model_name = request_metrics['filename'].split('_')[3]\n",
    "            input_tokens = int(request_metrics['filename'].split('_')[4])\n",
    "            output_tokens = int(request_metrics['filename'].split('_')[5])\n",
    "            concurrent_requests = int(request_metrics['filename'].split('_')[6])\n",
    "            \n",
    "            metrics['model'].append(model_name)\n",
    "            metrics['input_tokens'].append(input_tokens)\n",
    "            metrics['output_tokens'].append(output_tokens)\n",
    "            metrics['concurrent_requests'].append(concurrent_requests)\n",
    "\n",
    "            metrics['server_number_input_tokens'].append(request_metrics['server_number_input_tokens'])\n",
    "            metrics['server_number_output_tokens'].append(request_metrics['server_number_output_tokens'])\n",
    "            metrics['server_ttft_s'].append(request_metrics['server_ttft_s'])\n",
    "            metrics['server_output_token_per_s_per_request'].append(request_metrics['server_output_token_per_s_per_request'])\n",
    "            metrics['server_end_to_end_latency_s'].append(request_metrics['server_end_to_end_latency_s'])\n",
    "            metrics['client_ttft_s'].append(request_metrics['client_ttft_s'])\n",
    "            metrics['client_output_token_per_s_per_request'].append(request_metrics['client_output_token_per_s_per_request'])\n",
    "            metrics['client_end_to_end_latency_s'].append(request_metrics['client_end_to_end_latency_s'])\n",
    "\n",
    "    df_metrics =  pd.DataFrame(metrics)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    df_metric_stats = df_metrics.groupby(by='filename')[[\n",
    "        'server_ttft_s',\n",
    "        'server_output_token_per_s_per_request',\n",
    "        'server_end_to_end_latency_s',\n",
    "        'client_ttft_s',\n",
    "        'client_output_token_per_s_per_request',\n",
    "        'client_end_to_end_latency_s'\n",
    "    ]].agg(['median','std'])\n",
    "    df_metric_stats.columns = ['_'.join(col).strip() for col in df_metric_stats.columns.values]\n",
    "    df_metric_stats.style.format(\"{:,.3f}\")\n",
    "    \n",
    "    # Calculate parameters\n",
    "    df_parameters = df_metrics.groupby(by='filename')[[\n",
    "        'provider',\n",
    "        'model',\n",
    "        'input_tokens',\n",
    "        'output_tokens',\n",
    "        'concurrent_requests'\n",
    "    ]].agg(['first'])    \n",
    "    df_parameters.columns = ['_'.join(col).strip() for col in df_parameters.columns.values]\n",
    "    df_parameters.columns = [col.split('_')[0] for col in df_parameters.columns.values]\n",
    "    df_parameters.style.format(\"{:,.0f}\")\n",
    "    \n",
    "    request_count = df_metrics.groupby(by='filename')[['provider']].count().rename(columns={'provider': 'request_count'})\n",
    "    request_count.style.format(\"{:,.0f}\")\n",
    "    \n",
    "    # Merge statistics and parameters\n",
    "    df = pd.concat([df_parameters, request_count, df_metric_stats], axis=1)\n",
    "    run_stats.append(df)\n",
    "    \n",
    "results = pd.concat(run_stats)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter runs based on analysis objective (Optional)\n",
    "\n",
    "In this example, we'll just filter Llama 70B models for input tokens 100, 1k and 10k, and concurrent requests 1 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.model = results.model.str.lower()\n",
    "results_70b = results[(results.model.str.contains('llama-3-1-70b')) & (results.input.isin([100,1_000,10_000])) & (results.concurrent.isin([1,10]))]\n",
    "results_70b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot benchmarking charts among providers\n",
    "\n",
    "You may change the pallette color based on the color that better identifies each provider. You can take the [following link](https://seaborn.pydata.org/tutorial/color_palettes.html) as reference. Also, you will need to update the suptitle to reflect the model you're showing and any other detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pallette for the providers, change colors and provider names as needed\n",
    "palette = {'provider1': 'orange', 'provider2': 'tab:purple', 'provider3': 'xkcd:blue'}\n",
    "\n",
    "# Get unique concurrent values\n",
    "concurrent_values = results_70b['concurrent'].unique()\n",
    "concurrent_values.sort()\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(len(concurrent_values), 1, figsize=(10, 5 * len(concurrent_values)), sharex=False)\n",
    "\n",
    "# Add a supertitle, it could be the model name\n",
    "fig.suptitle('<<Model_name>>', fontsize=20)\n",
    "\n",
    "# Plot each concurrent value\n",
    "for ax, concurrent in zip(axes, concurrent_values):\n",
    "    subset = results_70b[results_70b['concurrent'] == concurrent]\n",
    "    sns.barplot(data=subset, x='input', y='client_ttft_s_median', hue='provider', ax=ax, palette=palette, errorbar=None)\n",
    "    ax.set_title(f'Client TTFT (s) for Concurrent Requests: {concurrent}')\n",
    "    ax.set_xlabel('Input Tokens')\n",
    "    ax.set_ylabel('Client TTFT (s)')\n",
    "    ax.annotate('Note: A lower TTFT is better',\n",
    "            xy = (0.5, -0.2),\n",
    "            xycoords='axes fraction',\n",
    "            ha='center',\n",
    "            va=\"center\",\n",
    "            fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique concurrent values\n",
    "concurrent_values = results_70b['concurrent'].unique()\n",
    "concurrent_values.sort()\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(len(concurrent_values), 1, figsize=(10, 5 * len(concurrent_values)), sharex=False)\n",
    "\n",
    "# Add a supertitle, it could be the model name\n",
    "fig.suptitle('<<Model_name>>', fontsize=20)\n",
    "\n",
    "# Plot each concurrent value\n",
    "for ax, concurrent in zip(axes, concurrent_values):\n",
    "    subset = results_70b[results_70b['concurrent'] == concurrent]\n",
    "    sns.barplot(data=subset, x='input', y='client_end_to_end_latency_s_median', hue='provider', ax=ax, palette=palette, errorbar=None)\n",
    "    ax.set_title(f'Client E2E Latency (s) for Concurrent Requests: {concurrent}')\n",
    "    ax.set_xlabel('Input Tokens')\n",
    "    ax.set_ylabel('Client E2E Latency (s)')\n",
    "    ax.annotate('Note: A lower Latency is better',\n",
    "        xy = (0.5, -0.2),\n",
    "        xycoords='axes fraction',\n",
    "        ha='center',\n",
    "        va=\"center\",\n",
    "        fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens / sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique concurrent values\n",
    "concurrent_values = results_70b['concurrent'].unique()\n",
    "concurrent_values.sort()\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(len(concurrent_values), 1, figsize=(10, 5 * len(concurrent_values)), sharex=False)\n",
    "\n",
    "# Add a supertitle, it could be the model name\n",
    "fig.suptitle('<<Model_name>>', fontsize=20)\n",
    "\n",
    "# Plot each concurrent value\n",
    "for ax, concurrent in zip(axes, concurrent_values):\n",
    "    subset = results_70b[results_70b['concurrent'] == concurrent]\n",
    "    sns.barplot(data=subset, x='input', y='client_output_token_per_s_per_request_median', hue='provider', ax=ax, palette=palette, errorbar=None)\n",
    "    ax.set_title(f'Client Tokens/sec per request for Concurrent Requests: {concurrent}')\n",
    "    ax.set_xlabel('Input Tokens')\n",
    "    ax.set_ylabel('Client Tokens/sec per request')\n",
    "    ax.annotate('Note: higher Tokens/sec is better',\n",
    "        xy = (0.5, -0.2),\n",
    "        xycoords='axes fraction',\n",
    "        ha='center',\n",
    "        va=\"center\",\n",
    "        fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique concurrent values\n",
    "concurrent_values = results_70b['concurrent'].unique()\n",
    "concurrent_values.sort()\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(len(concurrent_values), 1, figsize=(10, 5 * len(concurrent_values)), sharex=False)\n",
    "\n",
    "# Add a supertitle, it could be the model name\n",
    "fig.suptitle('<<Model_name>>', fontsize=20)\n",
    "\n",
    "# Plot each concurrent value\n",
    "results_70b['client_throughput_median'] = results_70b['concurrent']*results_70b['client_output_token_per_s_per_request_median']\n",
    "for ax, concurrent in zip(axes, concurrent_values):\n",
    "    subset = results_70b[results_70b['concurrent'] == concurrent]\n",
    "    sns.barplot(data=subset, x='input', y='client_throughput_median', hue='provider', ax=ax, palette=palette, errorbar=None)\n",
    "    ax.set_title(f'Client Throughput (tok/s) for Concurrent Requests: {concurrent}')\n",
    "    ax.set_xlabel('Input Tokens')\n",
    "    ax.set_ylabel('Client Throughput (tok/s)')\n",
    "    ax.annotate('Note: A higher Throughput is better',\n",
    "        xy = (0.5, -0.2),\n",
    "        xycoords='axes fraction',\n",
    "        ha='center',\n",
    "        va=\"center\",\n",
    "        fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_venv",
   "language": "python",
   "name": "benchmark_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
