{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4129f9b5",
   "metadata": {},
   "source": [
    "# Edgar Q&A Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99735fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from langchain.chains import RetrievalQA, LLMChain, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory, ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from utils.model_wrappers.api_gateway import APIGateway\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(repo_dir,'.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e9ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIRECTORY = os.path.join(kit_dir,'data/vectordbs/tsla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c3804",
   "metadata": {},
   "source": [
    "### Load embedding model and SEC vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79554c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name='intfloat/e5-large-v2',\n",
    "    embed_instruction=\"\",  # no instruction is needed for candidate passages\n",
    "    query_instruction=\"Represent the query for retrieval: \",\n",
    "    encode_kwargs=encode_kwargs,\n",
    "\n",
    ")\n",
    "vectordb = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa297fb1",
   "metadata": {},
   "source": [
    "### Choose LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51af6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SambaNovaCloud\n",
    "api_type = \"sncloud\"\n",
    "llm_expert = 'llama3-8b'\n",
    "\n",
    "# Using SambaStudio\n",
    "# api_type = \"sambastudio\"\n",
    "# llm_expert = 'Meta-Llama-3-70B-Instruct-4096'\n",
    "\n",
    "# Set gateway\n",
    "llm = APIGateway.load_llm(\n",
    "    type=\"sncloud\",\n",
    "    streaming=False,\n",
    "    coe=True,\n",
    "    max_tokens_to_generate=512,\n",
    "    temperature=0.0,\n",
    "    select_expert='llama3-8b',\n",
    ")\n",
    "\n",
    "llm.invoke(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c1f57",
   "metadata": {},
   "source": [
    "### Retrieve information from vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    #chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    input_key=\"question\",\n",
    "    output_key=\"response\",\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "custom_prompt_template = \"\"\"\n",
    "You're an expert in filing reports\\n\\n \n",
    "Given the following context enclosed in backticks regarding a company annual/quarterly report filing:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "Consider the question:  \n",
    "{question}\n",
    "Answer the question using only the information from the context. If the answer to the question can't be extracted from the preovious context, then say \"I do not have information regarding this\".\n",
    "Helpful Answer:\"\"\"\n",
    "CUSTOMPROMPT = PromptTemplate(\n",
    "    template=custom_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "## Inject custom prompt\n",
    "qa.combine_documents_chain.llm_chain.prompt = CUSTOMPROMPT\n",
    "query = \"what are the products and services that the company has?\"\n",
    "response = qa({\"question\": query})\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cd358e",
   "metadata": {},
   "source": [
    "### Q&A chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dcb221",
   "metadata": {},
   "source": [
    "The following image shows the general idea of the architecture for this retriever chatbot. </br>\n",
    "More resources about how chatbots are implemented with langchain [here](https://python.langchain.com/docs/use_cases/chatbots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935966fa",
   "metadata": {},
   "source": [
    "![retriever_chatbot_architecture](https://python.langchain.com/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df36d9e",
   "metadata": {},
   "source": [
    "### Chatbot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a helpful assistant. Answer questions with the best of your capabilities and based on the chat history.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"<s>[INST] {question} [/INST]\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
    "\n",
    "conversation({\"question\": \"hi, my name is Rodrigo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8dfb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"can you tell me about the benefits of using AI in society?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"could you tell me more details about point 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"Great, what was my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57b1a2",
   "metadata": {},
   "source": [
    "### Chatbot with Summary and SEC retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(response: dict) -> None:\n",
    "    \n",
    "    print('\\nQUESTION:')\n",
    "    print(response['question'])\n",
    "    \n",
    "    print('\\nCONTEXT:')\n",
    "    for document in response['source_documents']:\n",
    "        pprint(document.page_content, width=180)\n",
    "        print()\n",
    "    \n",
    "    print('\\nCHAT HISTORY:')\n",
    "    for message in response['chat_history']:\n",
    "        message.pretty_print()\n",
    "        \n",
    "    print('\\nANSWER:')\n",
    "    print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c01517",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_condensed_question_template = \"\"\"You're an assitant\n",
    "Given the following chat history and follow up question, rephrase the follow up question using the chat history. Only output the rephrased question.\n",
    "\n",
    "Chat history:\n",
    "{chat_history}\n",
    "\n",
    "Follow up question: {question}\n",
    "\n",
    "Rephrased question:\"\"\"\n",
    "\n",
    "custom_condensed_question_prompt = PromptTemplate.from_template(custom_condensed_question_template)\n",
    "\n",
    "custom_qa_template = \"\"\"\n",
    "You're a helpful assistant in filing reports. Follow the following rules:\n",
    "1. If you don't know the answer, respond kindly that you don't have information about it. \n",
    "2. Do not try to make up an answer.\n",
    "3. Start the conversation expressing general assitance in anything the user would like to ask.\n",
    "Given the following context and follow up question, provide a helpful answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Follow up question: {question}\n",
    "\n",
    "Helpful answer:\"\"\"\n",
    "\n",
    "custom_qa_prompt = PromptTemplate.from_template(custom_qa_template)\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm, \n",
    "    max_token_limit=50,\n",
    "    buffer=\"The human and AI greet each other.\",\n",
    "    output_key='answer',\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm, \n",
    "    retriever=retriever, \n",
    "    memory=memory, \n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True, \n",
    "    verbose=True,\n",
    "    condense_question_prompt = custom_condensed_question_prompt,\n",
    "    combine_docs_chain_kwargs={'prompt': custom_qa_prompt}\n",
    ")\n",
    "\n",
    "response = qa({\"question\": \"what are the biggest risk that the company is facing?\"})\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa({\"question\": \"Tell me about the business of the company, what products and services does it offer?\"})\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa({\"question\": \"tell me more about the products that Tesla offers based on the report\"})\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd2c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edgar_venv",
   "language": "python",
   "name": "edgar_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
