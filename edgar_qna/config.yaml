api: "sambaverse" # set either sambastudio or sambaverse

embedding_model: 
    "type": "sambastudio" # set either sambastudio or cpu
    "batch_size": 1 #set depending of your endpoint configuration (1 if CoE embedding expert)
    "coe": True #set true if using Sambastudio embeddings in a CoE endpoint 
    "select_expert": "e5-mistral-7b-instruct" #set if using SambaStudio CoE embedding expert

llm: 
    "temperature": 0.1
    "max_tokens_to_generate": 1000
    "do_sample": False
    "sambaverse_model_name": "Meta/llama-2-70b-chat-hf"
    "coe": True  #set as true if using Sambastudio CoE endpoint
    "select_expert": "llama-2-70b-chat-hf" #set if using Sambaverse or SambaStudio CoE llm expert
    #fastApi CoE expert name -> "llama3-8b"

retrieval:
    "chunk_size": 500
    "chunk_overlap": 50
    "db_type": "chroma"
    "n_retrieved_documents": 3

query_decomposition:
    "n_generated_subquestions": 3

sec:
    "email": "mlengineer@snova_dummy.ai"
    "company": "snova_dummy"
    "report_type": "10-K"
    "last_n_documents": 1
