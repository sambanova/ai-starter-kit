{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Executing Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir =  os.path.abspath(os.path.join(current_dir, '..'))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, '..'))\n",
    "sys.path.append(repo_dir)\n",
    "\n",
    "from utils.dedicated_env.snsdk_wrapper import SnsdkWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step / Manual setting\n",
    "\n",
    "First instantiate the SambaStudio client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 12:48:48,995 [INFO] Using variables from Snapi config to set up Snsdk.\n"
     ]
    }
   ],
   "source": [
    "sambastudio_client = SnsdkWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List trainable models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta-llama-3-8b-instruct-128384-vocab',\n",
       " 'E5 Large V2',\n",
       " 'GPT13B 2k SS ITv3',\n",
       " 'GPT_1.5B_GT_Finetuned',\n",
       " 'Multilingual E5 Large',\n",
       " 'GPT_1.5B_Base_Model',\n",
       " 'CLIP-ViT-B-32-laion2B-s34B-b79k',\n",
       " 'CLIP ViT-B-32 Backbone (Deprecated)',\n",
       " 'meta-llama-3.1-70b',\n",
       " 'llava-v1.5-7b',\n",
       " 'Multilingual E5 Large Instruct',\n",
       " 'Hubert_ASR',\n",
       " 'GPT_1.5B_GT_Pretrained',\n",
       " 'Suzume-Llama-3-8B-Multilingual-Publichealth',\n",
       " 'FakeBox',\n",
       " 'Deepseek-coder-6.7b-instruct',\n",
       " 'RC4_VIEW_TEST',\n",
       " 'SimpleTextClassGenerativeTrained',\n",
       " 'RC4_Colab_Test',\n",
       " 'Suzume-Llama-3-8B-Multilingual',\n",
       " 'Deepseek-coder-6.7b-base',\n",
       " 'meta-llama-3.1-70b-instruct',\n",
       " 'meta-llama-3.1-8b-instruct',\n",
       " '1218_SN10_GPT13B 8k SS HAv3',\n",
       " 'GPT_13B_Human_Aligned_Instruction_Tuned_V2',\n",
       " '1218_meta-llama-3.1-8b',\n",
       " 'meta-llama-3.1-8b',\n",
       " '1218_meta-llama-3-70b-128256-vocab',\n",
       " 'Llama-2-7b-16k-hf',\n",
       " 'YANZHEC_TEST_SNAPI_GPT1.5B_GT_Finetuned',\n",
       " 'meta-llama-3-8b-instruct-128256-vocab',\n",
       " 'GPT_1.5B_Dialog_Act_Classification_Finetuned',\n",
       " 'LlamaGuard_7b',\n",
       " 'meta-llama-3-70b-instruct-128256-vocab',\n",
       " 'Thai_LLaMA_70B',\n",
       " 'GPT_13B_Generative_Inference',\n",
       " 'Llama-2-13b-hf',\n",
       " 'meta-llama-3-8b-nan-generator',\n",
       " 'meta-llama-guard-2-8b-128384-vocab',\n",
       " 'Zephyr-7B-Beta',\n",
       " 'Llama-2-7b-chat-hf',\n",
       " 'GPT_13B_Base_Model',\n",
       " 'Llama-2-7b-sambalingo-thai-base-hf',\n",
       " 'GPT_13B_GT_Base_Model_300k_MaxVocabSize',\n",
       " 'meta-llama-guard-2-8b-128256-vocab',\n",
       " 'Llama-2-70b-hf',\n",
       " 'Llama 2 7B base 8-socket',\n",
       " 'mistral-7b-instruct-v0.2',\n",
       " 'Llama-2-7b-sambalingo-thai-chat-hf',\n",
       " 'Llama 2 7B chat 8-socket',\n",
       " 'Llama-2-7b-hf',\n",
       " 'Llama-2-13b-chat-hf',\n",
       " 'llava-v1.5-7b (deprecated)',\n",
       " 'GPT 13B 8k SS SN Pretrained',\n",
       " 'GPT_13B_Dialog_Summarization_Finetuned',\n",
       " 'Llama-2-70-16k-hf',\n",
       " 'GPT13B 8k SS HAv3',\n",
       " 'GPT_13B_Instruction_Tuned_V2',\n",
       " 'GPT13B 8k SS ITv3',\n",
       " 'llama-2-70b-chat-hf',\n",
       " 'Meta-Llama-3-8B-Instruct',\n",
       " 'law-chat',\n",
       " 'meta-llama-3-70b-128256-vocab',\n",
       " 'Llama-2-7b-chat-hf',\n",
       " 'NSQL-Llama-2-7B',\n",
       " 'Test Upload',\n",
       " 'Llama-2-70b-chat-hf',\n",
       " 'GPT13B 2k SS HAv3',\n",
       " 'Llama-2-70b-chat-16k-hf',\n",
       " 'law-chat',\n",
       " 'mistral-7b-v0.1',\n",
       " 'meta-llama-3-70b-instruct-128384-vocab',\n",
       " 'Sarashina2-7b',\n",
       " 'llama-2-7B-chat-hf',\n",
       " 'meta-llama-3-70b-128384-vocab',\n",
       " 'Meta-Llama-3-70B-Instruct',\n",
       " 'Sarashina2-70b',\n",
       " 'Llama-2-13B-chat-hf',\n",
       " 'Llama-2-7b-chat-16k-hf',\n",
       " 'meta-llama-3-8b-128256-vocab',\n",
       " 'meta-llama-3-8b-128384-vocab',\n",
       " 'Sarashina2-7B',\n",
       " 'Sarashina2-70B',\n",
       " 'GPT_1.5B_NER_Finetuned']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model[\"model_checkpoint_name\"]for model in sambastudio_client.list_models(filter_job_types=[\"train\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Suzume-Llama-3-8B-Multilingual'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openthaigpt_50k_IT0913',\n",
       " 'Generative_Inference_Dataset',\n",
       " 'GPT_13B_Inference_Dataset',\n",
       " 'FiQA',\n",
       " 'Super_Glue_4k_SS',\n",
       " 'E5_Large_V2_Inference_Example',\n",
       " 'ASR_With_Diarization_Dataset',\n",
       " 'Restore_Punctuation_Data',\n",
       " 'ASR_Without_Diarization_Dataset',\n",
       " 'GPT_13B_8k_SS_Toy_Training_Dataset',\n",
       " 'Librispeech',\n",
       " 'GPT_1.5B_Training_Dataset',\n",
       " 'GPT_13B_Training_Dataset',\n",
       " 'Speaker_Diarization',\n",
       " 'test',\n",
       " 'Coding_Generative_Train_4k_SS_Dataset',\n",
       " 'Mistral_Tokenized_Copa',\n",
       " 'thai-dpo-sft-ss4k',\n",
       " 'RBAC_Test_Curl',\n",
       " 'test_upload',\n",
       " 'Coding_Generative_Inference_Dataset',\n",
       " 'E5_Large_V2_Training_MSMarco_Distillation',\n",
       " 'console_upload',\n",
       " 'Super_Glue_8k_SS_128k_vocab',\n",
       " 'yc_snapi_add_localmachine_test_13B_2451_rc3',\n",
       " '0606qa03orgadmin',\n",
       " 'LLaVA-example',\n",
       " 'openwebtext_ss4096_32k_vocab',\n",
       " 'aniket-e5-dataset-upload-trial5',\n",
       " 'GPT_1.5B_Inference_Dataset',\n",
       " 'test_dataset',\n",
       " 'Super_Glue_16k_SS',\n",
       " 'Caltech_256_Clip',\n",
       " 'Superglue_Sarashina_4k',\n",
       " 'Superglue_Sarashina_8k',\n",
       " '1029test',\n",
       " '1113AWS',\n",
       " 'smol_sql_dataset',\n",
       " 'Superglue_Llama3p1_8k_SS',\n",
       " 'publichealth']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dataset[\"dataset_name\"] for dataset in sambastudio_client.list_datasets()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'publichealth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Project configs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = {\n",
    "    'project_name': 'byoc-fine-tuning-project',\n",
    "    'project_description': 'this project will be used to test the BYOC and Fine-tuning e2e pipeline implementation'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 12:48:56,862 [INFO] Project with name 'byoc fine-tuning project' found with id 296e0c05-1338-4119-9ef6-0a5667b55b09\n",
      "2024-12-19 12:48:56,866 [INFO] Project with name 'byoc fine-tuning project' already exists with id '296e0c05-1338-4119-9ef6-0a5667b55b09', using it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'296e0c05-1338-4119-9ef6-0a5667b55b09'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the create project method from client with project parameters\n",
    "sambastudio_client.create_project(\n",
    "    project_name = project['project_name'],\n",
    "    project_description = project['project_description']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set train job config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 12:48:58,118 [INFO] Default Hyperparameters for train in SN40L-8 for Suzume-Llama-3-8B-Multilingual: \n",
      "\n",
      "                    ['batch_size:`8`', 'debug_mode:`off`', 'do_eval:`false`', 'dump_inputs:`false`', 'eval_steps:`50`', 'evaluation_strategy:`no`', 'fix_rank_rdu_mapping:`false`', 'grad_accumulation_steps:`1`', 'learning_rate:`1.0e-05`', 'logging_steps:`1`', 'lr_schedule:`fixed_lr`', 'max_seq_length:`8192`', 'model_parallel_rdus:`1`', 'model_parameter_count:`8b`', 'num_iterations:`100`', 'prompt_loss_weight:`0.0`', 'run_mode:`balanced`', 'save_optimizer_state:`true`', 'save_steps:`50`', 'skip_checkpoint:`false`', 'subsample_eval:`0.01`', 'subsample_eval_seed:`123`', 'use_token_type_ids:`true`', 'vocab_size:`128256`', 'warmup_steps:`0`', 'weight_decay:`0.1`']\n",
      "\n",
      "                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SN40L-8': [{'constrains': None,\n",
      "              'description': 'The per-worker batch size',\n",
      "              'field_name': 'batch_size',\n",
      "              'settings': {'DEFAULT': '8', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': None,\n",
      "              'description': \"Toggles debug mode. Debug mode 'on' mode turns \"\n",
      "                             'on additional output to help diagnose certain '\n",
      "                             \"issues during training. Please keep 'off' unless \"\n",
      "                             'advised otherwise by your SambaNova admin.',\n",
      "              'field_name': 'debug_mode',\n",
      "              'settings': {'DEFAULT': 'off', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': ['true', 'false']},\n",
      "              'description': 'whether or not to do final evaluation',\n",
      "              'field_name': 'do_eval',\n",
      "              'settings': {'DEFAULT': 'false', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': ['true', 'false']},\n",
      "              'description': 'Dump input to a file for debugging purposes. '\n",
      "                             'Creates several MB of data per step and slows '\n",
      "                             'down training; should only be used if requested '\n",
      "                             'by Sambanova to collect debug information.',\n",
      "              'field_name': 'dump_inputs',\n",
      "              'settings': {'DEFAULT': 'false', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '1',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'Period of evaluating the model in number of '\n",
      "                             'training steps. This parameter is only effective '\n",
      "                             \"when evaluation_strategy is set to 'steps'.\",\n",
      "              'field_name': 'eval_steps',\n",
      "              'settings': {'DEFAULT': '50', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': ['no', 'steps', 'epoch']},\n",
      "              'description': 'Strategy to validate the model during training',\n",
      "              'field_name': 'evaluation_strategy',\n",
      "              'settings': {'DEFAULT': 'no', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': ['true', 'false']},\n",
      "              'description': 'Fix which rank is assigned to which RDU for '\n",
      "                             'data-parallel jobs. Used for reproducibility of '\n",
      "                             'a job. Should only be used with a multiple of 8 '\n",
      "                             'RDUs.',\n",
      "              'field_name': 'fix_rank_rdu_mapping',\n",
      "              'settings': {'DEFAULT': 'false', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '1',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'How many steps you want to accumulate before '\n",
      "                             'updating the weights. This parameter can be used '\n",
      "                             'to increase the effective batch size during '\n",
      "                             'gradient decent without exceeding the memory '\n",
      "                             'limit.',\n",
      "              'field_name': 'grad_accumulation_steps',\n",
      "              'settings': {'DEFAULT': '1', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '0',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'learning rate to use in optimizer',\n",
      "              'field_name': 'learning_rate',\n",
      "              'settings': {'DEFAULT': '1.0e-05', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '1',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'Period of logging training loss in number of '\n",
      "                             'training steps',\n",
      "              'field_name': 'logging_steps',\n",
      "              'settings': {'DEFAULT': '1', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': ['polynomial_decay_schedule_with_warmup',\n",
      "                                        'cosine_schedule_with_warmup',\n",
      "                                        'fixed_lr']},\n",
      "              'description': 'Type of learning rate scheduler to use',\n",
      "              'field_name': 'lr_schedule',\n",
      "              'settings': {'DEFAULT': 'fixed_lr', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': None,\n",
      "              'description': 'Sequence length to pad or truncate the dataset',\n",
      "              'field_name': 'max_seq_length',\n",
      "              'settings': {'DEFAULT': '8192', 'USER_MODIFIABLE': False}},\n",
      "             {'constrains': None,\n",
      "              'description': 'The number of RDUs to run in model parallel.',\n",
      "              'field_name': 'model_parallel_rdus',\n",
      "              'settings': {'DEFAULT': '1', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': None,\n",
      "              'description': 'The parameter count of the model',\n",
      "              'field_name': 'model_parameter_count',\n",
      "              'settings': {'DEFAULT': '8b', 'USER_MODIFIABLE': False}},\n",
      "             {'constrains': {'ge': '1',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'number of iterations to run',\n",
      "              'field_name': 'num_iterations',\n",
      "              'settings': {'DEFAULT': '100', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '0',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'Loss scale for prompt tokens',\n",
      "              'field_name': 'prompt_loss_weight',\n",
      "              'settings': {'DEFAULT': '0.0', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': None,\n",
      "              'description': 'The mode to run with.',\n",
      "              'field_name': 'run_mode',\n",
      "              'settings': {'DEFAULT': 'balanced', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': ['true', 'false']},\n",
      "              'description': 'Whether to save the optimizer state when saving '\n",
      "                             'a checkpoint',\n",
      "              'field_name': 'save_optimizer_state',\n",
      "              'settings': {'DEFAULT': 'true', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '1',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'Period of saving the model checkpoints in number '\n",
      "                             'of training steps',\n",
      "              'field_name': 'save_steps',\n",
      "              'settings': {'DEFAULT': '50', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': ['true', 'false']},\n",
      "              'description': 'whether or not to skip the checkpoint',\n",
      "              'field_name': 'skip_checkpoint',\n",
      "              'settings': {'DEFAULT': 'false', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '0',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'Subsample for the evaluation dataset',\n",
      "              'field_name': 'subsample_eval',\n",
      "              'settings': {'DEFAULT': '0.01', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '1',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'Random seed to use for the subsample evaluation',\n",
      "              'field_name': 'subsample_eval_seed',\n",
      "              'settings': {'DEFAULT': '123', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': ['true', 'false']},\n",
      "              'description': 'Whether to use token_type_ids to compute loss',\n",
      "              'field_name': 'use_token_type_ids',\n",
      "              'settings': {'DEFAULT': 'true', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': None,\n",
      "              'description': 'Maximum size of vocabulary',\n",
      "              'field_name': 'vocab_size',\n",
      "              'settings': {'DEFAULT': '128256', 'USER_MODIFIABLE': False}},\n",
      "             {'constrains': {'ge': '0',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'warmup steps to use in learning rate scheduler '\n",
      "                             'in optimizer',\n",
      "              'field_name': 'warmup_steps',\n",
      "              'settings': {'DEFAULT': '0', 'USER_MODIFIABLE': True}},\n",
      "             {'constrains': {'ge': '0',\n",
      "                             'gt': '',\n",
      "                             'le': '',\n",
      "                             'lt': '',\n",
      "                             'values': []},\n",
      "              'description': 'weight decay rate to use in optimizer',\n",
      "              'field_name': 'weight_decay',\n",
      "              'settings': {'DEFAULT': '0.1', 'USER_MODIFIABLE': True}}]}\n"
     ]
    }
   ],
   "source": [
    "# check required hyperparams for training job \n",
    "hyperparams = sambastudio_client.get_default_hyperparms(model,'train')\n",
    "pprint(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = {\n",
    "    'job_name': 'e2e_fc_taining_job2',\n",
    "    'job_description': 'e2e finetuning training job public health for suzume multilingual',\n",
    "    'job_type': 'train',\n",
    "    'model': model,\n",
    "    'model_version': '1',\n",
    "    'parallel_instances': '1',\n",
    "    'dataset_name': dataset_name,\n",
    "    'load_state': False,\n",
    "    'sub_path': '',\n",
    "    'hyperparams': {\n",
    "        'batch_size': 8,\n",
    "        'max_seq_length': 8192,\n",
    "        'run_mode': 'balanced',\n",
    "        'vocab_size': 128256,\n",
    "        'do_eval': False,\n",
    "        'evaluation_strategy': 'no',\n",
    "        'fix_rank_rdu_mapping': False, \n",
    "        'grad_accumulation_steps': 1,\n",
    "        'learning_rate': 0.00001,\n",
    "        'logging_steps': 1,\n",
    "        'lr_schedule': 'fixed_lr',\n",
    "        'save_strategy': 'epoch',\n",
    "        'num_train_epochs': 5,\n",
    "        'save_interval': 1,\n",
    "        'prompt_loss_weight': 0.0,\n",
    "        'save_optimizer_state': True,\n",
    "        'subsample_eval': 0.01,\n",
    "        'subsample_eval_seed': 123,\n",
    "        'warmup_steps': 0,\n",
    "        'weight_decay': 0.1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 14:42:42,232 [INFO] Project with name 'byoc fine-tuning project' found with id 296e0c05-1338-4119-9ef6-0a5667b55b09\n",
      "2024-12-18 14:42:45,306 [INFO] Model 'Suzume-Llama-3-8B-Multilingual' with id '35847978-1a28-45da-99b1-0c2cea94d116' available for training and deployment found\n",
      "2024-12-18 14:42:45,662 [INFO] Dataset with name 'publichealth' found with id 6ac585ad-107c-45f5-a2de-129dd1a69279\n",
      "2024-12-18 14:42:45,983 [INFO] Job with name 'e2e_fc_taining_job2' created: '{'job_id': '0a09cb4e-01be-4807-911a-f1d2cc3904bf', 'job_name': 'e2e_fc_taining_job2', 'job_type': 'train', 'user_id': 'jorge.piedrahita', 'project_id': '296e0c05-1338-4119-9ef6-0a5667b55b09', 'tenant_id': 'f254d0b5-fb45-4501-9740-93183e7c6f4c', 'rdu_arch': 'SN40L-8', 'result_path': '', 'parallel_instances': 1, 'app_id': '61fa0993-04a2-42ca-9db1-1eff693ea978', 'model_checkpoint': 'Suzume-Llama-3-8B-Multilingual', 'checkpoint_id': '', 'dataset_id': '6ac585ad-107c-45f5-a2de-129dd1a69279', 'description': 'e2e finetuning training job public health for suzume multilingual', 'status': 'CREATED', 'image_version': '1.1.6-20241025', 'variant_set_version': '3bd6d55fcf31c2b5c9c06293fa557c2e', 'variant_name': 'sn40.b=8.d=off.m=8192.m=1.m=8b.r=balanced.v=128256', 'project_name': 'byoc fine-tuning project', 'dataset_name': '', 'input_data_path': '', 'hyperparams': [], 'time_created': '2024-12-18T19:42:46.059862000Z', 'time_updated': '2024-12-18T19:42:46.059862000Z', 'load_state': False, 'app_name': 'Samba1 Llama3 Experts', 'environment_variables': '', 'model_id': '35847978-1a28-45da-99b1-0c2cea94d116', 'model_status': 'Available', 'model_version': 1, 'dataset': 'publichealth', 'dataset_path': 'default/default/datasets/local-dataset-6ac585ad-107c-45f5-a2de-129dd1a69279', 'tracking_id': 'd9a1e8ff-9955-47fd-bfa2-c659226d7104', 'status_code': 200, 'headers': {'access-control-allow-headers': 'Accept, Content-Type, Content-Length, Accept-Encoding, Authorization, ResponseType, Access-Control-Allow-Origin', 'access-control-allow-methods': 'GET, POST, PATCH, DELETE', 'access-control-allow-origin': 'https://sjc3-tstest.sambanova.net', 'content-security-policy': \"default-src 'self'\", 'content-type': 'application/json', 'permissions-policy': 'none', 'referrer-policy': 'no-referrer', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-content-type-options': 'nosniff', 'x-correlation-id': 'd9a1e8ff-9955-47fd-bfa2-c659226d7104', 'x-frame-options': 'DENY', 'date': 'Wed, 18 Dec 2024 19:42:46 GMT', 'x-envoy-upstream-service-time': '173', 'server': 'istio-envoy', 'content-encoding': 'gzip', 'vary': 'Accept-Encoding', 'transfer-encoding': 'chunked'}}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0a09cb4e-01be-4807-911a-f1d2cc3904bf'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sambastudio_client.run_training_job(\n",
    "    project_name = project[\"project_name\"],\n",
    "    job_name = job['job_name'],\n",
    "    job_description = job['job_description'],\n",
    "    job_type = job['job_type'],\n",
    "    model = job['model'],\n",
    "    model_version = job['model_version'],\n",
    "    dataset_name = job['dataset_name'],\n",
    "    parallel_instances = job['parallel_instances'],\n",
    "    load_state = job['load_state'],\n",
    "    sub_path = job['sub_path'],\n",
    "    rdu_arch = 'SN40L-8',\n",
    "    hyperparams = job['hyperparams']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 14:43:19,561 [INFO] Project with name 'byoc fine-tuning project' found with id 296e0c05-1338-4119-9ef6-0a5667b55b09\n",
      "2024-12-18 14:43:20,002 [INFO] Project with name 'byoc fine-tuning project' found with id 296e0c05-1338-4119-9ef6-0a5667b55b09\n",
      "2024-12-18 14:43:20,704 [INFO] Job with name 'e2e_fc_taining_job2' in project 'byoc fine-tuning project' found with id '0a09cb4e-01be-4807-911a-f1d2cc3904bf'\n",
      "2024-12-18 14:43:21,005 [INFO] Job `e2e_fc_taining_job2` with progress status: TRAINING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'job_id': '0a09cb4e-01be-4807-911a-f1d2cc3904bf',\n",
       " 'job_name': 'e2e_fc_taining_job2',\n",
       " 'job_type': 'train',\n",
       " 'user_id': 'jorge.piedrahita',\n",
       " 'project_id': '296e0c05-1338-4119-9ef6-0a5667b55b09',\n",
       " 'tenant_id': 'f254d0b5-fb45-4501-9740-93183e7c6f4c',\n",
       " 'rdu_arch': 'SN40L-8',\n",
       " 'result_path': '',\n",
       " 'parallel_instances': 1,\n",
       " 'app_id': '61fa0993-04a2-42ca-9db1-1eff693ea978',\n",
       " 'model_checkpoint': 'Suzume-Llama-3-8B-Multilingual',\n",
       " 'checkpoint_id': '',\n",
       " 'dataset_id': '6ac585ad-107c-45f5-a2de-129dd1a69279',\n",
       " 'description': 'e2e finetuning training job public health for suzume multilingual',\n",
       " 'status': 'TRAINING',\n",
       " 'image_version': '',\n",
       " 'variant_set_version': '',\n",
       " 'variant_name': '',\n",
       " 'project_name': '',\n",
       " 'dataset_name': '',\n",
       " 'input_data_path': '',\n",
       " 'hyperparams': [{'DATATYPE': '',\n",
       "   'DESCRIPTION': 'The per-worker batch size',\n",
       "   'FIELD_NAME': 'batch_size',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '8'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': \"Toggles debug mode. Debug mode 'on' mode turns on additional output to help diagnose certain issues during training. Please keep 'off' unless advised otherwise by your SambaNova admin.\",\n",
       "   'FIELD_NAME': 'debug_mode',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'off'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'whether or not to do final evaluation',\n",
       "   'FIELD_NAME': 'do_eval',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'false'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Dump input to a file for debugging purposes. Creates several MB of data per step and slows down training; should only be used if requested by Sambanova to collect debug information.',\n",
       "   'FIELD_NAME': 'dump_inputs',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'false'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': \"Period of evaluating the model in number of training steps. This parameter is only effective when evaluation_strategy is set to 'steps'.\",\n",
       "   'FIELD_NAME': 'eval_steps',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '50'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Strategy to validate the model during training',\n",
       "   'FIELD_NAME': 'evaluation_strategy',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'no'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Fix which rank is assigned to which RDU for data-parallel jobs. Used for reproducibility of a job. Should only be used with a multiple of 8 RDUs.',\n",
       "   'FIELD_NAME': 'fix_rank_rdu_mapping',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'false'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'How many steps you want to accumulate before updating the weights. This parameter can be used to increase the effective batch size during gradient decent without exceeding the memory limit.',\n",
       "   'FIELD_NAME': 'grad_accumulation_steps',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '1'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'learning rate to use in optimizer',\n",
       "   'FIELD_NAME': 'learning_rate',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '0.00001'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Period of logging training loss in number of training steps',\n",
       "   'FIELD_NAME': 'logging_steps',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '1'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Type of learning rate scheduler to use',\n",
       "   'FIELD_NAME': 'lr_schedule',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'fixed_lr'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Sequence length to pad or truncate the dataset',\n",
       "   'FIELD_NAME': 'max_seq_length',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '8192'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'The number of RDUs to run in model parallel.',\n",
       "   'FIELD_NAME': 'model_parallel_rdus',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '1'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'The parameter count of the model',\n",
       "   'FIELD_NAME': 'model_parameter_count',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '8b'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'number of iterations to run',\n",
       "   'FIELD_NAME': 'num_iterations',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '100'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Loss scale for prompt tokens',\n",
       "   'FIELD_NAME': 'prompt_loss_weight',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '0'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'The mode to run with.',\n",
       "   'FIELD_NAME': 'run_mode',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'balanced'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Whether to save the optimizer state when saving a checkpoint',\n",
       "   'FIELD_NAME': 'save_optimizer_state',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'true'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Period of saving the model checkpoints in number of training steps',\n",
       "   'FIELD_NAME': 'save_steps',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '50'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'whether or not to skip the checkpoint',\n",
       "   'FIELD_NAME': 'skip_checkpoint',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'false'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Subsample for the evaluation dataset',\n",
       "   'FIELD_NAME': 'subsample_eval',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '0.01'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Random seed to use for the subsample evaluation',\n",
       "   'FIELD_NAME': 'subsample_eval_seed',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '123'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Whether to use token_type_ids to compute loss',\n",
       "   'FIELD_NAME': 'use_token_type_ids',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': 'true'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Maximum size of vocabulary',\n",
       "   'FIELD_NAME': 'vocab_size',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '128256'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'warmup steps to use in learning rate scheduler in optimizer',\n",
       "   'FIELD_NAME': 'warmup_steps',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '0'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'weight decay rate to use in optimizer',\n",
       "   'FIELD_NAME': 'weight_decay',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '0.1'},\n",
       "  {'DATATYPE': '',\n",
       "   'DESCRIPTION': 'Number of sockets each instance of the model uses',\n",
       "   'FIELD_NAME': 'sockets',\n",
       "   'MESSAGE': '',\n",
       "   'TASK_TYPE': [],\n",
       "   'TYPE_SPECIFIC_SETTINGS': {},\n",
       "   'CONSTRAINTS': None,\n",
       "   'VARIANT_SELECTION': False,\n",
       "   'FIELD_VALUE': '1'}],\n",
       " 'time_created': '2024-12-18T19:42:46.059862000Z',\n",
       " 'time_updated': '2024-12-18T19:43:16.390360000Z',\n",
       " 'load_state': False,\n",
       " 'app_name': 'Samba1 Llama3 Experts',\n",
       " 'environment_variables': '',\n",
       " 'model_id': '35847978-1a28-45da-99b1-0c2cea94d116',\n",
       " 'model_status': 'Available',\n",
       " 'model_version': 1,\n",
       " 'dataset': 'publichealth',\n",
       " 'dataset_path': 'default/default/datasets/local-dataset-6ac585ad-107c-45f5-a2de-129dd1a69279',\n",
       " 'tracking_id': 'd9a1e8ff-9955-47fd-bfa2-c659226d7104'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sambastudio_client.check_job_progress(\n",
    "    project_name=project['project_name'],\n",
    "    job_name=job['job_name'],\n",
    "    verbose=True,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promote Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 16:03:15,340 [INFO] Project with name 'byoc fine-tuning project' found with id b11867e6-7ca8-45bd-b09b-41cbc7ba73ce\n",
      "2024-11-25 16:03:15,664 [INFO] Project with name 'byoc fine-tuning project' found with id b11867e6-7ca8-45bd-b09b-41cbc7ba73ce\n",
      "2024-11-25 16:03:15,905 [INFO] Job with name 'e2e_fc_taining_job' in project 'byoc fine-tuning project' found with id '1819ba81-9f93-4197-a7c3-51df6a3f8f0e'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'checkpoint_name': '1819ba81-9f93-4197-a7c3-51df6a3f8f0e-10',\n",
       "  'checkpoint_id': '6925d395-3251-4465-8ec6-225761536680',\n",
       "  'steps': 10,\n",
       "  'time_created': '2024-11-25T20:55:13.923615Z',\n",
       "  'metrics': {'single_value': {'train_learning_rate': 0.0,\n",
       "    'train_loss': 1.6116},\n",
       "   'multi_value': {},\n",
       "   'last_batch_omitted': []},\n",
       "  'labels': None,\n",
       "  'job_id': '1819ba81-9f93-4197-a7c3-51df6a3f8f0e',\n",
       "  'app_id': '61fa0993-04a2-42ca-9db1-1eff693ea978',\n",
       "  'app_name': 'Samba1 Llama3 Experts',\n",
       "  'path': 'default/default/b11867e6-7ca8-45bd-b09b-41cbc7ba73ce/jobs/1819ba81-9f93-4197-a7c3-51df6a3f8f0e/checkpoints/1819ba81-9f93-4197-a7c3-51df6a3f8f0e-10',\n",
       "  'transformers_version': '',\n",
       "  'torch_version': '',\n",
       "  'user_id': 'jorge.piedrahita',\n",
       "  'tenant_id': 'f254d0b5-fb45-4501-9740-93183e7c6f4c',\n",
       "  'image_version': '1.1.6-20241025',\n",
       "  'dependent_jobs': []},\n",
       " {'checkpoint_name': '1819ba81-9f93-4197-a7c3-51df6a3f8f0e-5',\n",
       "  'checkpoint_id': '19a5cf40-1663-406c-816a-fceb759624f5',\n",
       "  'steps': 5,\n",
       "  'time_created': '2024-11-25T20:48:35.023844Z',\n",
       "  'metrics': {'single_value': {'train_learning_rate': 0.0,\n",
       "    'train_loss': 2.1781},\n",
       "   'multi_value': {},\n",
       "   'last_batch_omitted': []},\n",
       "  'labels': None,\n",
       "  'job_id': '1819ba81-9f93-4197-a7c3-51df6a3f8f0e',\n",
       "  'app_id': '61fa0993-04a2-42ca-9db1-1eff693ea978',\n",
       "  'app_name': 'Samba1 Llama3 Experts',\n",
       "  'path': 'default/default/b11867e6-7ca8-45bd-b09b-41cbc7ba73ce/jobs/1819ba81-9f93-4197-a7c3-51df6a3f8f0e/checkpoints/1819ba81-9f93-4197-a7c3-51df6a3f8f0e-5',\n",
       "  'transformers_version': '',\n",
       "  'torch_version': '',\n",
       "  'user_id': 'jorge.piedrahita',\n",
       "  'tenant_id': 'f254d0b5-fb45-4501-9740-93183e7c6f4c',\n",
       "  'image_version': '1.1.6-20241025',\n",
       "  'dependent_jobs': []}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will promote the checkpoint with less training loss so we list it sorted \n",
    "checkpoints = sambastudio_client.list_checkpoints(\n",
    "    project_name=project['project_name'],\n",
    "    job_name=job['job_name'],\n",
    "    sort=True\n",
    ")\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Promoted checkpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set checkpoint to promote config\n",
    "model_checkpoint = {\n",
    "    'checkpoint_name': checkpoints[0]['name'],\n",
    "    'model_name': 'Suzume-Llama-3-8B-Multilingual-Publichealth',\n",
    "    'model_description': 'finetuned suzume multilingual in public health qa dataset',\n",
    "    'model_type': 'finetuned'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 16:03:51,838 [INFO] Project with name 'byoc fine-tuning project' found with id b11867e6-7ca8-45bd-b09b-41cbc7ba73ce\n",
      "2024-11-25 16:03:52,088 [INFO] Project with name 'byoc fine-tuning project' found with id b11867e6-7ca8-45bd-b09b-41cbc7ba73ce\n",
      "2024-11-25 16:03:52,329 [INFO] Job with name 'e2e_fc_taining_job' in project 'byoc fine-tuning project' found with id '1819ba81-9f93-4197-a7c3-51df6a3f8f0e'\n",
      "2024-11-25 16:03:53,245 [INFO] Model checkpoint '1819ba81-9f93-4197-a7c3-51df6a3f8f0e-10' promoted to model 'Suzume-Llama-3-8B-Multilingual-Publichealth'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c867b392-2d02-453d-9fd8-e14016e39153'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the promote_checkpoint method from client with checkpoint parameters\n",
    "sambastudio_client.promote_checkpoint(\n",
    "    checkpoint_name = model_checkpoint['name'],\n",
    "    project_name=project['project_name'],\n",
    "    job_name=job['job_name'],\n",
    "    model_name=model_checkpoint['model_name'],\n",
    "    model_description=model_checkpoint['model_description'],\n",
    "    model_type=model_checkpoint['model_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_id': 'c867b392-2d02-453d-9fd8-e14016e39153',\n",
       "  'model_checkpoint_name': 'Suzume-Llama-3-8B-Multilingual-Publichealth',\n",
       "  'version': 1}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the promoted model is now in SambaStudio models\n",
    "[model for model in sambastudio_client.list_models() if model['model_checkpoint_name']==model_checkpoint['model_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all saved training checkpoints, after promotion (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 16:06:20,296 [INFO] Model checkpoint '1819ba81-9f93-4197-a7c3-51df6a3f8f0e-10' deleted\n",
      "2024-11-25 16:06:20,586 [INFO] Model checkpoint '1819ba81-9f93-4197-a7c3-51df6a3f8f0e-5' deleted\n"
     ]
    }
   ],
   "source": [
    "# We can delete all intermediate checkpoints saved during the training job \n",
    "for checkpoint in checkpoints:\n",
    "    sambastudio_client.delete_checkpoint(checkpoint[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlined Execution\n",
    "\n",
    "The training job and checkpoint promotion can be done in a streamlined way setting all the job and checkpoint parameters in a config file like in the [finetune_config.yaml](../finetune_config.yaml) example, and executing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(kit_dir, 'finetune_config.yaml')\n",
    "sambastudio_client = SnsdkWrapper(config_file)\n",
    "sambastudio_client.create_project()\n",
    "sambastudio_client.run_training_job()\n",
    "sambastudio_client.check_job_progress(wait=True)\n",
    "checkpoints = sambastudio_client.list_checkpoints(sort=True)\n",
    "sambastudio_client.promote_checkpoint(checkpoints[0]['checkpoint_name'])\n",
    "for checkpoint in checkpoints:\n",
    "    sambastudio_client.delete_checkpoint(checkpoint[\"checkpoint_name\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
